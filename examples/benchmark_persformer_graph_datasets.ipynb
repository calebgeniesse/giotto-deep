{"cells":[{"cell_type":"markdown","metadata":{},"source":[" ## Benchmarking PersFormer on the graph datasets.\n"," We will compare the accuracy on the graph datasets of our SetTransformer\n"," based on PersFormer with the perslayer introduced in the paper:\n"," https://arxiv.org/abs/1904.09378"]},{"cell_type":"markdown","metadata":{},"source":[" ## Benchmarking MUTAG\n"," We will compare the test accuracies of PersLay and PersFormer on the MUTAG\n"," dataset. It consists of 188 graphs categorised into two classes.\n"," We will train the PersFormer on the same input features as PersFormer to\n"," get a fair comparison.\n"," The features PersLay is trained on are the extended persistence diagrams of\n"," the vertices of the graph filtered by the heat kernel signature (HKS)\n"," at time t=10.\n"," The maximum (wrt to the architecture and the hyperparameters) mean test\n"," accuracy of PersLay is 89.8(Â±0.9) and the train accuracy with the same\n"," model and the same hyperparameters is 92.3.\n"," They performed 10-fold evaluation, i.e. splitting the dataset into\n"," 10 equally-sized folds and then record the test accuracy of the i-th\n"," fold and training the model on the 9 other folds."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import libraries:\n","import torch\n","from torch import Tensor\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader\n","import matplotlib.pyplot as plt  # type: ignore\n","\n","from gdeep.topology_layers import load_data, SetTransformer,\\\n","    SelfAttentionSetTransformer, train\n","\n","# for SAM training\n","from gdeep.topology_layers import sam_train\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use on of the following datasets to train the model\n","# MUTAG, PROTEINS, COX2\n","dataset_name = \"COX2\"\n","\n","pers_only = True\n","use_sam = False\n","n_epochs = 30\n","lr = 1e-3\n","batch_size = 16\n","ln = True  # LayerNorm in Set Transformer\n","use_regularization = False  # Use L2-regularization\n","balance_dataset = True  # balance dataset to 50 by removing datapoint from\n","use_induced_attention = False  # use trainable query vector instead of\n","# self-attention; use induced attention for large sets because of the\n","# quadratic scaling of self-attention.\n","# the class with more points\n","# only use the persistence diagrams as features not the spectral features\n","train_size = 0.8  # ratio train size to total size of dataset\n","optimizer = lambda params: torch.optim.Adam(params, lr=lr)  # noqa: E731\n","\n","# CUDA for PyTorch\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","\n","# Compare with PersLay baseline\n","if dataset_name == \"PROTEINS\" and pers_only:\n","    benchmark_accuracy = 72.2\n","elif dataset_name == \"MUTAG\" and pers_only:\n","    benchmark_accuracy = 85.2\n","elif dataset_name == \"COX2\" and pers_only:\n","    benchmark_accuracy = 81.5\n","elif dataset_name == \"COLLAB\" and pers_only:\n","    benchmark_accuracy = 71.6\n","elif dataset_name == \"NCI1\" and pers_only:\n","    benchmark_accuracy = 72.63\n","else:\n","    raise NotImplementedError(\"benchmark_accuracy not defined\")\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load extended persistence diagrams and additional features\n","\n","\n","x_pds_dict, x_features, y = load_data(dataset_name)\n","\n","# if `pers_only` set spectral features to 0\n","if pers_only:\n","    x_features = 0.0 * x_features\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Padding persistence diagrams to make them the same size\n","\n","# transform x_pds to a single tensor with tailing zeros\n","num_types = x_pds_dict[0].shape[1] - 2\n","num_graphs = len(x_pds_dict.keys())  # type: ignore\n","\n","max_number_of_points = max([x_pd.shape[0]\n","                            for _, x_pd in x_pds_dict.items()])  # type: ignore\n","\n","x_pds = torch.zeros((num_graphs, max_number_of_points, num_types + 2))\n","\n","for idx, x_pd in x_pds_dict.items():  # type: ignore\n","    x_pds[idx, :x_pd.shape[0], :] = x_pd\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Balance labels in dataset\n","\n","if balance_dataset:\n","    if y.sum() / y.shape[0] > 0.5:\n","        class_to_remove = 1\n","        num_classes_to_remove = int(2 * y.sum() - y.shape[0])\n","    else:\n","        class_to_remove = 0\n","        num_classes_to_remove = int(y.shape[0] - 2 * y.sum())\n","    idxs_to_remove = ((y == class_to_remove)\n","                      .nonzero(as_tuple=False)[:num_classes_to_remove, 0]\n","                      .tolist())\n","\n","    idxs_to_remain = [i for i in range(y.shape[0]) if i not in idxs_to_remove]\n","\n","    y = y[idxs_to_remain]\n","    x_pds = x_pds[idxs_to_remain]\n","    x_features = x_features[idxs_to_remain]\n","\n","    print('number of data points removed:', num_classes_to_remove)\n","\n","print('balance:', (y.sum() / y.shape[0]).item())\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set up dataset and dataloader\n","# create the datasets\n","# https://discuss.pytorch.org/t/make-a-tensordataset-and-dataloader\n","# -with-multiple-inputs-parameters/26605\n","total_size = x_pds.shape[0]\n","\n","graph_ds = TensorDataset(x_pds, x_features, y)\n","\n","train_size = int(total_size * train_size)\n","graph_ds_train, graph_ds_val = torch.utils.data.random_split(\n","                                                    graph_ds,\n","                                                    [train_size,\n","                                                     total_size - train_size])\n","\n","# create data loaders\n","graph_dl_train = DataLoader(\n","    graph_ds_train,\n","    num_workers=4,\n","    batch_size=batch_size,\n","    shuffle=True\n",")\n","\n","graph_dl_val = DataLoader(\n","    graph_ds_val,\n","    num_workers=4,\n","    batch_size=batch_size,\n","    shuffle=False\n",")\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Compute balance of train and validation datasets\n","val_balance = 0\n","val_total = 0\n","for _, _, y_batch in graph_dl_val:\n","    val_balance += y_batch.sum()\n","    val_total += y_batch.shape[0]\n","print('train_size:', val_total)\n","print('train_balance', val_balance / val_total)\n","\n","train_balance = 0\n","train_total = 0\n","for _, _, y_batch in graph_dl_train:\n","    train_balance += y_batch.sum()\n","    train_total += y_batch.shape[0]\n","print('val_size:', train_total)\n","print('val_balance', train_balance / train_total)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define Model architecture for the graph classifier\n","\n","\n","class GraphClassifier(nn.Module):\n","    \"\"\"Classifier for Graphs using persistence features and additional\n","    features. The vectorization is based on a set transformer.\n","    \"\"\"\n","    def __init__(self,\n","                 num_features,\n","                 dim_input=6,\n","                 num_outputs=1,\n","                 dim_output=50,\n","                 num_classes=2,\n","                 ln=ln):\n","        super(GraphClassifier, self).__init__()\n","        if use_induced_attention:\n","            self.st = SetTransformer(\n","                dim_input=dim_input,\n","                num_outputs=num_outputs,\n","                dim_output=dim_output,\n","                ln=ln\n","                )\n","        else:\n","            self.st = SelfAttentionSetTransformer(\n","                dim_input=dim_input,\n","                num_outputs=num_outputs,\n","                dim_output=dim_output,\n","                ln=ln\n","                )\n","        self.num_classes = num_classes\n","        self.ln = nn.LayerNorm(dim_output + num_features)\n","        self.ff_1 = nn.Linear(dim_output + num_features, 50)\n","        self.ff_2 = nn.Linear(50, 20)\n","        self.ff_3 = nn.Linear(20, num_classes)\n","\n","    def forward(self, x_pd: Tensor, x_feature: Tensor) -> Tensor:\n","        \"\"\"Forward pass of the graph classifier.\n","        The persistence features are encoded with a set transformer\n","        and concatenated with the feature vector. These concatenated\n","        features are used for classification using a fully connected\n","        feed -forward layer.\n","\n","        Args:\n","            x_pd (Tensor): persistence diagrams of the graph\n","            x_feature (Tensor): additional graph features\n","        \"\"\"\n","        pd_vector = self.st(x_pd)\n","        #print(pd_vector.shape, x_feature.shape)\n","        features_stacked = torch.hstack((pd_vector, x_feature))\n","        x = self.ln(features_stacked)\n","        x = nn.ReLU()(self.ff_1(x))\n","        x = nn.ReLU()(self.ff_2(x))\n","        x = self.ff_3(x)\n","        return x\n","\n","\n","# define graph classifier\n","gc = GraphClassifier(\n","        num_features=graph_ds_train[0][1].shape[0],\n","        dim_input=graph_ds_train[0][0].shape[1],\n","        num_outputs=1,\n","        dim_output=50)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if use_sam:\n","    train_fct = sam_train\n","else:\n","    train_fct = train\n","\n","# train the model and return losses and accuracies information\n","(losses,\n"," val_losses,\n"," train_accuracies,\n"," val_accuracies) = train_fct(\n","                            gc,\n","                            graph_dl_train,\n","                            graph_dl_val,\n","                            lr=lr,\n","                            verbose=True,\n","                            num_epochs=n_epochs,\n","                            use_cuda=use_cuda,\n","                            use_regularization=use_regularization,\n","                            optimizer=optimizer\n","                            )\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# plot losses\n","plt.plot(losses, label='train_loss')\n","plt.plot([4 * x for x in val_losses], label='4 * val_loss')\n","plt.legend()\n","plt.title(\"Losses \" + dataset_name + \" extended persistence features only\")\n","plt.show()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# plot accuracies\n","plt.plot(train_accuracies, label='train_acc')\n","plt.plot(val_accuracies, label='val_acc')\n","plt.plot([benchmark_accuracy]*len(train_accuracies),\n","         label='PersLay PD only')\n","plt.legend()\n","plt.title(\"Accuracies \" + dataset_name + \" extended persistence features only\")\n","plt.show()\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lr = 1e-3\n","n_epochs = 200"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}