{"cells":[{"cell_type":"markdown","metadata":{},"source":[" ## Benchmarking PersFormer on the graph datasets.\n"," We will compare the accuracy on the graph datasets of our SetTransformer\n"," based on PersFormer with the perslayer introduced in the paper:\n"," https://arxiv.org/abs/1904.09378"]},{"cell_type":"markdown","metadata":{},"source":[" ## Benchmarking MUTAG\n"," We will compare the test accuracies of PersLay and PersFormer on the MUTAG\n"," dataset. It consists of 188 graphs categorised into two classes.\n"," We will train the PersFormer on the same input features as PersFormer to\n"," get a fair comparison.\n"," The features PersLay is trained on are the extended persistence diagrams of\n"," the vertices of the graph filtered by the heat kernel signature (HKS)\n"," at time t=10.\n"," The maximum (wrt to the architecture and the hyperparameters) mean test\n"," accuracy of PersLay is 89.8(Â±0.9) and the train accuracy with the same\n"," model and the same hyperparameters is 92.3.\n"," They performed 10-fold evaluation, i.e. splitting the dataset into\n"," 10 equally-sized folds and then record the test accuracy of the i-th\n"," fold and training the model on the 9 other folds."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import libraries:\n","from typing import Tuple, Dict\n","import numpy as np  # typing: ignore\n","import random\n","import torch\n","from torch import Tensor\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader, dataset\n","from einops import rearrange  # typing: ignore\n","from os.path import join, isfile\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","from gdeep.topology_layers import ISAB, PMA, SetTransformer\n","# for loading extended persistence diagrams that are in hdf5 format\n","import h5py  # typing: ignore\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DATASET_NAME = \"NCI1\"\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","def persistence_diagrams_to_sequence(\n","        tensor_dict: Dict[str, Dict[str, Tensor]]\n","     ):\n","    \"\"\"Convert tensor dictionary to sequence of Tensors\n","        Output will be a List of tensors of the shape [graphs, absolute\n","        number of points per graph, 2(for the x and y coordinate)\n","        + number of types]\n","\n","    Args:\n","        tensor_dict (Dict[str, Dict[str, Tensor]]): Dictionary of types and\n","            Dictionary of graphs and Tensors of points in the persistence\n","            diagrams.\n","\n","    Returns:\n","        Dict[Int, Tensor]: List of tensors of the shape described above\n","    \"\"\"\n","    types = list(tensor_dict.keys())\n","\n","    sequence_dict = {}\n","\n","    def encode_points(graph_idx, type_idx, type_, n_pts):\n","        one_hot = F.one_hot(\n","                torch.tensor([type_idx] * n_pts),\n","                num_classes=len(types))\n","        return torch.cat([\n","                    tensor_dict[type_][str(graph_idx)],\n","                    one_hot.expand((n_pts, len(types)))\n","                ], axis=-1)\n","\n","    for graph_idx in [int(k) for k in tensor_dict[types[0]].keys()]:\n","        tensor_list = []\n","        for type_idx, type_ in enumerate(types):\n","            n_pts = tensor_dict[type_][str(graph_idx)].shape[0]\n","            if(n_pts > 0):\n","                tensor_list.append(encode_points(graph_idx,\n","                                                type_idx,\n","                                                type_,\n","                                                n_pts))\n","        sequence_dict[graph_idx] = torch.cat(tensor_list, axis=0)\n","    return sequence_dict\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load extended persistence diagrams and additional features\n","    \n","\n","def load_data(\n","        dataset_: str = \"MUTAG\",\n","        path_dataset: str = \"graph_data\",\n","        verbose: bool = False\n","        ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","    \"\"\"Load dataset from files.\n","\n","    Args:\n","        dataset (str, optional): File name of the dataset to load. There should\n","            be a hdf5 file for the extended persistence diagrams of the dataset\n","            as well as a csv file for the additional features in the path\n","            dataset directory. Defaults\n","            to \"MUTAG\".\n","        path_dataset (str, optional): Directory name of the dataset to load.\n","            Defaults to None.\n","        verbose (bool, optional): If `True` print size of the loaded dataset.\n","            Defaults to False.\n","    Returns:\n","        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Tuple of the loaded\n","            dataset consisting of the persistent features of the graphs, the\n","            additional features.\n","    \"\"\"\n","    filenames = {}\n","    for file_suffix in [\".hdf5\", \".csv\"]:\n","        try:\n","            filenames[file_suffix] = join(path_dataset,\n","                                          dataset_,\n","                                          dataset_ + file_suffix)\n","            assert(isfile(filenames[file_suffix]))\n","        except AssertionError:\n","            print(dataset_ + file_suffix +\n","                  \" does not exist in given directory!\")\n","    diagrams_file = h5py.File(filenames[\".hdf5\"], \"r\")\n","    # directory with persistance diagram type as keys\n","    # every directory corresponding to a key contains\n","    # subdirectories '0', '1', ... corresponding to the graphs.\n","    # For example, one can access a diagram by\n","    # diagrams_file['Ext1_10.0-hks']['1']\n","    # This is a hdf5 dataset object that contains the points of then\n","    # corresponding persistence diagram. These may contain different\n","    # numbers of points.\n","\n","    persistence_array_dict: Dict[int, np.array] = {}\n","    # list of tensorised persistence diagrams\n","\n","    additional_features = pd.read_csv(filenames[\".csv\"], index_col=0, header=0)\n","    labels = additional_features[['label']].values  # true labels of graphs\n","    label_encoder = LabelEncoder()\n","    y = label_encoder.fit_transform(labels.reshape(-1))\n","    x_features = np.array(additional_features)[:, 1:]\n","    # additional graph features\n","\n","    number_of_graphs = additional_features.shape[0]\n","    # number of graphs in the dataset\n","\n","    # convert values in diagrams_file from numpy.ndarray to torch.tensor\n","    tensor_dict = {}\n","\n","    for type_ in diagrams_file.keys():\n","        tensor_dict[type_] = {}\n","        for graph in diagrams_file[type_].keys():\n","            tensor_dict[type_][graph] = torch.tensor(\n","                                            diagrams_file[type_][graph]\n","                                            )\n","\n","    # for pt_idx, persistence_type in enumerate(diagrams_file.keys()):\n","    #     diagram_dict = diagrams_file[persistence_type]\n","    #     temp_arr_dict: Dict[int, np.array] = {}\n","    #     # dictionary containing the graph index as key and the\n","    #     # points (as np.array) of the persistence diagram of type\n","    #     # `persistence_type` as value.\n","\n","    #     # compute maximal number of points an store persistence points in\n","    #     # temp_arr_dict\n","    #     max_number_of_points = 0\n","    #     for graph_idx in diagrams_file[persistence_type].keys():\n","    #         pt_arr = np.array(diagram_dict[graph_idx])\n","    #         max_number_of_points = max(max_number_of_points, pt_arr.shape[0])\n","    #         temp_arr_dict[int(graph_idx)] = pt_arr\n","    #     persistence_array_dict[pt_idx] = np.zeros((\n","    #                                                 number_of_graphs,\n","    #                                                 max_number_of_points,\n","    #                                                 2\n","    #                                              ))\n","    #     # store all persistence points in temp_arr_dict in a single tensor\n","    #     # arrays will be filled by zeros to have a uniform tensor of shape\n","    #     # [number_of_graphs, max_number_of_points, 2]\n","    #     for graph_idx in temp_arr_dict.keys():\n","    #         persistence_array_dict[pt_idx] = temp_arr_dict[graph_idx]\n","\n","    if verbose:\n","        print(\n","            \"Dataset:\", dataset_,\n","            \"\\nNumber of graphs:\", number_of_graphs,\n","            \"\\nNumber of classes\", label_encoder.classes_.shape[0]\n","            )\n","    return (persistence_diagrams_to_sequence(tensor_dict),\n","            torch.tensor(x_features, dtype=torch.float),\n","            torch.tensor(y))\n","\n","# def convert_to_one_hot(x: torch.Tensor, axis: int = 0) -> torch.Tensor:\n","#     \"\"\"Convert tensor to one-hot representation along given axis.\n","\n","#     Args:\n","#         x (torch.Tensor): Tensor to be converted.\n","#         axis (int, optional): Axis of the conversion. Defaults to 0.\n","\n","#     Returns:\n","#         torch.Tensor: Tensor converted\n","#     \"\"\"\n","\n","#     return x\n","\n","\n","x_pds_dict, x_features, y = load_data(DATASET_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Test persistence_diagrams_to_sequence with MUTAG dataset\n","\n","def random_compatibility_test(n_trials: int = 10) -> None:\n","    \"\"\"Randomly check if persistence diagram is correctly converted to\n","    sequence.\n","\n","    Raises:\n","        Exception: Assertion error.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    filename = join(\"graph_data\",\n","                    \"MUTAG\",\n","                    \"MUTAG\" + \".hdf5\")\n","    diagrams_file = h5py.File(filename, \"r\")\n","    \n","    # load_data with `load_data` methode\n","    seq_pd, _ , _ = load_data(\"MUTAG\")\n","\n","    for _ in range(n_trials):\n","\n","        type_ = random.choice(list(diagrams_file.keys()))\n","\n","        type_index = list(diagrams_file.keys()).index(type_)\n","\n","\n","        graph_number = random.choice(list(diagrams_file[type_].keys()))\n","\n","        # find indices that belong to type_\n","        idx = seq_pd[int(graph_number)][:, 2 + type_index] == 1.0\n","\n","        computed_pts = seq_pd[int(graph_number)][idx][:, :2]\n","\n","\n","        original_pts = torch.tensor(diagrams_file[type_][graph_number])\n","\n","        try:\n","            assert torch.allclose(original_pts, computed_pts)\n","        except AssertionError:\n","            raise AssertionError(\"persistence_diagrams_to_sequence does not\" +\n","                                 \"return the right sequence tensor\")\n","            \n","random_compatibility_test()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Test persistence_diagrams_to_sequence\n","\n","\n","tensor_dict = {\"type1\": {\n","                    \"1\": torch.tensor([[0.0066, 0.7961],\n","                                       [0.6612, 0.0359],\n","                                       [0.8394, 0.1597]]),\n","                    \"2\": torch.tensor([[0.1787, 0.1809],\n","                                       [0.2645, 0.5766],\n","                                       [0.5666, 0.1630],\n","                                       [0.9986, 0.0259]]),\n","                    \"0\": torch.tensor([[0.6910, 0.1265],\n","                                       [0.9085, 0.0230],\n","                                       [0.4977, 0.6386],\n","                                       [0.1331, 0.8196],\n","                                       [0.6929, 0.1859],\n","                                       [0.4216, 0.2283],\n","                                       [0.4996, 0.3380]]),\n","                    },\n","               \"type2\": {\n","                    \"1\": torch.tensor([[0.0932, 0.7327],\n","                                       [0.7248, 0.7940],\n","                                       [0.5550, 0.9960]]),\n","                    \"2\": torch.tensor([[0.9541, 0.6892],\n","                                       [0.7984, 0.8061],\n","                                       [0.5266, 0.0644],\n","                                       [0.0630, 0.2176]]),\n","                    \"0\": torch.tensor([[0.0896, 0.9181],\n","                                       [0.8755, 0.4239],\n","                                       [0.3665, 0.5990],\n","                                       [0.0960, 0.3615],\n","                                       [0.7895, 0.0670],\n","                                       [0.3407, 0.6902],\n","                                       [0.4052, 0.3058],\n","                                       [0.4820, 0.6540],\n","                                       [0.9083, 0.2075],\n","                                       [0.2015, 0.3533]])\n","                    }\n","               }\n","\n","output = persistence_diagrams_to_sequence(tensor_dict)\n","\n","expected_output = {1: torch.tensor(\n","        [[0.0066, 0.7961, 1.0000, 0.0000],\n","         [0.6612, 0.0359, 1.0000, 0.0000],\n","         [0.8394, 0.1597, 1.0000, 0.0000],\n","         [0.0932, 0.7327, 0.0000, 1.0000],\n","         [0.7248, 0.7940, 0.0000, 1.0000],\n","         [0.5550, 0.9960, 0.0000, 1.0000]]),\n"," 2: torch.tensor(\n","        [[0.1787, 0.1809, 1.0000, 0.0000],\n","         [0.2645, 0.5766, 1.0000, 0.0000],\n","         [0.5666, 0.1630, 1.0000, 0.0000],\n","         [0.9986, 0.0259, 1.0000, 0.0000],\n","         [0.9541, 0.6892, 0.0000, 1.0000],\n","         [0.7984, 0.8061, 0.0000, 1.0000],\n","         [0.5266, 0.0644, 0.0000, 1.0000],\n","         [0.0630, 0.2176, 0.0000, 1.0000]]),\n"," 0: torch.tensor(\n","        [[0.6910, 0.1265, 1.0000, 0.0000],\n","         [0.9085, 0.0230, 1.0000, 0.0000],\n","         [0.4977, 0.6386, 1.0000, 0.0000],\n","         [0.1331, 0.8196, 1.0000, 0.0000],\n","         [0.6929, 0.1859, 1.0000, 0.0000],\n","         [0.4216, 0.2283, 1.0000, 0.0000],\n","         [0.4996, 0.3380, 1.0000, 0.0000],\n","         [0.0896, 0.9181, 0.0000, 1.0000],\n","         [0.8755, 0.4239, 0.0000, 1.0000],\n","         [0.3665, 0.5990, 0.0000, 1.0000],\n","         [0.0960, 0.3615, 0.0000, 1.0000],\n","         [0.7895, 0.0670, 0.0000, 1.0000],\n","         [0.3407, 0.6902, 0.0000, 1.0000],\n","         [0.4052, 0.3058, 0.0000, 1.0000],\n","         [0.4820, 0.6540, 0.0000, 1.0000],\n","         [0.9083, 0.2075, 0.0000, 1.0000],\n","         [0.2015, 0.3533, 0.0000, 1.0000]])}\n","\n","for i in range(3):\n","    try:\n","        assert(torch.allclose(output[i], expected_output[i]))\n","    except AssertionError:\n","        print(\"expected:\\n\", expected_output[i])\n","        print(\"actual:\\n\", output[i])\n","        raise AssertionError(\"persistence_diagrams_to_sequence does not match\")\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def diagram_to_tensor(\n","    tensor_dict_per_type: Dict[str, torch.Tensor]\n","        ) -> torch.Tensor:\n","    \"\"\"Convert dictionary of diagrams for fixed type to tensor representation\n","    with tailing zeros\n","\n","    Args:\n","        tensor_dict (Dict[str, torch.Tensor]): Dictionary of persistence\n","            diagrams of a fixed type. Keys are strings of graph indices and\n","            values are tensor representations of persistence diagrams.\n","            The keys are assumed to be in range(len(tensor_dict_per_type)).\n","\n","    Returns:\n","        torch.Tensor: [description]\n","    \"\"\"\n","    try:\n","        assert all([int(k) in range(len(tensor_dict_per_type))\n","                    for k in tensor_dict_per_type.keys()])\n","    except AssertionError:\n","        print(\"Tensor dictionary should contain all keys in\",\n","              \"range(len(tensor_dict_per_type))\")\n","        raise\n","    max_number_of_points = max([v.shape[0]\n","                                for v in tensor_dict_per_type.values()])\n","\n","    diagram_tensor = torch.zeros((\n","                            len(tensor_dict_per_type),\n","                            max_number_of_points,\n","                            2\n","                        ))\n","    for graph_idx, diagram in tensor_dict_per_type.items():\n","        # number of points in persistence diagram\n","        npts = tensor_dict_per_type[graph_idx].shape[0]\n","        diagram_tensor[int(graph_idx)][:npts] = tensor_dict_per_type[graph_idx]\n","\n","    return diagram_tensor\n","\n","\n","# check if tensorised diagrams have the correct shape\n","\n","def test_diagram_to_tensor():\n","    try:\n","        assert all((\n","                    diagram_to_tensor(\n","                        tensor_dict[\"type1\"]).shape == torch.Size([3, 7, 2]),\n","                    diagram_to_tensor(\n","                        tensor_dict[\"type2\"]).shape == torch.Size([3, 10, 2])\n","                ))\n","    except AssertionError:\n","        print(\"Converted diagrams do not have correct shape.\")\n","        raise\n","\n","test_diagram_to_tensor()\n","\n","    # n_types = len(tensor_dict)  # number of diagram types\n","\n","    # diagrams_list = []\n","    # for type_ in tensor_dict:\n","    #     diagrams_list.append(diagram_to_tensor(tensor_dict[type_]))\n","\n","    # max_number_of_points_per_type = max([diagram.shape[1] for\n","    #                                     diagram in diagrams_list])\n","\n","    # data = []\n","\n","    # for type_idx, diagram in enumerate(diagrams_list):\n","    #     # diagram tensor with one-hot encoding in the zeroth coordinate\n","    #     # and a dimension in the second coordinate that fits the number\n","    #     # of points in the diagrams for all types\n","    #     diagram_tensor = torch.zeros((\n","    #                         diagram.shape[0],\n","    #                         max_number_of_points_per_type,\n","    #                         2\n","    #                     ))\n","    #     # shape: [graph_idx]\n","    #     diagram_cat = torch.tensor([type_idx] * diagram.shape[0], dtype=torch.int32)\n","    #     # shape: [graph_idx, point_idx, coordinate]\n","    #     diagram_tensor[:, :diagram.shape[1], :] = diagram\n","        \n","    #     try:\n","    #     if type_idx == 0:\n","    #         # shape \n","    #         data = \n","    #     print(diagram_tensor.shape)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set up dataset and dataloader\n","\n","# transform x_pds to a single tensor with tailing zeros\n","num_types = x_pds_dict[0].shape[1] - 2\n","num_graphs = len(x_pds_dict.keys())\n","\n","max_number_of_points = max([x_pd.shape[0]\n","                            for _, x_pd in x_pds_dict.items()])\n","\n","x_pds = torch.zeros((num_graphs, max_number_of_points, num_types + 2))\n","\n","for idx, x_pd in x_pds_dict.items():\n","    x_pds[idx, :x_pd.shape[0], :] = x_pd\n","\n","# https://discuss.pytorch.org/t/make-a-tensordataset-and-dataloader\n","# -with-multiple-inputs-parameters/26605\n","total_size = x_pds.shape[0]\n","\n","graph_ds = TensorDataset(x_pds, x_features, y)\n","\n","train_size = int(total_size * 0.8)\n","graph_ds_train, graph_ds_val = torch.utils.data.random_split(\n","                                                    graph_ds,\n","                                                    [train_size,\n","                                                    total_size - train_size])\n","\n","graph_dl_train = DataLoader(\n","    graph_ds_train,\n","    batch_size=64,\n","    shuffle=True\n",")\n","\n","graph_dl_val = DataLoader(\n","    graph_ds_val,\n","    batch_size=64,\n","    shuffle=False\n",")\n","\n","# for batch_idx, (x_pd, x_feature, label) in enumerate(mutag_dl):\n","#     print(batch_idx, x_pd.shape, x_feature.shape, label.shape)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# st = SetTransformer(\n","#         dim_input=mutag_ds[0][0].shape[1],\n","#         num_outputs=1,\n","#         dim_output=10\n","#         )\n","# st(mutag_ds[0][0].unsqueeze(0))\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train Persformer on MUTAG dataset\n","\n","class GraphClassifier(nn.Module):\n","    \"\"\"Classifier for Graphs using persistence features and additional\n","    features. The vectorization is based on a set transformer.\n","    \"\"\"\n","    def __init__(self,\n","                 num_features,\n","                 dim_input=6,\n","                 num_outputs=1,\n","                 dim_output=10,\n","                 num_classes=2):\n","        super(GraphClassifier, self).__init__()\n","        self.st = SetTransformer(\n","            dim_input=dim_input,\n","            num_outputs=num_outputs,\n","            dim_output=dim_output\n","            )\n","        self.num_classes = num_classes\n","        self.bn = nn.BatchNorm1d(dim_output + num_features)\n","        self.ff_1 = nn.Linear(dim_output + num_features, 50)\n","        self.ff_2 = nn.Linear(50, 20)\n","        self.ff_3 = nn.Linear(20, num_classes)\n","        \n","        \n","    def forward(self, x_pd: Tensor, x_feature: Tensor) -> Tensor:\n","        \"\"\"Forward pass of the graph classifier.\n","        The persistence features are encoded with a set transformer\n","        and concatenated with the feature vector. These concatenated\n","        features are used for classification using a fully connected\n","        feed -forward layer.\n","\n","        Args:\n","            x_pd (Tensor): persistence diagrams of the graph\n","            x_feature (Tensor): additional graph features\n","        \"\"\"\n","        pd_vector = self.st(x_pd)\n","        features_stacked = torch.hstack((pd_vector, x_feature))\n","        x = self.bn(features_stacked)\n","        x = nn.ReLU()(self.ff_1(features_stacked))\n","        x = nn.ReLU()(self.ff_2(x))\n","        x = self.ff_3(x)\n","        return x\n","        \n","gc = GraphClassifier(\n","        num_features=graph_ds_train[0][1].shape[0],\n","        dim_input=graph_ds_train[0][0].shape[1],\n","        num_outputs=1,\n","        dim_output=10)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x_pd, x_feature, y= next(iter(graph_dl_train))\n","# print(x_pd.shape)\n","gc(x_pd, x_feature).shape\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compute_accuracy(\n","                    model: nn.Module,\n","                    dl,\n","                    use_cuda: bool = False\n","                  ) -> Tuple[int, float]:\n","    \"\"\"Print the accuracy of the network on the dataset\n","    provided by the data loader.\n","\n","    Args:\n","        model (nn.Module): Model to be evaluated.\n","        dataloader ([type]): dataloader of the dataset the model is being\n","            evaluated.\n","        use_cuda (bool, optional): If the model is on GPU. Defaults to False.\n","    \"\"\"\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    for x_pd, x_feature, label in dl:\n","        outputs = model(x_pd, x_feature).squeeze(1)\n","        _, predictions = torch.max(outputs, 1)\n","        total += label.size(0)\n","        correct += (predictions == label).sum().item()\n","\n","    return (total, 100 * correct/total)\n","\n","def train(model, train_dl, val_dl, criterion=nn.CrossEntropyLoss(),\n","          lr: float = 1e-3, num_epochs=10,\n","          verbose=False):\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    losses: List[float] = []\n","    for epoch in range(num_epochs):\n","        model.train()\n","        loss_per_epoch = 0\n","        for batch_idx, (x_pd, x_feature, label) in enumerate(train_dl):\n","            loss = criterion(model(x_pd, x_feature), label.long())\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            loss_per_epoch += loss.item()\n","        losses.append(loss_per_epoch)\n","        if verbose:\n","            # print train loss, test and model accuracy\n","            print(\"epoch:\", epoch, \"loss:\", loss_per_epoch)\n","            test_total, test_accuracy = compute_accuracy(model,\n","                                                         train_dl,\n","                                                         val_dl)\n","            print('Test',\n","                    'accuracy of the network on the', test_total,\n","                    'diagrams: %8.2f %%' % test_accuracy\n","                    )\n","            if val_dl is not None:\n","                val_total, val_accuracy = compute_accuracy(model,\n","                                                             val_dl)\n","                print('Val',\n","                        'accuracy of the network on the', val_total,\n","                        'diagrams: %8.2f %%' % val_accuracy\n","                        )\n","    return losses\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gc.st.enc[0].mab0.fc_q.weight"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["        \n","losses = train(gc, graph_dl_train, graph_dl_val, verbose=True, num_epochs=100)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gc.st.enc[0].mab0.fc_q.weight\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.plot(losses)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use SAM optimizer \n","from sam import SAM\n","\n","def sam_train(model, train_dl, val_dl, criterion=nn.CrossEntropyLoss(),\n","          lr: float = 1e-3, num_epochs=10,\n","          verbose=False):\n","    base_optimizer = torch.optim.SGD\n","    optimizer = SAM(model.parameters(), base_optimizer, lr=0.01, momentum=0.9)\n","    gc.train()\n","    losses: List[float] = []\n","    for epoch in range(num_epochs):\n","        loss_per_epoch = 0\n","        for batch_idx, (x_pd, x_feature, label) in enumerate(train_dl):\n","            \n","            # first forward-backward pass\n","            loss = criterion(model(x_pd, x_feature), label.long())\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.first_step(zero_grad=True)\n","            # second forward-backward pass\n","            criterion(model(x_pd, x_feature), label.long()).backward()\n","            optimizer.second_step(zero_grad=True)\n","            \n","            loss_per_epoch += loss.item()\n","        losses.append(loss_per_epoch)\n","        if verbose:\n","            # print train loss, test and model accuracy\n","            print(\"epoch:\", epoch, \"loss:\", loss_per_epoch)\n","            test_total, test_accuracy = compute_accuracy(model,\n","                                                         train_dl,\n","                                                         val_dl)\n","            print('Test',\n","                    'accuracy of the network on the', test_total,\n","                    'diagrams: %8.2f %%' % test_accuracy\n","                    )\n","            if val_dl is not None:\n","                val_total, val_accuracy = compute_accuracy(model,\n","                                                             val_dl)\n","                print('Val',\n","                        'accuracy of the network on the', val_total,\n","                        'diagrams: %8.2f %%' % val_accuracy\n","                        )\n","    return losses\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["losses = sam_train(gc, graph_dl_train, graph_dl_val, verbose=True, num_epochs=200)\n","\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","filename = join(\"graph_data\",\n","                \"MUTAG\",\n","                \"MUTAG\" + \".hdf5\")\n","diagrams_file = h5py.File(filename, \"r\")\n","\n","for x, v in diagrams_file['Ext1_10.0-hks'].items():\n","    if np.isclose(np.array(v)[0, 0], 0.0657, atol=1e-4):\n","        print(x, np.array(v))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter\n","import numpy as np\n","\n","writer = SummaryWriter(log_dir=\"summaries\")\n","\n","for n_iter in range(100):\n","    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n","    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n","    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n","    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for n_iter in range(100):\n","    writer.add_scalar('Loss/train', np.random.random(), n_iter)\n","    writer.add_scalar('Loss/test', np.random.random(), n_iter)\n","    writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n","    writer.add_scalar('Accuracy/test', np.random.random(), n_iter)#"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tensorboard --logdir='data'"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from pandas_datareader import data\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import datetime as dt\n","import urllib.request, json\n","import os\n","import numpy as np\n","\n","# This code has been tested with TensorFlow 1.6\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%tensorboard --logdir summaries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%load_ext tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","import datetime, os"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fashion_mnist = tf.keras.datasets.fashion_mnist\n","\n","(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_model():\n","  return tf.keras.models.Sequential([\n","    tf.keras.layers.Flatten(input_shape=(28, 28)),\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    tf.keras.layers.Dropout(0.2),\n","    tf.keras.layers.Dense(10, activation='softmax')\n","  ])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_model():\n","  \n","  model = create_model()\n","  model.compile(optimizer='adam',\n","                loss='sparse_categorical_crossentropy',\n","                metrics=['accuracy'])\n","\n","  logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","  tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","\n","  model.fit(x=x_train, \n","            y=y_train, \n","            epochs=5, \n","            validation_data=(x_test, y_test), \n","            callbacks=[tensorboard_callback])\n","\n","train_model()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%tensorboard --logdir logs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":4}}