{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: Question answer\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with the basic functioning of *giotto-deep* API.\n",
    "\n",
    "The example described in this tutorial is the one of question answer.\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. creation of a dataset\n",
    " 2. creation of a model\n",
    " 3. define metrics and losses\n",
    " 4. train the model\n",
    " 5. extract some features of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from gdeep.models import FFNet\n",
    "\n",
    "from gdeep.visualisation import persistence_diagrams_of_activations\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gdeep.data.datasets import DatasetBuilder\n",
    "from gdeep.trainer import Trainer\n",
    "\n",
    "from gtda.diagrams import BettiCurve\n",
    "\n",
    "from gtda.plotting import plot_betti_surfaces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the reuslts of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "bd = DatasetBuilder(name=\"SQuAD2\", convert_to_map_dataset=True)\n",
    "ds_tr_str, ds_val_str, ds_ts_str = bd.build()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a context and a question whose answer can be found within that context. The correct answer as well as the starting characters are also provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before preprocessing: \\n\", ds_tr_str[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required preprocessing\n",
    "\n",
    "Neural networks cannot direcly deal with strings. We have first to preprocess the dataset in three main ways:\n",
    " 1. Tokenise the strings into its words\n",
    " 2. Build a vocabulary out of these words\n",
    " 3. Embed each word into a vector, so that each sentence becomes a list of vectors\n",
    "\n",
    "The first two steps are performed by the `PreprocessTextQA`. The embedding will be added directly to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.data import PreprocessingPipeline\n",
    "from gdeep.data import TransformingDataset\n",
    "from gdeep.data.preprocessors import Normalization, TokenizerQA\n",
    "from gdeep.data.datasets import DataLoaderBuilder\n",
    "\n",
    "\n",
    "tokenizer = TokenizerQA()\n",
    "\n",
    "# in case you need to combine multiple preprocessing:\n",
    "# ppp = PreprocessingPipeline(((PreprocessTextData(), IdentityTransform(), TextDataset),\n",
    "#                             (Normalisation(), IdentityTransform(), BasicDataset)))\n",
    "\n",
    "\n",
    "tokenizer.fit_to_dataset(ds_tr_str)\n",
    "transformed_textds = tokenizer.attach_transform_to_dataset(ds_tr_str)\n",
    "\n",
    "transformed_textts = tokenizer.attach_transform_to_dataset(\n",
    "    ds_val_str\n",
    ")  # this has been fitted on the train set!\n",
    "\n",
    "print(\"After the preprocessing: \\n\", transformed_textds[0])\n",
    "\n",
    "# the only part of the training/test set we are interested in\n",
    "train_indices = list(range(64 * 2))\n",
    "test_indices = list(range(64 * 1))\n",
    "\n",
    "dl_tr2, dl_ts2, _ = DataLoaderBuilder((transformed_textds, transformed_textts)).build(\n",
    "    (\n",
    "        {\"batch_size\": 16, \"sampler\": SubsetRandomSampler(train_indices)},\n",
    "        {\"batch_size\": 16, \"sampler\": SubsetRandomSampler(test_indices)},\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train your model\n",
    "\n",
    "The model for QA shall accept as input the context and the question and return the probabilities for the initial and final token of the answer in the input context. The output than, is a pair of logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Transformer\n",
    "from torch.optim import Adam, SparseAdam, SGD\n",
    "import copy\n",
    "\n",
    "# my simple transformer model\n",
    "class QATransformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim):\n",
    "        super(QATransformer, self).__init__()\n",
    "        self.transformer = Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=2,\n",
    "            num_encoder_layers=1,\n",
    "            num_decoder_layers=1,\n",
    "            dim_feedforward=512,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "        self.embedding_src = nn.Embedding(src_vocab_size, embed_dim, sparse=True)\n",
    "        self.embedding_tgt = nn.Embedding(tgt_vocab_size, embed_dim, sparse=True)\n",
    "        self.generator = nn.Linear(embed_dim, 2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # print(X.shape)\n",
    "        src = X[:, 0, :]\n",
    "        tgt = X[:, 1, :]\n",
    "        # print(src.shape, tgt.shape)\n",
    "        src_emb = self.embedding_src(src)\n",
    "        tgt_emb = self.embedding_tgt(tgt)\n",
    "        # print(src_emb.shape, tgt_emb.shape)\n",
    "        self.outs = self.transformer(src_emb, tgt_emb)\n",
    "        # print(outs.shape)\n",
    "        logits = self.generator(self.outs)\n",
    "        # print(logits.shape)\n",
    "        # out = torch.topk(logits, k=1, dim=2).indices.reshape(-1,44)\n",
    "        # print(out, out.shape)\n",
    "        return logits\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        \"\"\"this is needed to make sure that the \n",
    "        non-leaf nodes do not\n",
    "        interfere with copy.deepcopy()\n",
    "        \"\"\"\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        memo[id(self)] = result\n",
    "        for k, v in self.__dict__.items():\n",
    "            setattr(result, k, copy.deepcopy(v, memo))\n",
    "        return result\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"this method is used only at the inference step\"\"\"\n",
    "        return self.transformer.encoder(self.embedding_src(src), src_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        \"\"\"this method is used only at the inference step\"\"\"\n",
    "        return self.transformer.decoder(self.embedding_tgt(tgt), memory, tgt_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1500000\n",
    "\n",
    "src_vocab_size = vocab_size\n",
    "tgt_vocab_size = vocab_size\n",
    "emb_size = 64\n",
    "\n",
    "model = QATransformer(src_vocab_size, tgt_vocab_size, emb_size)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "This loss function is a adapted version of the Cross Entropy for the trnasformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(output_of_network, label_of_dataloader):\n",
    "    # print(output_of_network.shape, label_of_dataloader.shape)\n",
    "    tgt_out = label_of_dataloader\n",
    "    # print(tgt_out)\n",
    "    logits = output_of_network\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    return cel(logits, tgt_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a pipeline class with the model, dataloaders loss_fn and tensorboard writer\n",
    "pipe = Trainer(model, (dl_tr2, dl_ts2), loss_fn, writer)\n",
    "\n",
    "# train the model\n",
    "pipe.train(SGD, 3, False, {\"lr\": 0.01}, {\"batch_size\": 16})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering questions!\n",
    "\n",
    "Here we have a question and its associated context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = next(iter(ds_val_str))\n",
    "bb[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Get the vocabulary and numericize the question and context to then input both to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = tokenizer.vocabulary\n",
    "context = tokenizer.tokenizer(bb[0])\n",
    "question = tokenizer.tokenizer(bb[1])\n",
    "\n",
    "# get the indexes in the vocabulary of the tokens\n",
    "context_idx = torch.tensor(list(map(voc.__getitem__, context)))\n",
    "question_idx = torch.tensor(list(map(voc.__getitem__, question)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = next(iter(dl_tr2))\n",
    "length_to_pad = aa[0].shape[-1]\n",
    "pad_fn = lambda item: torch.cat(\n",
    "    [item, tokenizer.pad_item * torch.ones(length_to_pad - item.shape[0])]\n",
    ").to(torch.long)\n",
    "\n",
    "# these tansors are ready to be fitted into the model\n",
    "context_ready_for_model = pad_fn(context_idx)\n",
    "question_ready_for_model = pad_fn(question_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the two tensors of context and question together and input them to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = (\n",
    "    torch.stack((context_ready_for_model, question_ready_for_model))\n",
    "    .reshape(1, *aa[0].shape[1:])\n",
    "    .long()\n",
    ")\n",
    "out = model(inp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output corresponds to the digits for the start and end tokens of the answer. It is now time to extract them with `torch.argmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_idx = torch.argmax(out, dim=1)\n",
    "\n",
    "try:\n",
    "    if answer_idx[0][1] > answer_idx[0][0]:\n",
    "        print(\n",
    "            \"The model proposes: '\",\n",
    "            context[answer_idx[0][0] : answer_idx[0][1]],\n",
    "            \"...'\",\n",
    "        )\n",
    "    else:\n",
    "        print(\"The model proposes: '\", context[answer_idx[0][0]], \"...'\")\n",
    "except IndexError:\n",
    "    print(\"The model was not able to find the answer.\")\n",
    "print(\"The actual answer was: '\" + bb[2][0] + \"'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract inner data from your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.models import ModelExtractor\n",
    "\n",
    "me = ModelExtractor(pipe.model, loss_fn)\n",
    "\n",
    "lista = me.get_layers_param()\n",
    "\n",
    "for k, item in lista.items():\n",
    "    print(k, item.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cpu\")\n",
    "x = next(iter(dl_tr2))[0]\n",
    "pipe.model.eval()\n",
    "pipe.model(x.to(DEVICE))\n",
    "\n",
    "list_activations = me.get_activations(x)\n",
    "len(list_activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(dl_tr2))[0][0]\n",
    "if x.dtype is not torch.int64:\n",
    "    res = me.get_decision_boundary(x, n_epochs=1)\n",
    "    res.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, target = next(iter(dl_tr2))\n",
    "if x.dtype is torch.float:\n",
    "    for gradient in me.get_gradients(x, target=target)[1]:\n",
    "        print(gradient.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise activations and other topological aspects of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.visualisation import Visualiser\n",
    "\n",
    "vs = Visualiser(pipe)\n",
    "\n",
    "vs.plot_data_model()\n",
    "# vs.plot_activations(x)\n",
    "# vs.plot_persistence_diagrams(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}