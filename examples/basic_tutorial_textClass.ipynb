{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: text data\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with the basic functioning of *giotto-deep* API.\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. creation of a dataset\n",
    " 2. creation of a model\n",
    " 3. define metrics and losses\n",
    " 4. run benchmarks\n",
    " 5. visualise results interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from gdeep.models import FFNet\n",
    "\n",
    "from gdeep.visualisation import  persistence_diagrams_of_activations\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gdeep.data import TorchDataLoader\n",
    "\n",
    "from gtda.diagrams import BettiCurve\n",
    "\n",
    "from gtda.plotting import plot_betti_surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the reuslts of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# the only part of the training set we are interested in\n",
    "train_indices = list(range(64*20))\n",
    "\n",
    "# many time we get an IterableDataset which is good for memory consumption, but cannot be sampled!\n",
    "# we can load the whole dataset in memory and sample it\n",
    "dl = TorchDataLoader(name=\"AG_NEWS\", convert_to_map_dataset=True)\n",
    "dl_tr, dl_ts = dl.build_dataloaders(sampler=SubsetRandomSampler(train_indices))\n",
    "\n",
    "# for (_label, _text) in dl_tr:\n",
    "#     print(_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.data import PreprocessText\n",
    "\n",
    "prec = PreprocessText((dl_tr, dl_ts))\n",
    "\n",
    "(dl_tr, dl_ts) = prec.build_dataloaders()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.pipeline import Pipeline\n",
    "from torch.optim import Adam, SparseAdam, SGD\n",
    "from captum.attr import Occlusion, IntegratedGradients, visualization\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        mean = torch.mean(embedded,dim=1)\n",
    "        return self.fc(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(prec.vocabulary)\n",
    "print(\"len\", vocab_size)\n",
    "vocab_size = 225981 # to be discussed, since len(prec.vocabulary) is returning 95810 whereas the index of '.' is 225971\n",
    "emsize = 64\n",
    "# print(vocab_size, emsize)\n",
    "model = TextClassificationModel(vocab_size, emsize, 4)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "pipe = Pipeline(model, (dl_tr, dl_ts), loss_fn, writer)\n",
    "\n",
    "# train the model\n",
    "pipe.train(SGD, 7, False, {\"lr\":0.01}, {\"batch_size\":20})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simply use interpretability tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.analysis.interpretability import Interpreter\n",
    "from gdeep.visualisation import Visualiser\n",
    "\n",
    "vs = Visualiser(pipe)\n",
    "inter = Interpreter(pipe.model, method=\"LayerIntegratedGradients\")\n",
    "\n",
    "inter.interpret_text(\"I am writing about money and business\", 0, prec.vocabulary, prec.tokenizer, layer=pipe.model.embedding)\n",
    "\n",
    "vs.plot_interpreter_text(inter);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract inner data from your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.models import ModelExtractor\n",
    "\n",
    "me = ModelExtractor(model, loss_fn)\n",
    "\n",
    "lista = me.get_layers_param()\n",
    "\n",
    "for k, item in lista.items():\n",
    "    print(k,item.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(dl_tr))[0][0]\n",
    "if x.dtype is not torch.int64:\n",
    "    res = me.get_decision_boundary(x, n_epochs=1)\n",
    "    res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(dl_tr))[0]\n",
    "list_activations = me.get_activations(x)\n",
    "len(list_activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, target = next(iter(dl_tr))\n",
    "if x.dtype is torch.float:\n",
    "    for gradient in me.get_gradients(x, target=target)[1]:\n",
    "        print(gradient.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise activations and other topological aspects of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vs.plot_data_model()\n",
    "vs.plot_activations(x)\n",
    "vs.plot_persistence_diagrams(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.betti_plot_layers((0, 1), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
