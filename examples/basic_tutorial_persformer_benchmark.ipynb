{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: Training a topological model\n",
    "### Author: Raphael Reinauer\n",
    "### Date: 2022-04-05\n",
    " This short tutorial shows how to use the GDeep framework to train topological\n",
    " models using the topological datasets provided by the GDeep dataset cloud.\n",
    " The main steps of the tutorial are the following:\n",
    " 1. Specify the dataset you want to use.\n",
    " 2. Specify the model and the hyperparameter space you want to use.\n",
    " 3. Run a large scale hyperparameter search to find the good hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPUs...\n"
     ]
    }
   ],
   "source": [
    "# This snippet will deactivate autoreload if this file\n",
    "# is run as a script and activate it if it is run as a notebook.\n",
    "from gdeep.utility.utils import autoreload_if_notebook\n",
    "\n",
    "autoreload_if_notebook()\n",
    "# Include necessary imports\n",
    "from os.path import join\n",
    "\n",
    "# Import the GDeep hpo module\n",
    "from gdeep.search import PersformerHyperparameterSearch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training a topological model with the Dataset Cloud\n",
    " In this tutorial we will use the our custom datasets storage\n",
    " on [Google Cloud Datastore](https://cloud.google.com/datastore/) to\n",
    " load datasets and train a topological model.\n",
    " The dataset cloud storage contain a variety of topological datasets\n",
    " that can be easily used in GDeep.\n",
    " We will use the Mutag dataset from the\n",
    " [Mutagenicity Benchmark](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5276825/)\n",
    " to do performance benchmarking for the Persformer model.\n",
    " For this benchmark we will use the GDeep\n",
    " [Persformer](https://doi.org/10.48550/arXiv.2112.15210) model,\n",
    " the GDeep pipeline and the GDeep hyperparameter search.\n",
    " With only a few lines of code we can train multiple topological models\n",
    " with different hyperparameters and evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 20:39:52,331]\u001b[0m A new study created in memory with name: M9B6QOBYUDBQJVNMX5L8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'MutagDataset' already downloaded\n",
      "\n",
      "\n",
      "********** Fold  1 **************\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.708660 \tEpoch training accuracy: 34.04%                                                          \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.00000000\n",
      "Validation results: \n",
      " Accuracy: 32.98%,                 Avg loss: 0.712919 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.693695 \tEpoch training accuracy: 43.62%                                                          \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.00322646\n",
      "Validation results: \n",
      " Accuracy: 67.02%,                 Avg loss: 0.666268 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.634310 \tEpoch training accuracy: 65.96%                                                         \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.00645291\n",
      "Validation results: \n",
      " Accuracy: 67.02%,                 Avg loss: 0.629491 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.646320 \tEpoch training accuracy: 65.96%                                                         \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.00967937\n",
      "Validation results: \n",
      " Accuracy: 67.02%,                 Avg loss: 0.655370 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.629570 \tEpoch training accuracy: 65.96%                                                         \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01290583\n",
      "Validation results: \n",
      " Accuracy: 67.02%,                 Avg loss: 0.593210 \n",
      "\n",
      "\n",
      "\n",
      "********** Fold  2 **************\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.709529 \tEpoch training accuracy: 32.98%                                                         \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.00000000\n",
      "Validation results: \n",
      " Accuracy: 34.04%,                 Avg loss: 0.714980 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.693154 \tEpoch training accuracy: 51.06%                                                          \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.00322646\n",
      "Validation results: \n",
      " Accuracy: 65.96%,                 Avg loss: 0.667829 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.627086 \tEpoch training accuracy: 67.02%                                                         \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.00645291\n",
      "Validation results: \n",
      " Accuracy: 65.96%,                 Avg loss: 0.634352 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Batch training loss:  0.6319609880447388  \tBatch training accuracy:  64.28571428571429  \t[ 1 / 4 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-07 20:39:52,830]\u001b[0m Trial 0 finished with value: 0.6648936170212766 and parameters: {'optimizer': 'AdamW', 'lr': 0.019358743359385245, 'weight_decay': 2.3570364049932964e-06, 'batch_size': 28, 'dropout_dec': 0.1, 'dropout_enc': 0.4, 'dim_input': 6, 'n_layer_dec': 1, 'n_layer_enc': 2, 'dim_output': 2, 'activation': 'gelu', 'bias_attention': 'False', 'hidden_dim': '16', 'num_inds': '16', 'layer_norm': 'False', 'layer_norm_pooling': 'False', 'num_heads': '2', 'attention_type': 'self_attention', 'num_cycles': 1, 'num_training_steps': 5, 'num_warmup_steps': 6}. Best is trial 0 with value: 0.6648936170212766.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.641869 \tEpoch training accuracy: 67.02%                                                         \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.00967937\n",
      "Validation results: \n",
      " Accuracy: 65.96%,                 Avg loss: 0.615109 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.648341 \tEpoch training accuracy: 67.02%                                                        \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01290583\n",
      "Validation results: \n",
      " Accuracy: 65.96%,                 Avg loss: 0.602573 \n",
      "\n",
      "******************** RESULTS ********************\n",
      " \n",
      "Model:  Persformer \n",
      "Model Hyperparameters: {'activation': 'gelu', 'bias_attention': 'False', 'hidden_dim': '16', 'num_inds': '16', 'layer_norm': 'False', 'layer_norm_pooling': 'False', 'num_heads': '2', 'attention_type': 'self_attention', 'dropout_dec': 0.1, 'dropout_enc': 0.4, 'dim_input': 6, 'n_layer_dec': 1, 'n_layer_enc': 2, 'dim_output': 2}\n",
      "Optimizer: AdamW (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.019358743359385245\n",
      "    lr: 0.016132286132821038\n",
      "    weight_decay: 2.3570364049932964e-06\n",
      ")\n",
      "Optimizer parameters: {'lr': 0.019358743359385245, 'weight_decay': 2.3570364049932964e-06}\n",
      "Dataloader parameters: {'batch_size': 28}\n",
      "LR-scheduler parameters: {'num_cycles': 1, 'num_training_steps': 5, 'num_warmup_steps': 6}\n",
      "Best Validation accuracy: 67.02127659574468\n",
      "Study statistics: \n",
      "Number of pruned trials:  0\n",
      "Number of complete trials:  1\n",
      "******************** BEST TRIAL: ********************\n",
      "Metric Value for best trial:  0.6648936170212766\n",
      "Parameters Values for best trial:  {'optimizer': 'AdamW', 'lr': 0.019358743359385245, 'weight_decay': 2.3570364049932964e-06, 'batch_size': 28, 'dropout_dec': 0.1, 'dropout_enc': 0.4, 'dim_input': 6, 'n_layer_dec': 1, 'n_layer_enc': 2, 'dim_output': 2, 'activation': 'gelu', 'bias_attention': 'False', 'hidden_dim': '16', 'num_inds': '16', 'layer_norm': 'False', 'layer_norm_pooling': 'False', 'num_heads': '2', 'attention_type': 'self_attention', 'num_cycles': 1, 'num_training_steps': 5, 'num_warmup_steps': 6}\n",
      "DateTime start of the best trial:  2022-04-07 20:39:52.333166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matteocaorsi/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2634: RuntimeWarning:\n",
      "\n",
      "Degrees of freedom <= 0 for slice\n",
      "\n",
      "/Users/matteocaorsi/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2487: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in subtract\n",
      "\n",
      "/Users/matteocaorsi/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2493: RuntimeWarning:\n",
      "\n",
      "divide by zero encountered in true_divide\n",
      "\n",
      "/Users/matteocaorsi/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/function_base.py:2493: RuntimeWarning:\n",
      "\n",
      "invalid value encountered in multiply\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is how you use the api to search for the best hyperparameters for\n",
    "# the MutagDataset using the PersformerHyperparameterSearch class.\n",
    "# The search is performed using the hyperparameter\n",
    "# search space described in hpo_space file provided.\n",
    "# Please customize the file to your own dataset.\n",
    "# The results are written to the path_writer directory.\n",
    "\n",
    "dataset_name=\"MutagDataset\"  # name of the dataset - has to exist in the datacloud buckets\n",
    "download_directory = join(\"data\", \"DatasetCloud\")  # directory where the dataset is downloaded\n",
    "path_hpo_metadata = join('hpo_space', 'Mutag_hyperparameter_space.json')  # file describing the hyperparameter search space\n",
<<<<<<< HEAD
    "path_writer = join(\"runs\", \"auto_ml\")  # directory where the runs are stored using the tensorboard writer\n",
=======
    "path_writer = join(\"run\", \"auto_ml\")  # directory where the runs are stored using the tensorboard writer\n",
>>>>>>> persformer_benchmark
    "\n",
    "# Initialize the search object with the search parameters.\n",
    "hpo = PersformerHyperparameterSearch(dataset_name=dataset_name,\n",
    "                               download_directory=download_directory,\n",
    "                               path_hpo_metadata=path_hpo_metadata,\n",
    "                               path_writer=path_writer)\n",
    "\n",
    "# Start the hyperparameter search.\n",
    "hpo.search()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
