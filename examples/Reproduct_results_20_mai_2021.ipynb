{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba2b8445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train tensor([[0.0000, 0.0013],\n",
      "        [0.0000, 0.0013],\n",
      "        [0.0000, 0.0017],\n",
      "        [0.0000, 0.0018],\n",
      "        [0.0000, 0.0019],\n",
      "        [0.0000, 0.0021],\n",
      "        [0.0000, 0.0022],\n",
      "        [0.0000, 0.0025],\n",
      "        [0.0000, 0.0026],\n",
      "        [0.0000, 0.0027]])\n",
      "validation tensor([[0.0000, 0.0016],\n",
      "        [0.0000, 0.0016],\n",
      "        [0.0000, 0.0023],\n",
      "        [0.0000, 0.0024],\n",
      "        [0.0000, 0.0027],\n",
      "        [0.0000, 0.0029],\n",
      "        [0.0000, 0.0029],\n",
      "        [0.0000, 0.0029],\n",
      "        [0.0000, 0.0029],\n",
      "        [0.0000, 0.0030]])\n",
      "model has 291589 trainable parameters.\n",
      "epoch: 0 loss: 128.70869326591492\n",
      "Test accuracy of the network on the 5000 diagrams:    19.46 %\n",
      "Validation accuracy of the network on the 5000 diagrams:    20.36 %\n",
      "epoch: 1 loss: 127.85614287853241\n",
      "Test accuracy of the network on the 5000 diagrams:    19.60 %\n",
      "Validation accuracy of the network on the 5000 diagrams:    20.28 %\n",
      "epoch: 2 loss: 127.76621544361115\n",
      "Test accuracy of the network on the 5000 diagrams:    20.14 %\n",
      "Validation accuracy of the network on the 5000 diagrams:    20.94 %\n",
      "epoch: 3 loss: 127.498077750206\n",
      "Test accuracy of the network on the 5000 diagrams:    19.74 %\n",
      "Validation accuracy of the network on the 5000 diagrams:    19.68 %\n",
      "epoch: 4 loss: 127.45840239524841\n",
      "Test accuracy of the network on the 5000 diagrams:    20.32 %\n",
      "Validation accuracy of the network on the 5000 diagrams:    19.34 %\n",
      "epoch: 5 loss: 127.63925552368164\n",
      "Test accuracy of the network on the 5000 diagrams:    20.68 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-b7ffa024bc23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;31m# %%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-b7ffa024bc23>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, lr, verbose)\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"epoch:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-b7ffa024bc23>\u001b[0m in \u001b[0;36mcompute_accuracy\u001b[0;34m(model, type)\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     print(type.capitalize(),\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from typing import List\n",
    "import numpy as np  # type: ignore\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import multiprocessing\n",
    "import os\n",
    "from einops import rearrange  # type: ignore\n",
    "from gtda.homology import WeakAlphaPersistence  # type: ignore\n",
    "from gtda.plotting import plot_diagram  # type: ignore\n",
    "from gdeep.topology_layers import ISAB, PMA\n",
    "\n",
    "# %%\n",
    "try:\n",
    "    assert os.path.isdir('./data/ORBIT5K')\n",
    "except AssertionError:\n",
    "    if not os.path.isdir('./data'):\n",
    "        os.mkdir('./data')\n",
    "    os.mkdir('./data/ORBIT5K')\n",
    "\n",
    "# If `use_precomputed_dgms` is `False` the ORBIT5K dataset will\n",
    "# be recomputed, otherwise the ORBIT5K dataset in the folder\n",
    "# `data/ORBIT5K` will be used\n",
    "use_precomputed_dgms = False\n",
    "\n",
    "dgms_filename = os.path.join('data', 'ORBIT5K',\n",
    "                             'alpha_persistence_diagrams.npy')\n",
    "dgms_filename_validation = os.path.join('data', 'ORBIT5K',\n",
    "                                        'alpha_persistence_diagrams_' +\n",
    "                                        'validation.npy')\n",
    "\n",
    "if use_precomputed_dgms:\n",
    "    try:\n",
    "        assert(os.path.isfile(dgms_filename))\n",
    "    except AssertionError:\n",
    "        print('File data/ORBIT5K/alpha_persistence_diagrams.npy',\n",
    "              ' does not exist.')\n",
    "    try:\n",
    "        assert(os.path.isfile(dgms_filename_validation))\n",
    "    except AssertionError:\n",
    "        print('File data/ORBIT5K/alpha_persistence_diagrams.npy',\n",
    "              ' does not exist.')\n",
    "\n",
    "# %%\n",
    "# Create ORBIT5K dataset like in the PersLay paper\n",
    "parameters = (2.5, 3.5, 4.0, 4.1, 4.3)  # different classes of orbits\n",
    "homology_dimensions = (0, 1)\n",
    "\n",
    "config = {\n",
    "    'parameters': parameters,\n",
    "    'num_classes': len(parameters),\n",
    "    'num_orbits': 1_000,  # number of orbits per class\n",
    "    'num_pts_per_orbit': 1_000,\n",
    "    'homology_dimensions': homology_dimensions,\n",
    "    'num_homology_dimensions': len(homology_dimensions),\n",
    "    'validation_percentage': 100,  # size of validation dataset relative\n",
    "    # to training\n",
    "}\n",
    "\n",
    "if not use_precomputed_dgms:\n",
    "    for dataset_type in ['train', 'validation']:\n",
    "        # Generate dataset consisting of 5 different orbit types with\n",
    "        # 1000 sampled data points each.\n",
    "        # This is the dataset ORBIT5K used in the PersLay paper\n",
    "        if dataset_type == 'train':\n",
    "            num_orbits = config['num_orbits']\n",
    "        else:\n",
    "            num_orbits = int(config['num_orbits']  # type: ignore\n",
    "                             * config['validation_percentage'] / 100)\n",
    "        x = np.zeros((\n",
    "                        config['num_classes'],  # type: ignore\n",
    "                        num_orbits,\n",
    "                        config['num_pts_per_orbit'],\n",
    "                        2\n",
    "                    ))\n",
    "\n",
    "        # generate dataset\n",
    "        for cidx, p in enumerate(config['parameters']):  # type: ignore\n",
    "            x[cidx, :, 0, :] = np.random.rand(num_orbits, 2)\n",
    "\n",
    "            for i in range(1, config['num_pts_per_orbit']):  # type: ignore\n",
    "                x_cur = x[cidx, :, i - 1, 0]\n",
    "                y_cur = x[cidx, :, i - 1, 1]\n",
    "\n",
    "                x[cidx, :, i, 0] = (x_cur + p * y_cur * (1. - y_cur)) % 1\n",
    "                x_next = x[cidx, :, i, 0]\n",
    "                x[cidx, :, i, 1] = (y_cur + p * x_next * (1. - x_next)) % 1\n",
    "\n",
    "        \"\"\"\n",
    "        # old non-parallel version\n",
    "        for cidx, p in enumerate(config['parameters']):  # type: ignore\n",
    "            for i in range(config['num_orbits']):  # type: ignore\n",
    "                x[cidx][i] = generate_orbit(\n",
    "                    num_pts_per_orbit=config['num_pts_per_orbit'],\n",
    "                    parameter=p\n",
    "                    )\n",
    "        \"\"\"\n",
    "\n",
    "        assert(not np.allclose(x[0, 0], x[0, 1]))\n",
    "\n",
    "        # compute weak alpha persistence\n",
    "        wap = WeakAlphaPersistence(\n",
    "                            homology_dimensions=config['homology_dimensions'],\n",
    "                            n_jobs=multiprocessing.cpu_count()\n",
    "                            )\n",
    "        # c: class, o: orbit, p: point, d: dimension\n",
    "        x_stack = rearrange(x, 'c o p d -> (c o) p d')  # stack classes\n",
    "        diagrams = wap.fit_transform(x_stack)\n",
    "        # shape: (num_classes * n_samples, n_features, 3)\n",
    "\n",
    "        # combine class and orbit dimensions\n",
    "        diagrams = rearrange(\n",
    "                                diagrams,\n",
    "                                '(c o) p d -> c o p d',\n",
    "                                c=config['num_classes']  # type: ignore\n",
    "                            )\n",
    "\n",
    "        # plot sample persistence diagrams for debugging\n",
    "        if(True):\n",
    "            plot_diagram(diagrams[1, 2])\n",
    "            plot_diagram(diagrams[2, 2])\n",
    "\n",
    "        # save dataset\n",
    "        if dataset_type == 'train':\n",
    "            with open(dgms_filename, 'wb') as f:\n",
    "                np.save(f, diagrams)\n",
    "        else:\n",
    "            with open(dgms_filename_validation, 'wb') as f:\n",
    "                np.save(f, diagrams)\n",
    "# %%\n",
    "# load dataset\n",
    "for dataset_type in ['train', 'validation']:\n",
    "\n",
    "    if dataset_type == 'train':\n",
    "        with open(dgms_filename, 'rb') as f:\n",
    "            x = np.load(f)\n",
    "    else:\n",
    "        with open(dgms_filename_validation, 'rb') as f:\n",
    "            x = np.load(f)\n",
    "\n",
    "    # c: class, o: orbit, p: point in persistence diagram,\n",
    "    # d: coordinates + homology dimension\n",
    "    x = rearrange(\n",
    "                    x,\n",
    "                    'c o p d -> (c o) p d',\n",
    "                    c=config['num_classes']  # type: ignore\n",
    "                )\n",
    "    # convert homology dimension to one-hot encoding\n",
    "    x = np.concatenate(\n",
    "        (\n",
    "            x[:, :, :2],\n",
    "            (np.eye(config['num_homology_dimensions'])\n",
    "             [x[:, :, -1].astype(np.int32)]),\n",
    "        ),\n",
    "        axis=-1)\n",
    "    # convert from [orbit, sequence_length, feature] to\n",
    "    # [orbit, feature, sequence_length] to fit to the\n",
    "    # input_shape of `SmallSetTransformer`\n",
    "    # x = rearrange(x, 'o s f -> o f s')\n",
    "\n",
    "    # generate labels\n",
    "    y_list = []\n",
    "    for i in range(config['num_classes']):  # type: ignore\n",
    "        y_list += [i] * config['num_orbits']  # type: ignore\n",
    "\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    # load dataset to PyTorch dataloader\n",
    "\n",
    "    x_tensor = torch.Tensor(x)\n",
    "    y_tensor = torch.Tensor(y)\n",
    "    \n",
    "    print(dataset_type, x_tensor[1, :10, :2])\n",
    "\n",
    "    dataset = TensorDataset(x_tensor, y_tensor)\n",
    "    if dataset_type == 'train':\n",
    "        dataloader = DataLoader(dataset,\n",
    "                                shuffle=True,\n",
    "                                batch_size=2 ** 6,\n",
    "                                num_workers=6)\n",
    "    else:\n",
    "        dataloader_validation = DataLoader(dataset,\n",
    "                                           batch_size=2 ** 6,\n",
    "                                           num_workers=6)\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# initialize SetTransformer model\n",
    "class SetTransformer(nn.Module):\n",
    "    \"\"\" Vanilla SetTransformer from\n",
    "    https://github.com/juho-lee/set_transformer/blob/master/main_pointcloud.py\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_input=3,  # dimension of input data for each element in the set\n",
    "        num_outputs=1,\n",
    "        dim_output=40,  # number of classes\n",
    "        num_inds=32,  # number of induced points, see  Set Transformer paper\n",
    "        dim_hidden=128,\n",
    "        num_heads=4,\n",
    "        ln=False,  # use layer norm\n",
    "    ):\n",
    "        super(SetTransformer, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "            ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            PMA(dim_hidden, num_heads, num_outputs, ln=ln),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(dim_hidden, dim_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.dec(self.enc(input)).squeeze()\n",
    "\n",
    "\n",
    "model = SetTransformer(dim_input=4, dim_output=5)\n",
    "\n",
    "\n",
    "def num_params(model: nn.Module) -> int:\n",
    "    return sum([parameter.nelement() for parameter in model.parameters()])\n",
    "\n",
    "\n",
    "print('model has', num_params(model), 'trainable parameters.')  # type: ignore\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "# %%\n",
    "def train(model, num_epochs: int = 10, lr: float = 1e-3,\n",
    "          verbose: bool = False) -> List[float]:\n",
    "    \"\"\"Custom training loop for Set Transformer on the dataset ``\n",
    "    Args:\n",
    "        model (nn.Module): Set Transformer model to be trained\n",
    "        num_epochs (int, optional): Number of training epochs. Defaults to 10.\n",
    "        lr (float, optional): Learning rate for training. Defaults to 1e-3.\n",
    "        verbose (bool, optional): Print training loss, training accuracy and\n",
    "            validation if set to True. Defaults to False.\n",
    "    Returns:\n",
    "        List[float]: List of training losses\n",
    "    \"\"\"\n",
    "    if use_cuda:\n",
    "        model = nn.DataParallel(model)\n",
    "        model = model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses: List[float] = []\n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        loss_per_epoch = 0\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            # transfer to GPU\n",
    "            if use_cuda:\n",
    "                x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "            loss = criterion(model(x_batch), y_batch.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_per_epoch += loss.item()\n",
    "        losses.append(loss_per_epoch)\n",
    "        if verbose:\n",
    "            print(\"epoch:\", epoch, \"loss:\", loss_per_epoch)\n",
    "            compute_accuracy(model, 'test')\n",
    "            compute_accuracy(model, 'validation')\n",
    "    return losses\n",
    "\n",
    "\n",
    "def compute_accuracy(model, type: str = 'test') -> None:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    if type == 'test':\n",
    "        dl = dataloader\n",
    "    else:\n",
    "        dl = dataloader_validation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dl:\n",
    "            if use_cuda:\n",
    "                x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "            outputs = model(x_batch).squeeze(1)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predictions == y_batch).sum().item()\n",
    "\n",
    "    print(type.capitalize(),\n",
    "          'accuracy of the network on the', total,\n",
    "          'diagrams: %8.2f %%' % (100 * correct / total)\n",
    "          )\n",
    "\n",
    "\n",
    "# %%\n",
    "train(model, num_epochs=500, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "225c997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ffa6888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy of the network on the 5000 diagrams:    90.62 %\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(model, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "072cc93d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SetTransformer(\n",
       "  (enc): Sequential(\n",
       "    (0): ISAB(\n",
       "      (mab0): MAB(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=4, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=4, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (mab1): MAB(\n",
       "        (fc_q): Linear(in_features=4, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ISAB(\n",
       "      (mab0): MAB(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (mab1): MAB(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dec): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): PMA(\n",
       "      (mab): MAB(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "004991aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy of the network on the 5000 diagrams:    90.72 %\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(model, 'validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "155bf613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy of the network on the 5000 diagrams:    90.72 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "dl = dataloader_validation\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in dl:\n",
    "        if use_cuda:\n",
    "            x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "        outputs = model(x_batch).squeeze(1)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predictions == y_batch).sum().item()\n",
    "\n",
    "print(\"validation\".capitalize(),\n",
    "      'accuracy of the network on the', total,\n",
    "      'diagrams: %8.2f %%' % (100 * correct / total)\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0d6895f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SetTransformer(\n",
       "  (enc): Sequential(\n",
       "    (0): ISAB(\n",
       "      (mab0): MAB(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=4, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=4, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (mab1): MAB(\n",
       "        (fc_q): Linear(in_features=4, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ISAB(\n",
       "      (mab0): MAB(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (mab1): MAB(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dec): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): PMA(\n",
       "      (mab): MAB(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(\"set_transformer_orbit5k__induced_attention_90_72%_acc.pth\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00d9cb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train tensor([[0.0000, 0.0006],\n",
      "        [0.0000, 0.0007],\n",
      "        [0.0000, 0.0010],\n",
      "        [0.0000, 0.0010],\n",
      "        [0.0000, 0.0011],\n",
      "        [0.0000, 0.0014],\n",
      "        [0.0000, 0.0015],\n",
      "        [0.0000, 0.0016],\n",
      "        [0.0000, 0.0018],\n",
      "        [0.0000, 0.0018]])\n",
      "validation tensor([[0.0000, 0.0006],\n",
      "        [0.0000, 0.0007],\n",
      "        [0.0000, 0.0010],\n",
      "        [0.0000, 0.0010],\n",
      "        [0.0000, 0.0011],\n",
      "        [0.0000, 0.0014],\n",
      "        [0.0000, 0.0015],\n",
      "        [0.0000, 0.0016],\n",
      "        [0.0000, 0.0018],\n",
      "        [0.0000, 0.0018]])\n"
     ]
    }
   ],
   "source": [
    "# Create ORBIT5K dataset like in the PersLay paper\n",
    "parameters = (2.5, 3.5, 4.0, 4.1, 4.3)  # different classes of orbits\n",
    "homology_dimensions = (0, 1)\n",
    "\n",
    "config = {\n",
    "    'parameters': parameters,\n",
    "    'num_classes': len(parameters),\n",
    "    'num_orbits': 1_000,  # number of orbits per class\n",
    "    'num_pts_per_orbit': 1_000,\n",
    "    'homology_dimensions': homology_dimensions,\n",
    "    'num_homology_dimensions': len(homology_dimensions),\n",
    "    'validation_percentage': 100,  # size of validation dataset relative\n",
    "    # to training\n",
    "}\n",
    "\n",
    "if not use_precomputed_dgms:\n",
    "    for dataset_type in ['train', 'validation']:\n",
    "        # Generate dataset consisting of 5 different orbit types with\n",
    "        # 1000 sampled data points each.\n",
    "        # This is the dataset ORBIT5K used in the PersLay paper\n",
    "        if dataset_type == 'train':\n",
    "            num_orbits = config['num_orbits']\n",
    "        else:\n",
    "            num_orbits = int(config['num_orbits']  # type: ignore\n",
    "                             * config['validation_percentage'] / 100)\n",
    "        x = np.zeros((\n",
    "                        config['num_classes'],  # type: ignore\n",
    "                        num_orbits,\n",
    "                        config['num_pts_per_orbit'],\n",
    "                        2\n",
    "                    ))\n",
    "\n",
    "        # generate dataset\n",
    "        for cidx, p in enumerate(config['parameters']):  # type: ignore\n",
    "            x[cidx, :, 0, :] = np.random.rand(num_orbits, 2)\n",
    "\n",
    "            for i in range(1, config['num_pts_per_orbit']):  # type: ignore\n",
    "                x_cur = x[cidx, :, i - 1, 0]\n",
    "                y_cur = x[cidx, :, i - 1, 1]\n",
    "\n",
    "                x[cidx, :, i, 0] = (x_cur + p * y_cur * (1. - y_cur)) % 1\n",
    "                x_next = x[cidx, :, i, 0]\n",
    "                x[cidx, :, i, 1] = (y_cur + p * x_next * (1. - x_next)) % 1\n",
    "\n",
    "        \"\"\"\n",
    "        # old non-parallel version\n",
    "        for cidx, p in enumerate(config['parameters']):  # type: ignore\n",
    "            for i in range(config['num_orbits']):  # type: ignore\n",
    "                x[cidx][i] = generate_orbit(\n",
    "                    num_pts_per_orbit=config['num_pts_per_orbit'],\n",
    "                    parameter=p\n",
    "                    )\n",
    "        \"\"\"\n",
    "\n",
    "        assert(not np.allclose(x[0, 0], x[0, 1]))\n",
    "\n",
    "        # compute weak alpha persistence\n",
    "        wap = WeakAlphaPersistence(\n",
    "                            homology_dimensions=config['homology_dimensions'],\n",
    "                            n_jobs=multiprocessing.cpu_count()\n",
    "                            )\n",
    "        # c: class, o: orbit, p: point, d: dimension\n",
    "        x_stack = rearrange(x, 'c o p d -> (c o) p d')  # stack classes\n",
    "        diagrams = wap.fit_transform(x_stack)\n",
    "        # shape: (num_classes * n_samples, n_features, 3)\n",
    "\n",
    "        # combine class and orbit dimensions\n",
    "        diagrams = rearrange(\n",
    "                                diagrams,\n",
    "                                '(c o) p d -> c o p d',\n",
    "                                c=config['num_classes']  # type: ignore\n",
    "                            )\n",
    "\n",
    "        # plot sample persistence diagrams for debugging\n",
    "        if(False):\n",
    "            plot_diagram(diagrams[1, 2])\n",
    "            plot_diagram(diagrams[2, 2])\n",
    "\n",
    "        # save dataset\n",
    "        if dataset_type == 'train':\n",
    "            with open(dgms_filename, 'wb') as f:\n",
    "                np.save(f, diagrams)\n",
    "        else:\n",
    "            with open(dgms_filename_validation, 'wb') as f:\n",
    "                np.save(f, diagrams)\n",
    "# %%\n",
    "# load dataset\n",
    "for dataset_type in ['train', 'validation']:\n",
    "\n",
    "    with open(dgms_filename, 'rb') as f:\n",
    "        x = np.load(f)\n",
    "\n",
    "    # c: class, o: orbit, p: point in persistence diagram,\n",
    "    # d: coordinates + homology dimension\n",
    "    x = rearrange(\n",
    "                    x,\n",
    "                    'c o p d -> (c o) p d',\n",
    "                    c=config['num_classes']  # type: ignore\n",
    "                )\n",
    "    # convert homology dimension to one-hot encoding\n",
    "    x = np.concatenate(\n",
    "        (\n",
    "            x[:, :, :2],\n",
    "            (np.eye(config['num_homology_dimensions'])\n",
    "             [x[:, :, -1].astype(np.int32)]),\n",
    "        ),\n",
    "        axis=-1)\n",
    "    # convert from [orbit, sequence_length, feature] to\n",
    "    # [orbit, feature, sequence_length] to fit to the\n",
    "    # input_shape of `SmallSetTransformer`\n",
    "    # x = rearrange(x, 'o s f -> o f s')\n",
    "\n",
    "    # generate labels\n",
    "    y_list = []\n",
    "    for i in range(config['num_classes']):  # type: ignore\n",
    "        y_list += [i] * config['num_orbits']  # type: ignore\n",
    "\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    # load dataset to PyTorch dataloader\n",
    "\n",
    "    x_tensor = torch.Tensor(x)\n",
    "    y_tensor = torch.Tensor(y)\n",
    "    print(dataset_type, x_tensor[1, :10, :2])\n",
    "\n",
    "    dataset = TensorDataset(x_tensor, y_tensor)\n",
    "    if dataset_type == 'train':\n",
    "        dataloader = DataLoader(dataset,\n",
    "                                shuffle=True,\n",
    "                                batch_size=2 ** 6,\n",
    "                                num_workers=6)\n",
    "    else:\n",
    "        dataloader_validation = DataLoader(dataset,\n",
    "                                           batch_size=2 ** 6,\n",
    "                                           num_workers=6)\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0685ae90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd152d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5000, 1319, 4])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train tensor([[0.0000, 0.0010],\n",
    "        [0.0000, 0.0012],\n",
    "        [0.0000, 0.0013],\n",
    "        [0.0000, 0.0014],\n",
    "        [0.0000, 0.0015],\n",
    "        [0.0000, 0.0015],\n",
    "        [0.0000, 0.0015],\n",
    "        [0.0000, 0.0016],\n",
    "        [0.0000, 0.0016],\n",
    "        [0.0000, 0.0018]])\n",
    "validation tensor([[0.0000, 0.0010],\n",
    "        [0.0000, 0.0012],\n",
    "        [0.0000, 0.0013],\n",
    "        [0.0000, 0.0014],\n",
    "        [0.0000, 0.0015],\n",
    "        [0.0000, 0.0015],\n",
    "        [0.0000, 0.0015],\n",
    "        [0.0000, 0.0016],\n",
    "        [0.0000, 0.0016],\n",
    "        [0.0000, 0.0018]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aa260b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy of the network on the 5000 diagrams:    90.26 %\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"set_transformer_orbit5k__induced_attention_90_72%_acc.pth\")\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "dl = dataloader\n",
    "\n",
    "\n",
    "for x_batch, y_batch in dl:\n",
    "    if use_cuda:\n",
    "        x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "    outputs = model(x_batch).squeeze(1)\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "    total += y_batch.size(0)\n",
    "    correct += (predictions == y_batch).sum().item()\n",
    "\n",
    "print(\"validation\".capitalize(),\n",
    "  'accuracy of the network on the', total,\n",
    "  'diagrams: %8.2f %%' % (100 * correct / total)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de6f111c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy of the network on the 5000 diagrams:    90.44 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "dl = dataloader_validation\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in dl:\n",
    "        if use_cuda:\n",
    "            x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "        outputs = model(x_batch).squeeze(1)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predictions == y_batch).sum().item()\n",
    "\n",
    "print(\"validation\".capitalize(),\n",
    "      'accuracy of the network on the', total,\n",
    "      'diagrams: %8.2f %%' % (100 * correct / total)\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dc9c807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SetTransformer(\n",
       "  (enc): Sequential(\n",
       "    (0): ISAB(\n",
       "      (mab0): MAB(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=4, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=4, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (mab1): MAB(\n",
       "        (fc_q): Linear(in_features=4, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ISAB(\n",
       "      (mab0): MAB(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (mab1): MAB(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dec): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): PMA(\n",
       "      (mab): MAB(\n",
       "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
       "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=128, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b10f207e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train tensor([[0.0000, 0.0010],\n",
      "        [0.0000, 0.0010],\n",
      "        [0.0000, 0.0017],\n",
      "        [0.0000, 0.0017],\n",
      "        [0.0000, 0.0018],\n",
      "        [0.0000, 0.0019],\n",
      "        [0.0000, 0.0020],\n",
      "        [0.0000, 0.0020],\n",
      "        [0.0000, 0.0023],\n",
      "        [0.0000, 0.0024]])\n",
      "validation tensor([[0.0000, 0.0010],\n",
      "        [0.0000, 0.0010],\n",
      "        [0.0000, 0.0017],\n",
      "        [0.0000, 0.0017],\n",
      "        [0.0000, 0.0018],\n",
      "        [0.0000, 0.0019],\n",
      "        [0.0000, 0.0020],\n",
      "        [0.0000, 0.0020],\n",
      "        [0.0000, 0.0023],\n",
      "        [0.0000, 0.0024]])\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "for dataset_type in ['train', 'validation']:\n",
    "\n",
    "    with open(dgms_filename, 'rb') as f:\n",
    "        x = np.load(f)\n",
    "\n",
    "    # c: class, o: orbit, p: point in persistence diagram,\n",
    "    # d: coordinates + homology dimension\n",
    "    x = rearrange(\n",
    "                    x,\n",
    "                    'c o p d -> (c o) p d',\n",
    "                    c=config['num_classes']  # type: ignore\n",
    "                )\n",
    "    # convert homology dimension to one-hot encoding\n",
    "    x = np.concatenate(\n",
    "        (\n",
    "            x[:, :, :2],\n",
    "            (np.eye(config['num_homology_dimensions'])\n",
    "             [x[:, :, -1].astype(np.int32)]),\n",
    "        ),\n",
    "        axis=-1)\n",
    "    # convert from [orbit, sequence_length, feature] to\n",
    "    # [orbit, feature, sequence_length] to fit to the\n",
    "    # input_shape of `SmallSetTransformer`\n",
    "    # x = rearrange(x, 'o s f -> o f s')\n",
    "\n",
    "    # generate labels\n",
    "    y_list = []\n",
    "    for i in range(config['num_classes']):  # type: ignore\n",
    "        y_list += [i] * config['num_orbits']  # type: ignore\n",
    "\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    # load dataset to PyTorch dataloader\n",
    "\n",
    "    x_tensor = torch.Tensor(x)\n",
    "    y_tensor = torch.Tensor(y)\n",
    "    print(dataset_type, x_tensor[1, :10, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341a0b74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:giottodeep]",
   "language": "python",
   "name": "conda-env-giottodeep-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
