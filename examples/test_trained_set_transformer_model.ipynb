{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5216ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU!\n",
      "No TPUs...\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# for gridsearch\n",
    "\n",
    "#!pip install pyyaml==5.4.1\n",
    "\n",
    "# %%\n",
    "from IPython import get_ipython  # type: ignore\n",
    "\n",
    "# %% \n",
    "get_ipython().magic('load_ext autoreload')\n",
    "get_ipython().magic('autoreload 2')\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "from dotmap import DotMap\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Import the PyTorch modules\n",
    "import torch  # type: ignore\n",
    "from torch import nn  # type: ignore\n",
    "from torch.optim import SGD, Adam, RMSprop  # type: ignore\n",
    "\n",
    "# Import Tensorflow writer\n",
    "from torch.utils.tensorboard import SummaryWriter  # type: ignore\n",
    "\n",
    "# Import modules from XTransformers\n",
    "from x_transformers.x_transformers import AttentionLayers, Encoder, ContinuousTransformerWrapper\n",
    "\n",
    "\n",
    "# Import the giotto-deep modules\n",
    "from gdeep.data import OrbitsGenerator, DataLoaderKwargs\n",
    "#from gdeep.topology_layers import AttentionPooling\n",
    "from gdeep.topology_layers import ISAB, PMA, SAB\n",
    "from gdeep.pipeline import Pipeline\n",
    "from gdeep.search import Gridsearch\n",
    "import json\n",
    "#from gdeep.search import Gridsearch\n",
    "\n",
    "from optuna.pruners import MedianPruner, NopPruner\n",
    "\n",
    "# %%\n",
    "\n",
    "#Configs\n",
    "config_data = DotMap({\n",
    "    'batch_size_train': 12,\n",
    "    'num_orbits_per_class': 10_000,\n",
    "    'validation_percentage': 0.0,\n",
    "    'test_percentage': 0.0,\n",
    "    'num_jobs': 2,\n",
    "    'dynamical_system': 'classical_convention',\n",
    "    'homology_dimensions': (0, 1),\n",
    "    'dtype': 'float32',\n",
    "    'arbitrary_precision': False\n",
    "})\n",
    "\n",
    "\n",
    "config_model = DotMap({\n",
    "    'implementation': 'Old_SetTransformer', # SetTransformer, PersFormer,\n",
    "    # PytorchTransformer, DeepSet, X-Transformer\n",
    "    'dim_input': 4,\n",
    "    'num_outputs': 1,  # for classification tasks this should be 1\n",
    "    'num_classes': 5,  # number of classes\n",
    "    'dim_hidden': 128,\n",
    "    'num_heads': 8,\n",
    "    'num_induced_points': 32,\n",
    "    'layer_norm': False,  # use layer norm\n",
    "    'pre_layer_norm': False,\n",
    "    'num_layers_encoder': 4,\n",
    "    'num_layers_decoder': 3,\n",
    "    'attention_type': \"induced_attention\",\n",
    "    'activation': nn.GELU,\n",
    "    'dropout': 0.2,\n",
    "    'optimizer': torch.optim.Adam,\n",
    "    'learning_rate': 1e-3,\n",
    "    'num_epochs': 1000,\n",
    "    'pooling_type': \"max\",\n",
    "    'weight_decay': 0.0,\n",
    "    'n_accumulated_grads': 0,\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "# Define the data loader\n",
    "\n",
    "\n",
    "dataloaders_dicts = DataLoaderKwargs(train_kwargs = {\"batch_size\":\n",
    "                                                        config_data.batch_size_train,},\n",
    "                                     val_kwargs = {\"batch_size\": 4},\n",
    "                                     test_kwargs = {\"batch_size\": 3})\n",
    "\n",
    "og = OrbitsGenerator(num_orbits_per_class=config_data.num_orbits_per_class,\n",
    "                     homology_dimensions = config_data.homology_dimensions,\n",
    "                     validation_percentage=config_data.validation_percentage,\n",
    "                     test_percentage=config_data.test_percentage,\n",
    "                     n_jobs=config_data.num_jobs,\n",
    "                     dynamical_system = config_data.dynamical_system,\n",
    "                     dtype=config_data.dtype,\n",
    "                     arbitrary_precision=config_data.arbitrary_precision,\n",
    "                     )\n",
    "\n",
    "if config_data.arbitrary_precision:\n",
    "    orbits = np.load(os.path.join('data', 'orbit5k_arbitrary_precision.npy'))\n",
    "    og.orbits_from_array(orbits)\n",
    "\n",
    "if config_data.dim_input == 2:\n",
    "    dl_train, _, _ = og.get_dataloader_orbits(dataloaders_dicts)\n",
    "else:\n",
    "    dl_train, _, _ = og.get_dataloader_persistence_diagrams(dataloaders_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd9a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetTransformer(nn.Module):\n",
    "    \"\"\" Vanilla SetTransformer from\n",
    "    https://github.com/juho-lee/set_transformer/blob/master/main_pointcloud.py\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_input=3,  # dimension of input data for each element in the set\n",
    "        num_outputs=1,\n",
    "        dim_output=40,  # number of classes\n",
    "        num_inds=32,  # number of induced points, see  Set Transformer paper\n",
    "        dim_hidden=128,\n",
    "        num_heads=4,\n",
    "        ln=False,  # use layer norm\n",
    "    ):\n",
    "        super(SetTransformer, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "            ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            PMA(dim_hidden, num_heads, num_outputs, ln=ln),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(dim_hidden, dim_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.dec(self.enc(input)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d4c6fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy of the network on the 50000 diagrams:    89.79 %\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"set_transformer_orbit5k__induced_attention_90_72%_acc.pth\")\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "dl = dl_train\n",
    "use_cuda = True\n",
    "\n",
    "for x_batch, y_batch in dl:\n",
    "    if use_cuda:\n",
    "        x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "    outputs = model(x_batch).squeeze(1)\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "    total += y_batch.size(0)\n",
    "    correct += (predictions == y_batch).sum().item()\n",
    "\n",
    "print(\"validation\".capitalize(),\n",
    "  'accuracy of the network on the', total,\n",
    "  'diagrams: %8.2f %%' % (100 * correct / total)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88df0753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:giottodeep]",
   "language": "python",
   "name": "conda-env-giottodeep-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
