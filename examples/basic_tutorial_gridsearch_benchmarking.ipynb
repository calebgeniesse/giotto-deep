{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: image data\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with the basic functioning of *giotto-deep* API.\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. creation of a dataset\n",
    " 2. creation of a model\n",
    " 3. define metrics and losses\n",
    " 4. run benchmarks\n",
    " 5. visualise results interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from gdeep.models import FFNet\n",
    "\n",
    "from gdeep.visualisation import  persistence_diagrams_of_activations\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gdeep.data import TorchDataLoader\n",
    "\n",
    "\n",
    "from gtda.diagrams import BettiCurve\n",
    "\n",
    "from gtda.plotting import plot_betti_surfaces\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the reuslts of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10240\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "20\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "dl = TorchDataLoader(name=\"CIFAR10\")\n",
    "train_indices = []\n",
    "for i in range(10240):\n",
    "    train_indices.append(i)\n",
    "\n",
    "print(len(train_indices))\n",
    "dl_tr, dl_temp = dl.build_dataloader(batch_size=512, sampler=SubsetRandomSampler(train_indices))\n",
    "\n",
    "print(len(dl_tr))\n",
    "\n",
    "test_indices = []\n",
    "for i in range(3072):\n",
    "    test_indices.append(i)\n",
    "\n",
    "dl_ts, dl_temp = dl.build_dataloader(batch_size=512, sampler=SubsetRandomSampler(test_indices))\n",
    "\n",
    "dl_val = dl_ts\n",
    "\n",
    "print(len(dl_ts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "from gdeep.pipeline import Pipeline\n",
    "\n",
    "model = nn.Sequential(models.resnet18(pretrained=True), nn.Linear(1000,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "TOTAL EPOCHS  1\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 1.574904  [20/20]\n",
      "Time taken for this epoch: 74s\n",
      "Validation results: \n",
      " Accuracy: 3.3%,                 Avg loss: 0.000161 \n",
      "\n",
      "Test results: \n",
      " Accuracy: 3.3%,                 Avg loss: 0.000162 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD, Adam, RMSprop\n",
    "from gdeep.search import gridsearch\n",
    "\n",
    "# print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# pipe = Pipeline(model, (dl_tr, dl_ts), loss_fn, writer)\n",
    "# pipe = Pipeline(model, (dl_tr, dl_ts), loss_fn, writer, True, \"loss\", 5)\n",
    "pipe = Pipeline(model, [dl_tr, dl_val, dl_ts], loss_fn, writer)\n",
    "# pipe = Pipeline(model, [dl_tr, dl_ts], loss_fn, writer)\n",
    "\n",
    "# search = gridsearch.Gridsearch(pipe, \"loss\", 5)\n",
    "# search.search([SGD, Adam], 1, lr=[0.001, 0.01])\n",
    "\n",
    "# train the model\n",
    "pipe.train(SGD, 1, cross_validation = True, batch_size = 512, lr=0.01)\n",
    "# pipe.train([SGD, Adam], 1, lr=[0.001, 0.01])\n",
    "# pipe.train([SGD, Adam], 1, lr=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-29 16:07:12,363]\u001b[0m A new study created in memory with name: no-name-5acfb4a2-753f-4551-9f7a-d23151cdf12c\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 2.167293  [20/20]]\n",
      "Time taken for this epoch: 74s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-29 16:08:29,158]\u001b[0m Trial 0 finished with value: 2.167293071746826 and parameters: {'optimizer': <class 'torch.optim.adam.Adam'>, 'lr': 0.003606973846401288}. Best is trial 0 with value: 2.167293071746826.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " Accuracy: 2.0%,                 Avg loss: 0.000234 \n",
      "\n",
      "Done!\n",
      "Study statistics: \n",
      "  Number of finished trials:  1\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  1\n",
      "Best trial:\n",
      "  Value:  2.167293071746826\n",
      "  Params: \n",
      "    optimizer: <class 'torch.optim.adam.Adam'>\n",
      "    lr: 0.003606973846401288\n"
     ]
    }
   ],
   "source": [
    "from gdeep.search.gridsearch import Gridsearch\n",
    "from torch.optim import SGD, Adam, RMSprop\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "pipe = Pipeline(model, [dl_tr, dl_val, dl_ts], loss_fn, writer)\n",
    "# pipe = Pipeline([model1, model2, ...], [[dl_tr, dl_val, dl_ts], [dl_tr2, dl_val2, dl_ts2], ...], loss_fn, writer)\n",
    "\n",
    "search = Gridsearch(pipe, \"loss\", 1)\n",
    "search.start([SGD, Adam], 1, lr=[0.001, 0.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch with multiple pipelines (models/datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gdeep.search.gridsearch import Gridsearch\n",
    "\n",
    "# pipe1 = Pipeline(model1, [dl_tr, dl_ts, dl_ts], loss_fn, writer)\n",
    "# pipe2 = Pipeline(model2, [dl_tr, dl_ts, dl_ts], loss_fn, writer)\n",
    "\n",
    "# search1 = Gridsearch(pipe1, \"loss\", 2)\n",
    "# search1.search([SGD, Adam], 1, lr=[0.001, 0.01])\n",
    "\n",
    "# search2 = Gridsearch(pipe1, \"loss\", 2)\n",
    "# search2.search([SGD, Adam], 1, lr=[0.001, 0.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking a single model on multiple datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing multiple datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR10_10000\n"
     ]
    }
   ],
   "source": [
    "dataloaders_dicts = []\n",
    "dl = TorchDataLoader(name=\"CIFAR10\")\n",
    "train_indices = []\n",
    "for i in range(5120):\n",
    "    train_indices.append(i)\n",
    "\n",
    "dl_tr, dl_temp = dl.build_dataloader(batch_size=1024, sampler=SubsetRandomSampler(train_indices))\n",
    "\n",
    "test_indices = []\n",
    "for i in range(1024):\n",
    "    test_indices.append(i)\n",
    "\n",
    "dl_ts, dl_temp = dl.build_dataloader(batch_size=1024, sampler=SubsetRandomSampler(test_indices))\n",
    "\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"CIFAR10_5000\"\n",
    "temp_dict[\"dataloaders\"] = (dl_tr, dl_ts)\n",
    "\n",
    "dataloaders_dicts.append(temp_dict)\n",
    "\n",
    "\n",
    "train_indices = []\n",
    "for i in range(10240):\n",
    "    train_indices.append(i)\n",
    "\n",
    "dl_tr, dl_temp = dl.build_dataloader(batch_size=1024, sampler=SubsetRandomSampler(train_indices))\n",
    "\n",
    "test_indices = []\n",
    "for i in range(2048):\n",
    "    test_indices.append(i)\n",
    "\n",
    "dl_ts, dl_temp = dl.build_dataloader(batch_size=1024, sampler=SubsetRandomSampler(test_indices))\n",
    "\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"CIFAR10_10000\"\n",
    "temp_dict[\"dataloaders\"] = (dl_tr, dl_ts)\n",
    "\n",
    "dataloaders_dicts.append(temp_dict)\n",
    "\n",
    "print(dataloaders_dicts[1][\"name\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gdeep.pipeline import benchmark\n",
    "\n",
    "# bench = benchmark.Benchmark(writer)\n",
    "\n",
    "# bench.benchmark_model(model, dataloaders_dicts, loss_fn, SGD, 1, 0.01)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking a single dataset on multiple models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing multiple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dicts = []\n",
    "\n",
    "model = nn.Sequential(models.resnet18(pretrained=True), nn.Linear(1000,10))\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"resnet18\"\n",
    "temp_dict[\"model\"] = model\n",
    "\n",
    "models_dicts.append(temp_dict)\n",
    "\n",
    "model = nn.Sequential(models.vgg16(pretrained=True), nn.Linear(1000,10))\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"vgg16\"\n",
    "temp_dict[\"model\"] = model\n",
    "\n",
    "models_dicts.append(temp_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bench = benchmark.Benchmark(writer)\n",
    "\n",
    "# bench.benchmark_data(model_dicts, (dl_tr, dl_ts), loss_fn, SGD, 1, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gdeep.search.benchmark import Benchmark\n",
    "# from torch.optim import SGD, Adam, RMSprop\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# bench = Benchmark(writer)\n",
    "\n",
    "# bench.benchmark(models_dicts, dataloaders_dicts, loss_fn, optimizer = SGD, epochs = 1, learning_rate = 0.01, batch_size = 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking + Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of gdeep.search.gridsearch failed: Traceback (most recent call last):\n",
      "  File \"d:\\anaconda3\\envs\\giotto-deep2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"d:\\anaconda3\\envs\\giotto-deep2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"d:\\anaconda3\\envs\\giotto-deep2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"d:\\anaconda3\\envs\\giotto-deep2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"d:\\anaconda3\\envs\\giotto-deep2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"d:\\anaconda3\\envs\\giotto-deep2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 266, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: __init__() requires a code object with 0 free vars, not 1\n",
      "]\n",
      "\u001b[32m[I 2021-07-29 16:23:01,476]\u001b[0m A new study created in memory with name: no-name-29e7b7cf-e025-4661-84aa-fd76209182fa\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "Performing Gridsearch on Dataset: CIFAR10_5000, Model: resnet18\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 3.083695  [ 4/ 4]\n",
      "Time taken for this epoch: 14s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-29 16:23:15,828]\u001b[0m Trial 0 finished with value: 3.0836946964263916 and parameters: {'optimizer': <class 'torch.optim.sgd.SGD'>, 'lr': 0.00194738155636073}. Best is trial 0 with value: 3.0836946964263916.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 16:23:15,828]\u001b[0m A new study created in memory with name: no-name-7a99e89c-1658-4e5a-af50-e6e26e507269\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " Accuracy: 0.3%,                 Avg loss: 0.000083 \n",
      "\n",
      "Done!\n",
      "Study statistics: \n",
      "  Number of finished trials:  1\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  1\n",
      "Best trial:\n",
      "  Value:  3.0836946964263916\n",
      "  Params: \n",
      "    optimizer: <class 'torch.optim.sgd.SGD'>\n",
      "    lr: 0.00194738155636073\n",
      "****************************************\n",
      "Performing Gridsearch on Dataset: CIFAR10_5000, Model: vgg16\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 2.345125  [ 4/ 4]\n",
      "Time taken for this epoch: 32s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-29 16:23:49,503]\u001b[0m Trial 0 finished with value: 2.3451247215270996 and parameters: {'optimizer': <class 'torch.optim.sgd.SGD'>, 'lr': 0.0015950385251406213}. Best is trial 0 with value: 2.3451247215270996.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 16:23:49,503]\u001b[0m A new study created in memory with name: no-name-3bb3a01b-7581-44d5-b3c7-6068caa10450\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " Accuracy: 0.1%,                 Avg loss: 0.000046 \n",
      "\n",
      "Done!\n",
      "Study statistics: \n",
      "  Number of finished trials:  1\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  1\n",
      "Best trial:\n",
      "  Value:  2.3451247215270996\n",
      "  Params: \n",
      "    optimizer: <class 'torch.optim.sgd.SGD'>\n",
      "    lr: 0.0015950385251406213\n",
      "****************************************\n",
      "Performing Gridsearch on Dataset: CIFAR10_10000, Model: resnet18\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 3.018966  [ 8/ 8]\n",
      "Time taken for this epoch: 28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-29 16:24:17,879]\u001b[0m Trial 0 finished with value: 3.018965721130371 and parameters: {'optimizer': <class 'torch.optim.sgd.SGD'>, 'lr': 0.0025285098276113817}. Best is trial 0 with value: 3.018965721130371.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 16:24:17,879]\u001b[0m A new study created in memory with name: no-name-44a92847-d0af-43d9-b51a-c4ee2795323b\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " Accuracy: 0.7%,                 Avg loss: 0.000109 \n",
      "\n",
      "Done!\n",
      "Study statistics: \n",
      "  Number of finished trials:  1\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  1\n",
      "Best trial:\n",
      "  Value:  3.018965721130371\n",
      "  Params: \n",
      "    optimizer: <class 'torch.optim.sgd.SGD'>\n",
      "    lr: 0.0025285098276113817\n",
      "****************************************\n",
      "Performing Gridsearch on Dataset: CIFAR10_10000, Model: vgg16\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 2.305908  [ 8/ 8]\n",
      "Time taken for this epoch: 71s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-07-29 16:25:33,486]\u001b[0m Trial 0 finished with value: 2.305908203125 and parameters: {'optimizer': <class 'torch.optim.adam.Adam'>, 'lr': 0.0013368589659000776}. Best is trial 0 with value: 2.305908203125.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " Accuracy: 0.2%,                 Avg loss: 0.000092 \n",
      "\n",
      "Done!\n",
      "Study statistics: \n",
      "  Number of finished trials:  1\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  1\n",
      "Best trial:\n",
      "  Value:  2.305908203125\n",
      "  Params: \n",
      "    optimizer: <class 'torch.optim.adam.Adam'>\n",
      "    lr: 0.0013368589659000776\n"
     ]
    }
   ],
   "source": [
    "from gdeep.search.benchmark import Benchmark\n",
    "from gdeep.search.gridsearch import Gridsearch\n",
    "from torch.optim import SGD, Adam, RMSprop\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "bench = Benchmark(models_dicts, dataloaders_dicts, loss_fn, writer)\n",
    "\n",
    "search = Gridsearch(bench, \"loss\", 1)\n",
    "search.start([SGD, Adam], 1, 512, lr=[0.001, 0.01])\n",
    "\n",
    "# bench.benchmark(optimizer = [SGD,Adam], epochs = 1, learning_rate = [0.001,0.01], batch_size = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
