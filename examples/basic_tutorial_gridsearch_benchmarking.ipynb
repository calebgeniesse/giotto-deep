{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Basic tutorial: image data\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with the basic functioning of *giotto-deep* API.\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. creation of a dataset\n",
    " 2. creation of a model\n",
    " 3. define metrics and losses\n",
    " 4. run benchmarks\n",
    " 5. visualise results interactively"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from gdeep.models import FFNet\n",
    "\n",
    "from gdeep.visualisation import  persistence_diagrams_of_activations\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gdeep.data import TorchDataLoader\n",
    "\n",
    "\n",
    "from gtda.diagrams import BettiCurve\n",
    "\n",
    "from gtda.plotting import plot_betti_surfaces\n",
    "\n",
    "import optuna"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the reuslts of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "writer = SummaryWriter()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create your dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "dl = TorchDataLoader(name=\"CIFAR10\")\n",
    "train_indices = []\n",
    "for i in range(10240):\n",
    "    train_indices.append(i)\n",
    "\n",
    "print(len(train_indices))\n",
    "dl_tr, dl_temp = dl.build_dataloader(batch_size=512, sampler=SubsetRandomSampler(train_indices))\n",
    "\n",
    "print(len(dl_tr))\n",
    "\n",
    "test_indices = []\n",
    "for i in range(3072):\n",
    "    test_indices.append(i)\n",
    "\n",
    "dl_ts, dl_temp = dl.build_dataloader(batch_size=512, sampler=SubsetRandomSampler(test_indices))\n",
    "\n",
    "dl_val = dl_ts\n",
    "\n",
    "print(len(dl_ts))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10240\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "20\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "6\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "type(dl_ts)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define and train your model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import torchvision.models as models\n",
    "from gdeep.pipeline import Pipeline\n",
    "\n",
    "model = nn.Sequential(models.resnet18(pretrained=True), nn.Linear(1000,10))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a289d4578d7455f96590ac9563b699d"
      }
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from torch.optim import SGD, Adam, RMSprop\n",
    "from gdeep.search import gridsearch\n",
    "\n",
    "# print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# pipe = Pipeline(model, (dl_tr, dl_ts), loss_fn, writer)\n",
    "# pipe = Pipeline(model, (dl_tr, dl_ts), loss_fn, writer, True, \"loss\", 5)\n",
    "pipe = Pipeline(model, [dl_tr, dl_val, dl_ts], loss_fn, writer)\n",
    "# pipe = Pipeline(model, [dl_tr, dl_ts], loss_fn, writer)\n",
    "\n",
    "# search = gridsearch.Gridsearch(pipe, \"loss\", 5)\n",
    "# search.search([SGD, Adam], 1, lr=[0.001, 0.01])\n",
    "\n",
    "# train the model\n",
    "pipe.train(SGD, 1, cross_validation = True, batch_size = 512, lr=0.01)\n",
    "# pipe.train([SGD, Adam], 1, lr=[0.001, 0.01])\n",
    "# pipe.train([SGD, Adam], 1, lr=0.01)\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor()\n",
      "TOTAL EPOCHS  1\n",
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning:\n",
      "\n",
      "Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training loss: 1.638420  [20/20]\n",
      "Time taken for this epoch: 3s\n",
      "Validation results: \n",
      " Accuracy: 3.3%,                 Avg loss: 0.000162 \n",
      "\n",
      "Test results: \n",
      " Accuracy: 3.3%,                 Avg loss: 0.000162 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gridsearch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from gdeep.search.gridsearch import Gridsearch\n",
    "from torch.optim import SGD, Adam, RMSprop\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "pipe = Pipeline(model, [dl_tr, dl_val, dl_ts], loss_fn, writer)\n",
    "# pipe = Pipeline([model1, model2, ...], [[dl_tr, dl_val, dl_ts], [dl_tr2, dl_val2, dl_ts2], ...], loss_fn, writer)\n",
    "\n",
    "search = Gridsearch(pipe, \"loss\", 1)\n",
    "search.start([SGD, Adam], 1, lr=[0.001, 0.01])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m[I 2021-08-23 10:05:18,147]\u001b[0m A new study created in memory with name: no-name-e51ffafc-f474-4ca4-9f7b-278eb4079a2d\u001b[0m\n",
      "/usr/local/lib/python3.7/dist-packages/optuna/distributions.py:427: UserWarning:\n",
      "\n",
      "Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.sgd.SGD'> which is of type type.\n",
      "\n",
      "/usr/local/lib/python3.7/dist-packages/optuna/distributions.py:427: UserWarning:\n",
      "\n",
      "Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains <class 'torch.optim.adam.Adam'> which is of type type.\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 1.201136  [20/20]\n",
      "Time taken for this epoch: 3s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m[I 2021-08-23 10:05:21,455]\u001b[0m Trial 0 finished with value: 1.201135516166687 and parameters: {'optimizer': <class 'torch.optim.adam.Adam'>, 'lr': 0.001867290992443191}. Best is trial 0 with value: 1.201135516166687.\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation results: \n",
      " Accuracy: 3.9%,                 Avg loss: 0.000133 \n",
      "\n",
      "Done!\n",
      "Study statistics: \n",
      "  Number of finished trials:  1\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  1\n",
      "Best trial:\n",
      "  Value:  1.201135516166687\n",
      "  Params: \n",
      "    optimizer: <class 'torch.optim.adam.Adam'>\n",
      "    lr: 0.001867290992443191\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Gridsearch with multiple pipelines (models/datasets)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "# from gdeep.search.gridsearch import Gridsearch\n",
    "\n",
    "# pipe1 = Pipeline(model1, [dl_tr, dl_ts, dl_ts], loss_fn, writer)\n",
    "# pipe2 = Pipeline(model2, [dl_tr, dl_ts, dl_ts], loss_fn, writer)\n",
    "\n",
    "# search1 = Gridsearch(pipe1, \"loss\", 2)\n",
    "# search1.search([SGD, Adam], 1, lr=[0.001, 0.01])\n",
    "\n",
    "# search2 = Gridsearch(pipe1, \"loss\", 2)\n",
    "# search2.search([SGD, Adam], 1, lr=[0.001, 0.01])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Benchmarking a single model on multiple datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing multiple datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "dataloaders_dicts = []\n",
    "dl = TorchDataLoader(name=\"CIFAR10\")\n",
    "train_indices = []\n",
    "for i in range(5120):\n",
    "    train_indices.append(i)\n",
    "\n",
    "dl_tr, dl_temp = dl.build_dataloader(batch_size=1024, sampler=SubsetRandomSampler(train_indices))\n",
    "\n",
    "test_indices = []\n",
    "for i in range(1024):\n",
    "    test_indices.append(i)\n",
    "\n",
    "dl_ts, dl_temp = dl.build_dataloader(batch_size=1024, sampler=SubsetRandomSampler(test_indices))\n",
    "\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"CIFAR10_5000\"\n",
    "temp_dict[\"dataloaders\"] = (dl_tr, dl_ts)\n",
    "\n",
    "dataloaders_dicts.append(temp_dict)\n",
    "\n",
    "\n",
    "train_indices = []\n",
    "for i in range(10240):\n",
    "    train_indices.append(i)\n",
    "\n",
    "dl_tr, dl_temp = dl.build_dataloader(batch_size=1024, sampler=SubsetRandomSampler(train_indices))\n",
    "\n",
    "test_indices = []\n",
    "for i in range(2048):\n",
    "    test_indices.append(i)\n",
    "\n",
    "dl_ts, dl_temp = dl.build_dataloader(batch_size=1024, sampler=SubsetRandomSampler(test_indices))\n",
    "\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"CIFAR10_10000\"\n",
    "temp_dict[\"dataloaders\"] = (dl_tr, dl_ts)\n",
    "\n",
    "dataloaders_dicts.append(temp_dict)\n",
    "\n",
    "print(dataloaders_dicts[1][\"name\"])\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "CIFAR10_10000\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Benchmarking model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "# from gdeep.pipeline import benchmark\n",
    "\n",
    "# bench = benchmark.Benchmark(writer)\n",
    "\n",
    "# bench.benchmark_model(model, dataloaders_dicts, loss_fn, SGD, 1, 0.01)  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Benchmarking a single dataset on multiple models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing multiple models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "models_dicts = []\n",
    "\n",
    "model = nn.Sequential(models.resnet18(pretrained=True), nn.Linear(1000,10))\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"resnet18\"\n",
    "temp_dict[\"model\"] = model\n",
    "\n",
    "models_dicts.append(temp_dict)\n",
    "\n",
    "model = nn.Sequential(models.vgg16(pretrained=True), nn.Linear(1000,10))\n",
    "temp_dict = {}\n",
    "temp_dict[\"name\"] = \"vgg16\"\n",
    "temp_dict[\"model\"] = model\n",
    "\n",
    "models_dicts.append(temp_dict)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Benchmarking data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "# bench = benchmark.Benchmark(writer)\n",
    "\n",
    "# bench.benchmark_data(model_dicts, (dl_tr, dl_ts), loss_fn, SGD, 1, 0.001)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Benchmarking both"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# from gdeep.search.benchmark import Benchmark\n",
    "# from torch.optim import SGD, Adam, RMSprop\n",
    "\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# bench = Benchmark(writer)\n",
    "\n",
    "# bench.benchmark(models_dicts, dataloaders_dicts, loss_fn, optimizer = SGD, epochs = 1, learning_rate = 0.01, batch_size = 1024)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Benchmarking + Gridsearch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "from gdeep.search.benchmark import Benchmark\n",
    "from gdeep.search.gridsearch import Gridsearch\n",
    "from torch.optim import SGD, Adam, RMSprop\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "bench = Benchmark(models_dicts, dataloaders_dicts, loss_fn, writer)\n",
    "\n",
    "search = Gridsearch(bench, \"loss\", 1)\n",
    "search.start([SGD, Adam], 1, 512, lr=[0.001, 0.01])\n",
    "\n",
    "# bench.benchmark(optimizer = [SGD,Adam], epochs = 1, learning_rate = [0.001,0.01], batch_size = 1024)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[autoreload of gdeep.search.gridsearch failed: Traceback (most recent call last):\n",
      "  File \"d:\\anaconda3\\envs\\giotto-deep2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"d:\\anaconda3\\envs\\giotto-deep2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"d:\\anaconda3\\envs\\giotto-deep2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"d:\\anaconda3\\envs\\giotto-deep2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"d:\\anaconda3\\envs\\giotto-deep2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"d:\\anaconda3\\envs\\giotto-deep2\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 266, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: __init__() requires a code object with 0 free vars, not 1\n",
      "]\n",
      "\u001b[32m[I 2021-07-29 16:23:01,476]\u001b[0m A new study created in memory with name: no-name-29e7b7cf-e025-4661-84aa-fd76209182fa\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "****************************************\n",
      "Performing Gridsearch on Dataset: CIFAR10_5000, Model: resnet18\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 3.083695  [ 4/ 4]\n",
      "Time taken for this epoch: 14s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m[I 2021-07-29 16:23:15,828]\u001b[0m Trial 0 finished with value: 3.0836946964263916 and parameters: {'optimizer': <class 'torch.optim.sgd.SGD'>, 'lr': 0.00194738155636073}. Best is trial 0 with value: 3.0836946964263916.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 16:23:15,828]\u001b[0m A new study created in memory with name: no-name-7a99e89c-1658-4e5a-af50-e6e26e507269\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation results: \n",
      " Accuracy: 0.3%,                 Avg loss: 0.000083 \n",
      "\n",
      "Done!\n",
      "Study statistics: \n",
      "  Number of finished trials:  1\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  1\n",
      "Best trial:\n",
      "  Value:  3.0836946964263916\n",
      "  Params: \n",
      "    optimizer: <class 'torch.optim.sgd.SGD'>\n",
      "    lr: 0.00194738155636073\n",
      "****************************************\n",
      "Performing Gridsearch on Dataset: CIFAR10_5000, Model: vgg16\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 2.345125  [ 4/ 4]\n",
      "Time taken for this epoch: 32s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m[I 2021-07-29 16:23:49,503]\u001b[0m Trial 0 finished with value: 2.3451247215270996 and parameters: {'optimizer': <class 'torch.optim.sgd.SGD'>, 'lr': 0.0015950385251406213}. Best is trial 0 with value: 2.3451247215270996.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 16:23:49,503]\u001b[0m A new study created in memory with name: no-name-3bb3a01b-7581-44d5-b3c7-6068caa10450\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation results: \n",
      " Accuracy: 0.1%,                 Avg loss: 0.000046 \n",
      "\n",
      "Done!\n",
      "Study statistics: \n",
      "  Number of finished trials:  1\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  1\n",
      "Best trial:\n",
      "  Value:  2.3451247215270996\n",
      "  Params: \n",
      "    optimizer: <class 'torch.optim.sgd.SGD'>\n",
      "    lr: 0.0015950385251406213\n",
      "****************************************\n",
      "Performing Gridsearch on Dataset: CIFAR10_10000, Model: resnet18\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 3.018966  [ 8/ 8]\n",
      "Time taken for this epoch: 28s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m[I 2021-07-29 16:24:17,879]\u001b[0m Trial 0 finished with value: 3.018965721130371 and parameters: {'optimizer': <class 'torch.optim.sgd.SGD'>, 'lr': 0.0025285098276113817}. Best is trial 0 with value: 3.018965721130371.\u001b[0m\n",
      "\u001b[32m[I 2021-07-29 16:24:17,879]\u001b[0m A new study created in memory with name: no-name-44a92847-d0af-43d9-b51a-c4ee2795323b\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation results: \n",
      " Accuracy: 0.7%,                 Avg loss: 0.000109 \n",
      "\n",
      "Done!\n",
      "Study statistics: \n",
      "  Number of finished trials:  1\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  1\n",
      "Best trial:\n",
      "  Value:  3.018965721130371\n",
      "  Params: \n",
      "    optimizer: <class 'torch.optim.sgd.SGD'>\n",
      "    lr: 0.0025285098276113817\n",
      "****************************************\n",
      "Performing Gridsearch on Dataset: CIFAR10_10000, Model: vgg16\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Training loss: 2.305908  [ 8/ 8]\n",
      "Time taken for this epoch: 71s\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m[I 2021-07-29 16:25:33,486]\u001b[0m Trial 0 finished with value: 2.305908203125 and parameters: {'optimizer': <class 'torch.optim.adam.Adam'>, 'lr': 0.0013368589659000776}. Best is trial 0 with value: 2.305908203125.\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Validation results: \n",
      " Accuracy: 0.2%,                 Avg loss: 0.000092 \n",
      "\n",
      "Done!\n",
      "Study statistics: \n",
      "  Number of finished trials:  1\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  1\n",
      "Best trial:\n",
      "  Value:  2.305908203125\n",
      "  Params: \n",
      "    optimizer: <class 'torch.optim.adam.Adam'>\n",
      "    lr: 0.0013368589659000776\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}