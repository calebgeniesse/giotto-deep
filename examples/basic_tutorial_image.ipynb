{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: image data\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with the basic functioning of *giotto-deep* API.\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. creation of a dataset\n",
    " 2. creation of a model\n",
    " 3. define metrics and losses\n",
    " 4. run benchmarks\n",
    " 5. visualise results interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from gdeep.models import FFNet\n",
    "\n",
    "from gdeep.visualisation import  persistence_diagrams_of_activations\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gdeep.data import TorchDataLoader\n",
    "\n",
    "\n",
    "from gtda.diagrams import BettiCurve\n",
    "\n",
    "from gtda.plotting import plot_betti_surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the reuslts of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "dl = TorchDataLoader(name=\"CIFAR10\")\n",
    "\n",
    "# use only 320 images from cifar10\n",
    "train_indices = list(range(32*10))\n",
    "dl_tr, dl_ts = dl.build_dataloaders(batch_size=32, sampler=SubsetRandomSampler(train_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPUs...\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "from gdeep.pipeline import Pipeline\n",
    "\n",
    "# wrap a sequential model in a torch nn.Module\n",
    "class model3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model3, self).__init__()\n",
    "        self.seqmodel = nn.Sequential(models.resnet18(pretrained=True), nn.Linear(1000,10))\n",
    "    def forward(self, X):\n",
    "        return self.seqmodel(X)\n",
    "\n",
    "model = model3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  3.191237837076187  \tBatch training accuracy:  16.796875  \t[ 8 / 8 ]                               \n",
      "Time taken for this epoch: 3.00s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 9.375000%,                 Avg loss: 2.889026 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  2.553238496184349  \tBatch training accuracy:  26.171875  \t[ 8 / 8 ]                               \n",
      "Time taken for this epoch: 3.00s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 6.250000%,                 Avg loss: 3.464406 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.6466883569955826  \tBatch training accuracy:  48.828125  \t[ 8 / 8 ]                             \n",
      "Time taken for this epoch: 3.00s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 23.437500%,                 Avg loss: 3.096096 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.0960962772369385, 23.4375)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "# print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "pipe = Pipeline(model, (dl_tr, dl_ts), loss_fn, writer)\n",
    "\n",
    "# train the model\n",
    "pipe.train(SGD, 3, False, {\"lr\":0.01}, {\"batch_size\":32, \"sampler\":SubsetRandomSampler(train_indices)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simply use interpretability tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matteocaorsi/opt/anaconda3/lib/python3.9/site-packages/captum/_utils/gradient.py:56: UserWarning:\n",
      "\n",
      "Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "\n",
      "/Users/matteocaorsi/opt/anaconda3/lib/python3.9/site-packages/captum/attr/_core/guided_backprop_deconvnet.py:59: UserWarning:\n",
      "\n",
      "Setting backward hooks on ReLU activations.The hooks will be removed after the attribution is finished\n",
      "\n",
      "/Users/matteocaorsi/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1025: UserWarning:\n",
      "\n",
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gdeep.analysis.interpretability import Interpreter\n",
    "from gdeep.visualisation import Visualiser\n",
    "\n",
    "inter = Interpreter(pipe.model, method=\"GuidedGradCam\")\n",
    "output = inter.interpret_image(next(iter(dl_tr))[0][0].reshape(1,3,32,32), \n",
    "                      1, pipe.model.seqmodel[0].layer2[0].conv1);\n",
    "\n",
    "# visualise the interpreter\n",
    "vs = Visualiser(pipe)\n",
    "vs.plot_interpreter_image(inter);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract inner data from your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqmodel.0.conv1.weight torch.Size([64, 3, 7, 7])\n",
      "seqmodel.0.bn1.weight torch.Size([64])\n",
      "seqmodel.0.bn1.bias torch.Size([64])\n",
      "seqmodel.0.bn1.running_mean torch.Size([64])\n",
      "seqmodel.0.bn1.running_var torch.Size([64])\n",
      "seqmodel.0.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer1.0.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "seqmodel.0.layer1.0.bn1.weight torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn1.bias torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn1.running_mean torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn1.running_var torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "seqmodel.0.layer1.0.bn2.weight torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn2.bias torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn2.running_mean torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn2.running_var torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer1.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "seqmodel.0.layer1.1.bn1.weight torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn1.bias torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn1.running_mean torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn1.running_var torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "seqmodel.0.layer1.1.bn2.weight torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn2.bias torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn2.running_mean torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn2.running_var torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer2.0.conv1.weight torch.Size([128, 64, 3, 3])\n",
      "seqmodel.0.layer2.0.bn1.weight torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn1.bias torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn1.running_mean torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn1.running_var torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "seqmodel.0.layer2.0.bn2.weight torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn2.bias torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn2.running_mean torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn2.running_var torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer2.0.downsample.0.weight torch.Size([128, 64, 1, 1])\n",
      "seqmodel.0.layer2.0.downsample.1.weight torch.Size([128])\n",
      "seqmodel.0.layer2.0.downsample.1.bias torch.Size([128])\n",
      "seqmodel.0.layer2.0.downsample.1.running_mean torch.Size([128])\n",
      "seqmodel.0.layer2.0.downsample.1.running_var torch.Size([128])\n",
      "seqmodel.0.layer2.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer2.1.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "seqmodel.0.layer2.1.bn1.weight torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn1.bias torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn1.running_mean torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn1.running_var torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "seqmodel.0.layer2.1.bn2.weight torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn2.bias torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn2.running_mean torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn2.running_var torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer3.0.conv1.weight torch.Size([256, 128, 3, 3])\n",
      "seqmodel.0.layer3.0.bn1.weight torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn1.bias torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn1.running_mean torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn1.running_var torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "seqmodel.0.layer3.0.bn2.weight torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn2.bias torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn2.running_mean torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn2.running_var torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer3.0.downsample.0.weight torch.Size([256, 128, 1, 1])\n",
      "seqmodel.0.layer3.0.downsample.1.weight torch.Size([256])\n",
      "seqmodel.0.layer3.0.downsample.1.bias torch.Size([256])\n",
      "seqmodel.0.layer3.0.downsample.1.running_mean torch.Size([256])\n",
      "seqmodel.0.layer3.0.downsample.1.running_var torch.Size([256])\n",
      "seqmodel.0.layer3.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer3.1.conv1.weight torch.Size([256, 256, 3, 3])\n",
      "seqmodel.0.layer3.1.bn1.weight torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn1.bias torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn1.running_mean torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn1.running_var torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "seqmodel.0.layer3.1.bn2.weight torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn2.bias torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn2.running_mean torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn2.running_var torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer4.0.conv1.weight torch.Size([512, 256, 3, 3])\n",
      "seqmodel.0.layer4.0.bn1.weight torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn1.bias torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn1.running_mean torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn1.running_var torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "seqmodel.0.layer4.0.bn2.weight torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn2.bias torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn2.running_mean torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn2.running_var torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "seqmodel.0.layer4.0.downsample.1.weight torch.Size([512])\n",
      "seqmodel.0.layer4.0.downsample.1.bias torch.Size([512])\n",
      "seqmodel.0.layer4.0.downsample.1.running_mean torch.Size([512])\n",
      "seqmodel.0.layer4.0.downsample.1.running_var torch.Size([512])\n",
      "seqmodel.0.layer4.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer4.1.conv1.weight torch.Size([512, 512, 3, 3])\n",
      "seqmodel.0.layer4.1.bn1.weight torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn1.bias torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn1.running_mean torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn1.running_var torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "seqmodel.0.layer4.1.bn2.weight torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn2.bias torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn2.running_mean torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn2.running_var torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.fc.weight torch.Size([1000, 512])\n",
      "seqmodel.0.fc.bias torch.Size([1000])\n",
      "seqmodel.1.weight torch.Size([10, 1000])\n",
      "seqmodel.1.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "from gdeep.models import ModelExtractor\n",
    "\n",
    "me = ModelExtractor(pipe.model, loss_fn)\n",
    "\n",
    "lista = me.get_layers_param()\n",
    "\n",
    "for k, item in lista.items():\n",
    "    print(k,item.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the decision boundary computations:\n",
      "Step: 0/1\r"
     ]
    }
   ],
   "source": [
    "x = next(iter(dl_tr))[0][0]\n",
    "if x.dtype is not torch.int64:\n",
    "    res = me.get_decision_boundary(x, n_epochs=1)\n",
    "    res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = next(iter(dl_tr))[0]\n",
    "list_activations = me.get_activations(x)\n",
    "len(list_activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 7, 7])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([128, 64, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 64, 1, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([256, 128, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 128, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([512, 256, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 256, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([1000, 512])\n",
      "torch.Size([1000])\n",
      "torch.Size([10, 1000])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "x, target = next(iter(dl_tr))\n",
    "if x.dtype is torch.float:\n",
    "    for gradient in me.get_gradients(x, target=target)[1]:\n",
    "        print(gradient.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise activations and other topological aspects of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vs.plot_data_model()\n",
    "# vs.plot_activations(x)\n",
    "vs.plot_persistence_diagrams(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model\n",
    "\n",
    "In the next section we compute the confusion matrix on the entire training dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 12.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(35.625,\n",
       " 2.667057013511658,\n",
       " array([[ 1.,  1.,  1.,  2.,  0.,  0.,  0.,  3., 17.,  0.],\n",
       "        [ 0., 26.,  0.,  2.,  0.,  0.,  0.,  4., 14.,  1.],\n",
       "        [ 0.,  2.,  7.,  9.,  0.,  0.,  0.,  7.,  4.,  0.],\n",
       "        [ 0.,  4.,  0., 19.,  0.,  1.,  0.,  3.,  5.,  0.],\n",
       "        [ 0.,  0.,  1.,  6.,  4.,  0.,  0., 13.,  3.,  0.],\n",
       "        [ 0., 11.,  3.,  9.,  0.,  1.,  0.,  3.,  0.,  0.],\n",
       "        [ 0.,  6.,  1., 10.,  0.,  0.,  6., 11.,  3.,  0.],\n",
       "        [ 0.,  1.,  1.,  2.,  0.,  1.,  0., 18.,  5.,  1.],\n",
       "        [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0., 26.,  0.],\n",
       "        [ 0.,  7.,  0.,  0.,  0.,  0.,  0.,  3., 24.,  6.]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.evaluate_classification(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
