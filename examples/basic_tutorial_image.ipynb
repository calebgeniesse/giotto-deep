{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: image data\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with the basic functioning of *giotto-deep* API.\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. creation of a dataset\n",
    " 2. creation of a model\n",
    " 3. define metrics and losses\n",
    " 4. run benchmarks\n",
    " 5. visualise results interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from gdeep.models import FFNet\n",
    "\n",
    "from gdeep.visualisation import  persistence_diagrams_of_activations\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gdeep.data import TorchDataLoader\n",
    "\n",
    "\n",
    "from gtda.diagrams import BettiCurve\n",
    "\n",
    "from gtda.plotting import plot_betti_surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the reuslts of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "dl = TorchDataLoader(name=\"CIFAR10\")\n",
    "\n",
    "# use only 320 images from cifar10\n",
    "train_indices = list(range(32*10))\n",
    "dl_tr, dl_ts = dl.build_dataloaders(batch_size=32, sampler=SubsetRandomSampler(train_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPUs...\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "from gdeep.pipeline import Pipeline\n",
    "\n",
    "# wrap a sequential model in a torch nn.Module\n",
    "class model3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model3, self).__init__()\n",
    "        self.seqmodel = nn.Sequential(models.resnet18(pretrained=True), nn.Linear(1000,10))\n",
    "    def forward(self, X):\n",
    "        return self.seqmodel(X)\n",
    "\n",
    "model = model3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  3.066014349460602  \tBatch training accuracy:  16.015625  \t[ 8 / 8 ]                               \n",
      "Time taken for this epoch: 3.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " Accuracy: 12.500000%,                 Avg loss: 2.894294 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Batch training loss:  2.322780966758728  \tBatch training accuracy:  38.28125  \t[ 8 / 8 ]                              \n",
      "Time taken for this epoch: 3.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " Accuracy: 18.750000%,                 Avg loss: 2.977599 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Batch training loss:  1.535795383155346  \tBatch training accuracy:  46.484375  \t[ 8 / 8 ]                              \n",
      "Time taken for this epoch: 2.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " Accuracy: 23.437500%,                 Avg loss: 2.656071 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.6560710668563843, 23.4375)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "# print(model)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "pipe = Pipeline(model, (dl_tr, dl_ts), loss_fn, writer)\n",
    "\n",
    "# train the model\n",
    "pipe.train(SGD, 3, False, {\"lr\":0.01}, {\"batch_size\":32, \"sampler\":SubsetRandomSampler(train_indices)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simply use interpretability tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matteocaorsi/opt/anaconda3/lib/python3.9/site-packages/captum/_utils/gradient.py:56: UserWarning:\n",
      "\n",
      "Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "\n",
      "/Users/matteocaorsi/opt/anaconda3/lib/python3.9/site-packages/captum/attr/_core/guided_backprop_deconvnet.py:59: UserWarning:\n",
      "\n",
      "Setting backward hooks on ReLU activations.The hooks will be removed after the attribution is finished\n",
      "\n",
      "/Users/matteocaorsi/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1025: UserWarning:\n",
      "\n",
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gdeep.analysis.interpretability import Interpreter\n",
    "from gdeep.visualisation import Visualiser\n",
    "\n",
    "inter = Interpreter(pipe.model, method=\"GuidedGradCam\")\n",
    "output = inter.interpret_image(next(iter(dl_tr))[0][0].reshape(1,3,32,32), \n",
    "                      1, pipe.model.seqmodel[0].layer2[0].conv1);\n",
    "\n",
    "# visualise the interpreter\n",
    "vs = Visualiser(pipe)\n",
    "try:\n",
    "    vs.plot_interpreter_image(inter);\n",
    "except AssertionError:\n",
    "    print(\"The heatmap is made of all zeros...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract inner data from your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seqmodel.0.conv1.weight torch.Size([64, 3, 7, 7])\n",
      "seqmodel.0.bn1.weight torch.Size([64])\n",
      "seqmodel.0.bn1.bias torch.Size([64])\n",
      "seqmodel.0.bn1.running_mean torch.Size([64])\n",
      "seqmodel.0.bn1.running_var torch.Size([64])\n",
      "seqmodel.0.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer1.0.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "seqmodel.0.layer1.0.bn1.weight torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn1.bias torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn1.running_mean torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn1.running_var torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer1.0.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "seqmodel.0.layer1.0.bn2.weight torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn2.bias torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn2.running_mean torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn2.running_var torch.Size([64])\n",
      "seqmodel.0.layer1.0.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer1.1.conv1.weight torch.Size([64, 64, 3, 3])\n",
      "seqmodel.0.layer1.1.bn1.weight torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn1.bias torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn1.running_mean torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn1.running_var torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer1.1.conv2.weight torch.Size([64, 64, 3, 3])\n",
      "seqmodel.0.layer1.1.bn2.weight torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn2.bias torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn2.running_mean torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn2.running_var torch.Size([64])\n",
      "seqmodel.0.layer1.1.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer2.0.conv1.weight torch.Size([128, 64, 3, 3])\n",
      "seqmodel.0.layer2.0.bn1.weight torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn1.bias torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn1.running_mean torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn1.running_var torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer2.0.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "seqmodel.0.layer2.0.bn2.weight torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn2.bias torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn2.running_mean torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn2.running_var torch.Size([128])\n",
      "seqmodel.0.layer2.0.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer2.0.downsample.0.weight torch.Size([128, 64, 1, 1])\n",
      "seqmodel.0.layer2.0.downsample.1.weight torch.Size([128])\n",
      "seqmodel.0.layer2.0.downsample.1.bias torch.Size([128])\n",
      "seqmodel.0.layer2.0.downsample.1.running_mean torch.Size([128])\n",
      "seqmodel.0.layer2.0.downsample.1.running_var torch.Size([128])\n",
      "seqmodel.0.layer2.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer2.1.conv1.weight torch.Size([128, 128, 3, 3])\n",
      "seqmodel.0.layer2.1.bn1.weight torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn1.bias torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn1.running_mean torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn1.running_var torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer2.1.conv2.weight torch.Size([128, 128, 3, 3])\n",
      "seqmodel.0.layer2.1.bn2.weight torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn2.bias torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn2.running_mean torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn2.running_var torch.Size([128])\n",
      "seqmodel.0.layer2.1.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer3.0.conv1.weight torch.Size([256, 128, 3, 3])\n",
      "seqmodel.0.layer3.0.bn1.weight torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn1.bias torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn1.running_mean torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn1.running_var torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer3.0.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "seqmodel.0.layer3.0.bn2.weight torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn2.bias torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn2.running_mean torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn2.running_var torch.Size([256])\n",
      "seqmodel.0.layer3.0.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer3.0.downsample.0.weight torch.Size([256, 128, 1, 1])\n",
      "seqmodel.0.layer3.0.downsample.1.weight torch.Size([256])\n",
      "seqmodel.0.layer3.0.downsample.1.bias torch.Size([256])\n",
      "seqmodel.0.layer3.0.downsample.1.running_mean torch.Size([256])\n",
      "seqmodel.0.layer3.0.downsample.1.running_var torch.Size([256])\n",
      "seqmodel.0.layer3.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer3.1.conv1.weight torch.Size([256, 256, 3, 3])\n",
      "seqmodel.0.layer3.1.bn1.weight torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn1.bias torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn1.running_mean torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn1.running_var torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer3.1.conv2.weight torch.Size([256, 256, 3, 3])\n",
      "seqmodel.0.layer3.1.bn2.weight torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn2.bias torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn2.running_mean torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn2.running_var torch.Size([256])\n",
      "seqmodel.0.layer3.1.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer4.0.conv1.weight torch.Size([512, 256, 3, 3])\n",
      "seqmodel.0.layer4.0.bn1.weight torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn1.bias torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn1.running_mean torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn1.running_var torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer4.0.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "seqmodel.0.layer4.0.bn2.weight torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn2.bias torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn2.running_mean torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn2.running_var torch.Size([512])\n",
      "seqmodel.0.layer4.0.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer4.0.downsample.0.weight torch.Size([512, 256, 1, 1])\n",
      "seqmodel.0.layer4.0.downsample.1.weight torch.Size([512])\n",
      "seqmodel.0.layer4.0.downsample.1.bias torch.Size([512])\n",
      "seqmodel.0.layer4.0.downsample.1.running_mean torch.Size([512])\n",
      "seqmodel.0.layer4.0.downsample.1.running_var torch.Size([512])\n",
      "seqmodel.0.layer4.0.downsample.1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer4.1.conv1.weight torch.Size([512, 512, 3, 3])\n",
      "seqmodel.0.layer4.1.bn1.weight torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn1.bias torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn1.running_mean torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn1.running_var torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn1.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.layer4.1.conv2.weight torch.Size([512, 512, 3, 3])\n",
      "seqmodel.0.layer4.1.bn2.weight torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn2.bias torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn2.running_mean torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn2.running_var torch.Size([512])\n",
      "seqmodel.0.layer4.1.bn2.num_batches_tracked torch.Size([])\n",
      "seqmodel.0.fc.weight torch.Size([1000, 512])\n",
      "seqmodel.0.fc.bias torch.Size([1000])\n",
      "seqmodel.1.weight torch.Size([10, 1000])\n",
      "seqmodel.1.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "from gdeep.models import ModelExtractor\n",
    "\n",
    "me = ModelExtractor(pipe.model, loss_fn)\n",
    "\n",
    "lista = me.get_layers_param()\n",
    "\n",
    "for k, item in lista.items():\n",
    "    print(k,item.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the decision boundary computations:\n",
      "Step: 0/1\r"
     ]
    }
   ],
   "source": [
    "x = next(iter(dl_tr))[0][0]\n",
    "if x.dtype is not torch.int64:\n",
    "    res = me.get_decision_boundary(x, n_epochs=1)\n",
    "    res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = next(iter(dl_tr))[0]\n",
    "list_activations = me.get_activations(x)\n",
    "len(list_activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 7, 7])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([64, 64, 3, 3])\n",
      "torch.Size([64])\n",
      "torch.Size([64])\n",
      "torch.Size([128, 64, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 64, 1, 1])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([128, 128, 3, 3])\n",
      "torch.Size([128])\n",
      "torch.Size([128])\n",
      "torch.Size([256, 128, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 128, 1, 1])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256, 3, 3])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([512, 256, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 256, 1, 1])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512, 512, 3, 3])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([1000, 512])\n",
      "torch.Size([1000])\n",
      "torch.Size([10, 1000])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "x, target = next(iter(dl_tr))\n",
    "if x.dtype is torch.float:\n",
    "    for gradient in me.get_gradients(x, target=target)[1]:\n",
    "        print(gradient.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise activations and other topological aspects of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vs.plot_data_model()\n",
    "# vs.plot_activations(x)\n",
    "vs.plot_persistence_diagrams(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model\n",
    "\n",
    "In the next section we compute the confusion matrix on the entire training dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 10.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(21.25,\n",
       " 3.1018550157547,\n",
       " array([[ 1.,  0.,  2.,  2.,  9.,  1.,  9.,  0.,  0.,  1.],\n",
       "        [ 0.,  3.,  4.,  0., 15.,  1., 15.,  0.,  0.,  9.],\n",
       "        [ 0.,  0.,  3.,  1., 15.,  1.,  7.,  1.,  0.,  1.],\n",
       "        [ 0.,  0.,  1.,  3., 19.,  0.,  6.,  1.,  0.,  2.],\n",
       "        [ 0.,  0.,  1.,  0., 22.,  0.,  3.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  2., 10.,  7.,  5.,  1.,  0.,  1.],\n",
       "        [ 0.,  0.,  1.,  0., 17.,  0., 17.,  2.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0., 15.,  1.,  4.,  4.,  1.,  3.],\n",
       "        [ 0.,  0.,  0.,  0.,  8.,  2., 17.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  1.,  1., 20.,  2.,  4.,  1.,  2.,  8.]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.evaluate_classification(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
