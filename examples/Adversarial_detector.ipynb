{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce MNIST diagram dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.topactivation import TopactivationFC as TFC\n",
    "from gdeep.pipeline import Pipeline\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from gdeep.models import FFNet\n",
    "from torch import nn\n",
    "from gdeep.data import TorchDataLoader\n",
    "from gdeep.topology_layers.preprocessing import convert_gudhi_extended_persistence_to_persformer_input\n",
    "\n",
    "writer = SummaryWriter()\n",
    "dl = TorchDataLoader(name=\"MNIST\")\n",
    "dl_tr, dl_ts = dl.build_dataloaders(batch_size=32)\n",
    "\n",
    "arch = [28*28, 50, 50, 10]\n",
    "model = nn.Sequential(nn.Flatten(), FFNet( arch= arch ))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "pipe = Pipeline(model, (dl_tr, dl_ts), loss_fn, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.387287 \tEpoch training accuracy: 89.07%                                      00 ]                      2.942477876106196  \t[ 113 / 1500 ]                       \t[ 1442 / 1500 ]                     \n",
      "Time taken for this epoch: 12.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 93.71%,                 Avg loss: 0.206566 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.20656558565795421, 93.70833333333334)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "n_epochs = 10\n",
    "topactiv = TFC(pipe,arch)\n",
    "topactiv.pipe.train(Adam, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "epsilon = 0.1\n",
    "diagrams_MNIST= []\n",
    "labels_MNIST = []\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "counter = 0\n",
    "labels = []\n",
    "for data, target in dl_tr:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    diagrams_batch, labels_batch = topactiv.get_extended_persistence_randomly_perturbed(\n",
    "        data, target, epsilon)\n",
    "    for diagram, label in zip(diagrams_batch, labels_batch):\n",
    "        diagram_one_hot = convert_gudhi_extended_persistence_to_persformer_input([diagram])[0]\n",
    "        # Save diagram_one_hot to npy file\n",
    "        np.save(f\"data/adversarial_MNIST_10_0.1/diagrams/{counter}.npy\", diagram_one_hot)\n",
    "        labels.append(int(label))\n",
    "        counter += 1\n",
    "\n",
    "# Save labels to csv file\n",
    "np.savetxt(\"data/adversarial_MNIST_10_0.1/labels.csv\", labels, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/berkouknicolas/giotto-deep/examples/Adversarial_detector.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2233352e3231342e3135382e3339222c2275736572223a226265726b6f756b6e69636f6c6173227d/home/berkouknicolas/giotto-deep/examples/Adversarial_detector.ipynb#ch0000039vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgdeep\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtopology_layers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2233352e3231342e3135382e3339222c2275736572223a226265726b6f756b6e69636f6c6173227d/home/berkouknicolas/giotto-deep/examples/Adversarial_detector.ipynb#ch0000039vscode-remote?line=2'>3</a>\u001b[0m diagrams_formated \u001b[39m=\u001b[39m convert_gudhi_extended_persistence_to_persformer_input(diagrams_MNIST)\n",
      "File \u001b[0;32m~/giotto-deep/gdeep/topology_layers/preprocessing.py:49\u001b[0m, in \u001b[0;36mconvert_gudhi_extended_persistence_to_persformer_input\u001b[0;34m(diagrams)\u001b[0m\n\u001b[1;32m     <a href='file:///home/berkouknicolas/giotto-deep/gdeep/topology_layers/preprocessing.py?line=43'>44</a>\u001b[0m     encoded_diagrams_list\u001b[39m.\u001b[39mappend(encoded_diagram)\n\u001b[1;32m     <a href='file:///home/berkouknicolas/giotto-deep/gdeep/topology_layers/preprocessing.py?line=45'>46</a>\u001b[0m \u001b[39m# concatenate all diagrams into a single np.array of shape\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/berkouknicolas/giotto-deep/gdeep/topology_layers/preprocessing.py?line=46'>47</a>\u001b[0m \u001b[39m# (input_size, max_num_points, 6) by filling the remaining\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/berkouknicolas/giotto-deep/gdeep/topology_layers/preprocessing.py?line=47'>48</a>\u001b[0m \u001b[39m# entries with zeros\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/berkouknicolas/giotto-deep/gdeep/topology_layers/preprocessing.py?line=48'>49</a>\u001b[0m max_num_points \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39;49m(\u001b[39mlen\u001b[39;49m(diagram) \u001b[39mfor\u001b[39;49;00m diagram \u001b[39min\u001b[39;49;00m encoded_diagrams_list)\n\u001b[1;32m     <a href='file:///home/berkouknicolas/giotto-deep/gdeep/topology_layers/preprocessing.py?line=49'>50</a>\u001b[0m encoded_diagrams \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((input_size, max_num_points, \u001b[39m6\u001b[39m))\n\u001b[1;32m     <a href='file:///home/berkouknicolas/giotto-deep/gdeep/topology_layers/preprocessing.py?line=50'>51</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, diagram \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(encoded_diagrams_list):  \u001b[39m# type: ignore\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "from gdeep.topology_layers.preprocessing import *\n",
    "\n",
    "diagrams_formated = convert_gudhi_extended_persistence_to_persformer_input(diagrams_MNIST)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"diagrams_MNIST_test\" + n_epochs \"_\" + epsilon + \".pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(diagrams_formated, fp)\n",
    "\n",
    "with open(\"labels_MNIST_test\" + n_epochs \"_\" + epsilon + \".pkl\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(labels, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "ratio = 0.1\n",
    "indices = random.sample(range(len(diagrams_MNIST)), int(ratio * len(diagrams_MNIST)))\n",
    "diagrams_subsample = [diagrams_MNIST[indice] for indice in indices]\n",
    "labels_subsample = [labels[indice] for indice in indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"diagrams_MNIST_subsample.pkl\", \"wb\") as f1:\n",
    "    pickle.dump(diagrams_subsample, f1)\n",
    "with open(\"labels_subsample.pkl\", \"wb\") as f2:\n",
    "    pickle.dump(labels_subsample, f2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "n_bars = 500\n",
    "\n",
    "def get_longest_bars(diagram,k):\n",
    "    f = lambda l : - abs(l[1] - l[0])\n",
    "    k_longest_bars = sorted(diagram.tolist(), key = f)\n",
    "    return np.array(k_longest_bars[0:k])\n",
    "\n",
    "diagrams_subsample_k = [get_longest_bars(diagram, n_bars) for diagram in diagrams_subsample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"diagrams_MNIST_subsample_500.pkl\", \"wb\") as f1:\n",
    "    pickle.dump(diagrams_subsample_k, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, KFold, ShuffleSplit\n",
    "from tensorflow import random_uniform_initializer as rui\n",
    "import gudhi.representations as tda\n",
    "\n",
    "### Uncomment the following to process your diagrams (necessary)\n",
    "thresh = 500\n",
    "\n",
    "# Whole pipeline\n",
    "tmp = Pipeline([\n",
    "        (\"Selector\",      tda.DiagramSelector(use=True, point_type=\"finite\")),\n",
    "        (\"ProminentPts\",  tda.ProminentPoints(use=True, num_pts=thresh)),\n",
    "        (\"Scaler\",        tda.DiagramScaler(use=True, scalers=[([0,1], MinMaxScaler())])),\n",
    "        (\"Padding\",       tda.Padding(use=True)),\n",
    "                ])\n",
    "\n",
    "prm = {filt: {\"ProminentPts__num_pts\": min(thresh, max([len(dgm) for dgm in diags_dict[filt]]))} \n",
    "       for filt in diags_dict.keys() if max([len(dgm) for dgm in diags_dict[filt]]) > 0}\n",
    "\n",
    "# Apply the previous pipeline on the different filtrations.\n",
    "diags = []\n",
    "for dt in prm.keys():\n",
    "    param = prm[dt]\n",
    "    tmp.set_params(**param)\n",
    "    diags.append(tmp.fit_transform(diags_dict[dt]))\n",
    "\n",
    "# For each filtration, concatenate all diagrams in a single array.\n",
    "D, npts = [], len(diags[0])\n",
    "for dt in range(len(prm.keys())):\n",
    "    D.append(np.array(np.concatenate([diags[dt][i][np.newaxis,:] for i in range(npts)],axis=0),dtype=np.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import perslay as pl\n",
    "perslayParameters = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "perslayParameters[\"pweight_train\"]  = False\n",
    "perslayParameters[\"pweight\"]        = \"gmix\"\n",
    "perslayParameters[\"pweight_num\"]    = 3\n",
    "perslayParameters[\"pweight_init\"]   = np.array(np.vstack([np.random.uniform(0.,1.,[2,3]), \n",
    "                                                          5.*np.ones([2,3])]), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "perslayParameters[\"perm_op\"] = \"mean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "perslayParameters[\"layer\"]           = \"Image\"\n",
    "perslayParameters[\"layer_train\"]     = False\n",
    "perslayParameters[\"image_size\"]      = (100, 100)\n",
    "perslayParameters[\"image_bnds\"]      = ((-.501, 1.501), (-.501, 1.501))\n",
    "perslayParameters[\"lvariance_init\"]  = .1\n",
    "perslayParameters[\"final_model\"]     = tf.keras.Sequential([tf.keras.layers.Flatten()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pl.PerslayModel(name=\"perslay\", diagdim=2, perslay_parameters=[perslayParameters], rho=\"identity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import itertools\n",
    "import h5py\n",
    "import numpy              as np\n",
    "import matplotlib.pyplot  as plt\n",
    "import pandas             as pd\n",
    "import tensorflow         as tf\n",
    "import tensorflow_addons  as tfa\n",
    "import gudhi              as gd\n",
    "\n",
    "from scipy.sparse           import csgraph\n",
    "from scipy.io               import loadmat\n",
    "from scipy.linalg           import eigh\n",
    "from sklearn.preprocessing  import LabelEncoder, OneHotEncoder\n",
    "from tensorflow             import random_uniform_initializer as rui\n",
    "from perslay                import PerslayModel\n",
    "\n",
    "\n",
    "def get_parameters(dataset):\n",
    "    if dataset == \"MUTAG\" or dataset == \"PROTEINS\":\n",
    "        dataset_parameters = {\"data_type\": \"graph\", \"filt_names\": [\"Ord0_10.0-hks\", \"Rel1_10.0-hks\", \"Ext0_10.0-hks\", \"Ext1_10.0-hks\"]}\n",
    "    elif dataset == \"COX2\" or dataset == \"DHFR\" or dataset == \"NCI1\" or dataset == \"NCI109\" or dataset == \"IMDB-BINARY\" or dataset == \"IMDB-MULTI\":\n",
    "        dataset_parameters = {\"data_type\": \"graph\", \"filt_names\": [\"Ord0_0.1-hks\", \"Rel1_0.1-hks\", \"Ext0_0.1-hks\", \"Ext1_0.1-hks\", \"Ord0_10.0-hks\", \"Rel1_10.0-hks\", \"Ext0_10.0-hks\", \"Ext1_10.0-hks\"]}\n",
    "    elif dataset == \"ORBIT5K\" or dataset == \"ORBIT100K\":\n",
    "        dataset_parameters = {\"data_type\": \"orbit\", \"filt_names\": [\"Alpha0\", \"Alpha1\"]}\n",
    "    return dataset_parameters\n",
    "\n",
    "def get_model(dataset, nf):\n",
    "\n",
    "    if dataset == \"MUTAG\":\n",
    "\n",
    "        plp = {}\n",
    "        plp[\"pweight\"]        = \"grid\"\n",
    "        plp[\"pweight_init\"]   = rui(1., 1.)\n",
    "        plp[\"pweight_size\"]   = (10, 10)\n",
    "        plp[\"pweight_bnds\"]   = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"pweight_train\"]  = True\n",
    "        plp[\"layer\"]          = \"Image\"\n",
    "        plp[\"image_size\"]     = (20, 20)\n",
    "        plp[\"image_bnds\"]     = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"lvariance_init\"] = rui(3., 3.)\n",
    "        plp[\"layer_train\"]    = True\n",
    "        plp[\"perm_op\"]        = \"sum\"\n",
    "        perslay_parameters    = [plp for _ in range(4)]\n",
    "\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "        with mirrored_strategy.scope():\n",
    "            for i in range(4):\n",
    "                fmodel = tf.keras.Sequential([tf.keras.layers.Conv2D(10, 2, input_shape=(21,21,1)), tf.keras.layers.Flatten()])\n",
    "                perslay_parameters[i][\"final_model\"] = fmodel\n",
    "            rho = tf.keras.Sequential([tf.keras.layers.Dense(2, activation=\"sigmoid\", input_shape=(16000+nf,))])\n",
    "            model = PerslayModel(name=\"PersLay\", diagdim=2, perslay_parameters=perslay_parameters, rho=rho)\n",
    "            lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=20, decay_rate=0.5, staircase=True)\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-4)\n",
    "            optimizer = tfa.optimizers.MovingAverage(optimizer, average_decay=0.9) \n",
    "            loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "            metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "    elif dataset == \"PROTEINS\":\n",
    "\n",
    "        plp = {}\n",
    "        plp[\"pweight\"]        = \"grid\"\n",
    "        plp[\"pweight_init\"]   = rui(1., 1.)\n",
    "        plp[\"pweight_size\"]   = (10, 10)\n",
    "        plp[\"pweight_bnds\"]   = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"pweight_train\"]  = True\n",
    "        plp[\"layer\"]          = \"Image\"\n",
    "        plp[\"image_size\"]     = (15, 15)\n",
    "        plp[\"image_bnds\"]     = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"lvariance_init\"] = rui(3., 3.)\n",
    "        plp[\"layer_train\"]    = True\n",
    "        plp[\"perm_op\"]        = \"sum\"\n",
    "        perslay_parameters    = [plp for _ in range(4)]\n",
    "\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "        with mirrored_strategy.scope():\n",
    "            for i in range(4):\n",
    "                fmodel = tf.keras.Sequential([tf.keras.layers.Conv2D(10, 2, input_shape=(16,16,1)), tf.keras.layers.Flatten()])\n",
    "                perslay_parameters[i][\"final_model\"] = fmodel\n",
    "            rho = tf.keras.Sequential([tf.keras.layers.Dense(2, activation=\"sigmoid\", input_shape=(9000+nf,))])\n",
    "            model = PerslayModel(name=\"PersLay\", diagdim=2, perslay_parameters=perslay_parameters, rho=rho)\n",
    "            lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=20, decay_rate=0.5, staircase=True)\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-4)\n",
    "            optimizer = tfa.optimizers.MovingAverage(optimizer, average_decay=0.9) \n",
    "            loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "            metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "    elif dataset == \"NCI1\" or dataset == \"NCI109\":\n",
    "\n",
    "        plp = {}\n",
    "        plp[\"pweight\"]        = \"grid\"\n",
    "        plp[\"pweight_init\"]   = rui(1., 1.)\n",
    "        plp[\"pweight_size\"]   = (10, 10)\n",
    "        plp[\"pweight_bnds\"]   = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"pweight_train\"]  = True\n",
    "        plp[\"layer\"]          = \"PermutationEquivariant\"\n",
    "        plp[\"lpeq\"]           = [(25, None), (25, \"max\")]\n",
    "        plp[\"layer_train\"]    = True\n",
    "        plp[\"perm_op\"]        = \"sum\"\n",
    "        plp[\"final_model\"]    = \"identity\"\n",
    "        perslay_parameters    = [plp for _ in range(8)]\n",
    "\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "        with mirrored_strategy.scope():\n",
    "            rho = tf.keras.Sequential([tf.keras.layers.Dense(2, activation=\"sigmoid\", input_shape=(200+nf,))])\n",
    "            model = PerslayModel(name=\"PersLay\", diagdim=2, perslay_parameters=perslay_parameters, rho=rho)\n",
    "            lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=20, decay_rate=0.5, staircase=True)\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-4)\n",
    "            optimizer = tfa.optimizers.MovingAverage(optimizer, average_decay=0.9) \n",
    "            loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "            metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "    elif dataset == \"IMDB-MULTI\" or dataset == \"IMDB-BINARY\":\n",
    "\n",
    "        nlab = 2 if dataset == \"IMDB-BINARY\" else 3\n",
    "        plp = {}\n",
    "        plp[\"pweight\"]        = \"grid\"\n",
    "        plp[\"pweight_init\"]   = rui(1., 1.)\n",
    "        plp[\"pweight_size\"]   = (20, 20)\n",
    "        plp[\"pweight_bnds\"]   = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"pweight_train\"]  = True\n",
    "        plp[\"layer\"]          = \"Image\"\n",
    "        plp[\"image_size\"]     = (20, 20)\n",
    "        plp[\"image_bnds\"]     = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"lvariance_init\"] = rui(3., 3.)\n",
    "        plp[\"layer_train\"]    = True\n",
    "        plp[\"perm_op\"]        = \"sum\"\n",
    "        perslay_parameters    = [plp for _ in range(8)]\n",
    "\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "        with mirrored_strategy.scope():\n",
    "            for i in range(8):\n",
    "                fmodel = tf.keras.Sequential([tf.keras.layers.Conv2D(10, 2, input_shape=(21,21,1)), tf.keras.layers.Flatten()])\n",
    "                perslay_parameters[i][\"final_model\"] = fmodel\n",
    "            rho = tf.keras.Sequential([tf.keras.layers.Dense(nlab, activation=\"sigmoid\", input_shape=(32000+nf,))])\n",
    "            model = PerslayModel(name=\"PersLay\", diagdim=2, perslay_parameters=perslay_parameters, rho=rho)\n",
    "            lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=20, decay_rate=0.5, staircase=True)\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-4)\n",
    "            optimizer = tfa.optimizers.MovingAverage(optimizer, average_decay=0.9) \n",
    "            loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "            metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "    elif dataset == \"COX2\" or dataset == \"DHFR\":\n",
    "\n",
    "        plp = {}\n",
    "        plp[\"pweight\"]        = \"grid\"\n",
    "        plp[\"pweight_init\"]   = rui(1., 1.)\n",
    "        plp[\"pweight_size\"]   = (10, 10)\n",
    "        plp[\"pweight_bnds\"]   = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"pweight_train\"]  = True\n",
    "        plp[\"layer\"]          = \"Image\"\n",
    "        plp[\"image_size\"]     = (20, 20)\n",
    "        plp[\"image_bnds\"]     = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"lvariance_init\"] = rui(3., 3.)\n",
    "        plp[\"layer_train\"]    = True\n",
    "        plp[\"perm_op\"]        = \"sum\"\n",
    "        perslay_parameters    = [plp for _ in range(8)]\n",
    "\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "        with mirrored_strategy.scope():\n",
    "            for i in range(8):\n",
    "                fmodel = tf.keras.Sequential([tf.keras.layers.Conv2D(10, 2, input_shape=(21,21,1)), tf.keras.layers.Flatten()])\n",
    "                perslay_parameters[i][\"final_model\"] = fmodel\n",
    "            rho = tf.keras.Sequential([tf.keras.layers.Dense(2, activation=\"sigmoid\", input_shape=(32000+nf,))])\n",
    "            model = PerslayModel(name=\"PersLay\", diagdim=2, perslay_parameters=perslay_parameters, rho=rho)\n",
    "            lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=20, decay_rate=0.5, staircase=True)\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-4)\n",
    "            optimizer = tfa.optimizers.MovingAverage(optimizer, average_decay=0.9) \n",
    "            loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "            metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "    elif dataset == \"ORBIT5K\" or dataset == \"ORBIT100K\":\n",
    "\n",
    "        plp = {}\n",
    "        plp[\"pweight\"]        = \"grid\"\n",
    "        plp[\"pweight_init\"]   = rui(1., 1.)\n",
    "        plp[\"pweight_size\"]   = (10, 10)\n",
    "        plp[\"pweight_bnds\"]   = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"pweight_train\"]  = True\n",
    "        plp[\"layer\"]          = \"PermutationEquivariant\"\n",
    "        plp[\"lpeq\"]           = [(25, None), (25, \"max\")]\n",
    "        plp[\"lweight_init\"]   = rui(0.,1.)\n",
    "        plp[\"lbias_init\"]     = rui(0.,1.)\n",
    "        plp[\"lgamma_init\"]    = rui(0.,1.)\n",
    "        plp[\"layer_train\"]    = True\n",
    "        plp[\"perm_op\"]        = \"topk\"\n",
    "        plp[\"keep\"]           = 5\n",
    "        plp[\"final_model\"]    = \"identity\"\n",
    "        perslay_parameters    = [plp for _ in range(2)]\n",
    "\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "        with mirrored_strategy.scope():\n",
    "            rho = tf.keras.Sequential([tf.keras.layers.Dense(5, activation=\"sigmoid\", input_shape=(250+nf,))])\n",
    "            model = PerslayModel(name=\"PersLay\", diagdim=2, perslay_parameters=perslay_parameters, rho=rho)\n",
    "            lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=20, decay_rate=1., staircase=True)\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-4)\n",
    "            optimizer = tfa.optimizers.MovingAverage(optimizer, average_decay=0.9) \n",
    "            loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "            metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "    return model, optimizer, loss, metrics\n",
    "\n",
    "def hks_signature(eigenvectors, eigenvals, time):\n",
    "    return np.square(eigenvectors).dot(np.diag(np.exp(-time * eigenvals))).sum(axis=1)\n",
    "\n",
    "def generate_orbit(num_pts_per_orbit, param):\n",
    "    X = np.zeros([num_pts_per_orbit, 2])\n",
    "    xcur, ycur = np.random.rand(), np.random.rand()\n",
    "    for idx in range(num_pts_per_orbit):\n",
    "        xcur = (xcur + param * ycur * (1. - ycur)) % 1\n",
    "        ycur = (ycur + param * xcur * (1. - xcur)) % 1\n",
    "        X[idx, :] = [xcur, ycur]\n",
    "    return X\n",
    "\n",
    "def apply_graph_extended_persistence(A, filtration_val):\n",
    "    num_vertices = A.shape[0]\n",
    "    (xs, ys) = np.where(np.triu(A))\n",
    "    st = gd.SimplexTree()\n",
    "    for i in range(num_vertices):\n",
    "        st.insert([i], filtration=-1e10)\n",
    "    for idx, x in enumerate(xs):        \n",
    "        st.insert([x, ys[idx]], filtration=-1e10)\n",
    "    for i in range(num_vertices):\n",
    "        st.assign_filtration([i], filtration_val[i])\n",
    "    st.make_filtration_non_decreasing()\n",
    "    st.extend_filtration()\n",
    "    LD = st.extended_persistence()\n",
    "    dgmOrd0, dgmRel1, dgmExt0, dgmExt1 = LD[0], LD[1], LD[2], LD[3]\n",
    "    dgmOrd0 = np.vstack([np.array([[ min(p[1][0],p[1][1]), max(p[1][0],p[1][1]) ]]) for p in dgmOrd0 if p[0] == 0]) if len(dgmOrd0) else np.empty([0,2])\n",
    "    dgmRel1 = np.vstack([np.array([[ min(p[1][0],p[1][1]), max(p[1][0],p[1][1]) ]]) for p in dgmRel1 if p[0] == 1]) if len(dgmRel1) else np.empty([0,2])\n",
    "    dgmExt0 = np.vstack([np.array([[ min(p[1][0],p[1][1]), max(p[1][0],p[1][1]) ]]) for p in dgmExt0 if p[0] == 0]) if len(dgmExt0) else np.empty([0,2])\n",
    "    dgmExt1 = np.vstack([np.array([[ min(p[1][0],p[1][1]), max(p[1][0],p[1][1]) ]]) for p in dgmExt1 if p[0] == 1]) if len(dgmExt1) else np.empty([0,2])\n",
    "    return dgmOrd0, dgmExt0, dgmRel1, dgmExt1\n",
    "\n",
    "def generate_diagrams_and_features(dataset, path_dataset=\"\"):\n",
    "\n",
    "    dataset_parameters = get_parameters(dataset)\n",
    "    dataset_type = dataset_parameters[\"data_type\"]\n",
    "\n",
    "    if \"REDDIT\" in dataset:\n",
    "        print(\"Unfortunately, REDDIT data are not available yet for memory issues.\\n\")\n",
    "        print(\"Moreover, the link we used to download the data,\")\n",
    "        print(\"http://www.mit.edu/~pinary/kdd/datasets.tar.gz\")\n",
    "        print(\"is down at the commit time (May 23rd).\")\n",
    "        print(\"We will update this repository when we figure out a workaround.\")\n",
    "        return\n",
    "\n",
    "    path_dataset = \"./data/\" + dataset + \"/\" if not len(path_dataset) else path_dataset\n",
    "    if os.path.isfile(path_dataset + dataset + \".hdf5\"):\n",
    "        os.remove(path_dataset + dataset + \".hdf5\")\n",
    "    diag_file = h5py.File(path_dataset + dataset + \".hdf5\", \"w\")\n",
    "    list_filtrations = dataset_parameters[\"filt_names\"]\n",
    "    [diag_file.create_group(str(filtration)) for filtration in dataset_parameters[\"filt_names\"]]\n",
    "    \n",
    "    if dataset_type == \"graph\":\n",
    "\n",
    "        list_hks_times = np.unique([filtration.split(\"_\")[1] for filtration in list_filtrations])\n",
    "\n",
    "        # preprocessing\n",
    "        pad_size = 1\n",
    "        for graph_name in os.listdir(path_dataset + \"mat/\"):\n",
    "            A = np.array(loadmat(path_dataset + \"mat/\" + graph_name)[\"A\"], dtype=np.float32)\n",
    "            pad_size = np.max((A.shape[0], pad_size))\n",
    "\n",
    "        feature_names = [\"eval\"+str(i) for i in range(pad_size)] + [name+\"-percent\"+str(i) for name, i in itertools.product([f for f in list_hks_times if \"hks\" in f], 10*np.arange(11))]\n",
    "        features = pd.DataFrame(index=range(len(os.listdir(path_dataset + \"mat/\"))), columns=[\"label\"] + feature_names)\n",
    "\n",
    "        for idx, graph_name in enumerate((os.listdir(path_dataset + \"mat/\"))):\n",
    "\n",
    "            name = graph_name.split(\"_\")\n",
    "            gid = int(name[name.index(\"gid\") + 1]) - 1\n",
    "            A = np.array(loadmat(path_dataset + \"mat/\" + graph_name)[\"A\"], dtype=np.float32)\n",
    "            num_vertices = A.shape[0]\n",
    "            label = int(name[name.index(\"lb\") + 1])\n",
    "\n",
    "            L = csgraph.laplacian(A, normed=True)\n",
    "            egvals, egvectors = eigh(L)\n",
    "            eigenvectors = np.zeros([num_vertices, pad_size])\n",
    "            eigenvals = np.zeros(pad_size)\n",
    "            eigenvals[:min(pad_size, num_vertices)] = np.flipud(egvals)[:min(pad_size, num_vertices)]\n",
    "            eigenvectors[:, :min(pad_size, num_vertices)] = np.fliplr(egvectors)[:, :min(pad_size, num_vertices)]\n",
    "            graph_features = []\n",
    "            graph_features.append(eigenvals)\n",
    "\n",
    "            for fhks in list_hks_times:\n",
    "                hks_time = float(fhks.split(\"-\")[0])\n",
    "                filtration_val = hks_signature(egvectors, egvals, time=hks_time)\n",
    "                dgmOrd0, dgmExt0, dgmRel1, dgmExt1 = apply_graph_extended_persistence(A, filtration_val)\n",
    "                diag_file[\"Ord0_\" + str(hks_time) + \"-hks\"].create_dataset(name=str(gid), data=dgmOrd0)\n",
    "                diag_file[\"Ext0_\" + str(hks_time) + \"-hks\"].create_dataset(name=str(gid), data=dgmExt0)\n",
    "                diag_file[\"Rel1_\" + str(hks_time) + \"-hks\"].create_dataset(name=str(gid), data=dgmRel1)\n",
    "                diag_file[\"Ext1_\" + str(hks_time) + \"-hks\"].create_dataset(name=str(gid), data=dgmExt1)\n",
    "                graph_features.append(np.percentile(hks_signature(eigenvectors, eigenvals, time=hks_time), 10 * np.arange(11)))\n",
    "            features.loc[gid] = np.insert(np.concatenate(graph_features), 0, label)\n",
    "        features[\"label\"] = features[\"label\"].astype(int)\n",
    "\n",
    "    elif dataset_type == \"orbit\":\n",
    "\n",
    "        labs = []\n",
    "        count = 0\n",
    "        num_diag_per_param = 1000 if \"5K\" in dataset else 20000\n",
    "        for lab, r in enumerate([2.5, 3.5, 4.0, 4.1, 4.3]):\n",
    "            print(\"Generating\", num_diag_per_param, \"orbits and diagrams for r = \", r, \"...\")\n",
    "            for dg in range(num_diag_per_param):\n",
    "                X = generate_orbit(num_pts_per_orbit=1000, param=r)\n",
    "                alpha_complex = gd.AlphaComplex(points=X)\n",
    "                st = alpha_complex.create_simplex_tree(max_alpha_square=1e50)\n",
    "                st.persistence()\n",
    "                diag_file[\"Alpha0\"].create_dataset(name=str(count), data=np.array(st.persistence_intervals_in_dimension(0)))\n",
    "                diag_file[\"Alpha1\"].create_dataset(name=str(count), data=np.array(st.persistence_intervals_in_dimension(1)))\n",
    "                orbit_label = {\"label\": lab, \"pcid\": count}\n",
    "                labs.append(orbit_label)\n",
    "                count += 1\n",
    "        labels = pd.DataFrame(labs)\n",
    "        labels.set_index(\"pcid\")\n",
    "        features = labels[[\"label\"]]\n",
    "\n",
    "    features.to_csv(path_dataset + dataset + \".csv\")\n",
    "\n",
    "    return diag_file.close()\n",
    "\n",
    "def load_data(dataset, path_dataset=\"\", filtrations=[], verbose=False):\n",
    "\n",
    "    path_dataset = \"./data/\" + dataset + \"/\" if not len(path_dataset) else path_dataset\n",
    "    diagfile = h5py.File(path_dataset + dataset + \".hdf5\", \"r\")\n",
    "    filts = list(diagfile.keys()) if len(filtrations) == 0 else filtrations\n",
    "\n",
    "    diags_dict = dict()\n",
    "    if len(filts) == 0:\n",
    "        filts = diagfile.keys()\n",
    "    for filtration in filts:\n",
    "        list_dgm, num_diag = [], len(diagfile[filtration].keys())\n",
    "        for diag in range(num_diag):\n",
    "            list_dgm.append(np.array(diagfile[filtration][str(diag)]))\n",
    "        diags_dict[filtration] = list_dgm\n",
    "\n",
    "    # Extract features and encode labels with integers\n",
    "    feat = pd.read_csv(path_dataset + dataset + \".csv\", index_col=0, header=0)\n",
    "    F = np.array(feat)[:, 1:]  # 1: removes the labels\n",
    "    L = np.array(LabelEncoder().fit_transform(np.array(feat[\"label\"])))\n",
    "    L = OneHotEncoder(sparse=False, categories=\"auto\").fit_transform(L[:, np.newaxis])\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Dataset:\", dataset)\n",
    "        print(\"Number of observations:\", L.shape[0])\n",
    "        print(\"Number of classes:\", L.shape[1])\n",
    "\n",
    "    return diags_dict, F, L\n",
    "\n",
    "def visualize_diagrams(diags_dict, ilist=(0, 10, 20, 30, 40, 50)):\n",
    "    filts = diags_dict.keys()\n",
    "    n, m = len(filts), len(ilist)\n",
    "    fig, axs = plt.subplots(n, m, figsize=(m*n / 2, n*m / 2))\n",
    "    for (i, filtration) in enumerate(filts):\n",
    "        for (j, idx) in enumerate(ilist):\n",
    "            xs, ys = diags_dict[filtration][idx][:, 0], diags_dict[filtration][idx][:, 1]\n",
    "            axs[i, j].scatter(xs, ys)\n",
    "            axs[i, j].plot([0, 1], [0, 1])\n",
    "            axs[i, j].axis([0, 1, 0, 1])\n",
    "            axs[i, j].set_xticks([])\n",
    "            axs[i, j].set_yticks([])\n",
    "    # axis plot\n",
    "    cols = [\"idx = \" + str(i) for i in ilist]\n",
    "    rows = filts\n",
    "    for ax, col in zip(axs[0], cols):\n",
    "        ax.set_title(col)\n",
    "    for ax, row in zip(axs[:, 0], rows):\n",
    "        ax.set_ylabel(row, rotation=90, size=\"large\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def evaluate_model(L, F, D, train_sub, test_sub, model, optimizer, loss, metrics, num_epochs, batch_size=128, verbose=1, plots=False):\n",
    "\n",
    "    num_pts, num_labels, num_features, num_filt = L.shape[0], L.shape[1], F.shape[1], len(D)\n",
    "\n",
    "    train_num_pts, test_num_pts = len(train_sub), len(test_sub)\n",
    "    label_train, label_test = L[train_sub, :], L[test_sub, :]\n",
    "    feats_train, feats_test = F[train_sub, :], F[test_sub, :]\n",
    "    diags_train, diags_test = [D[dt][train_sub, :] for dt in range(num_filt)], [D[dt][test_sub, :] for dt in range(num_filt)]\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    history = model.fit(x=[diags_train, feats_train], y=label_train, validation_data=([diags_test, feats_test], label_test), epochs=num_epochs, batch_size=batch_size, shuffle=True, verbose=verbose)\n",
    "    train_results = model.evaluate([diags_train, feats_train], label_train, verbose=verbose)\n",
    "    test_results = model.evaluate([diags_test,  feats_test],  label_test, verbose=verbose)\n",
    "    \n",
    "    if plots:\n",
    "        ltrain, ltest = history.history[\"categorical_accuracy\"], history.history[\"val_categorical_accuracy\"]\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(np.array(ltrain), color=\"blue\", label=\"train acc\")\n",
    "        ax.plot(np.array(ltest),  color=\"red\",  label=\"test acc\")\n",
    "        ax.set_ylim(top=1.)\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(\"epochs\")\n",
    "        ax.set_ylabel(\"classif. accuracy\")\n",
    "        ax.set_title(\"Evolution of train/test accuracy\")\n",
    "        plt.show()\n",
    "\n",
    "    return history.history, train_results, test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pts = len(D[0])\n",
    "test_size = .3\n",
    "epochs    = 20\n",
    "F = np.array([[]]* num_pts)\n",
    "L = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_perm = np.random.permutation(num_pts)\n",
    "train, test = random_perm[:int((1-test_size)*num_pts)], random_perm[int((1-test_size)*num_pts):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Epoch 1/20\n",
      "INFO:tensorflow:Error reported to Coordinator: in user code:\n",
      "\n",
      "    /Users/berkouknicolas/anaconda3/lib/python3.7/site-packages/perslay/perslay.py:256 call  *\n",
      "        representations = self.compute_representations(diags, training)\n",
      "    /Users/berkouknicolas/anaconda3/lib/python3.7/site-packages/perslay/perslay.py:172 compute_representations  *\n",
      "        diag = diags[nf]\n",
      "\n",
      "    IndexError: tuple index out of range\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/berkouknicolas/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/coordinator.py\", line 297, in stop_on_exception\n",
      "    yield\n",
      "  File \"/Users/berkouknicolas/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/mirrored_run.py\", line 323, in run\n",
      "    self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)\n",
      "  File \"/Users/berkouknicolas/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/impl/api.py\", line 258, in wrapper\n",
      "    raise e.ag_error_metadata.to_exception(e)\n",
      "tensorflow.python.autograph.impl.api.StagingError: in user code:\n",
      "\n",
      "    /Users/berkouknicolas/anaconda3/lib/python3.7/site-packages/perslay/perslay.py:256 call  *\n",
      "        representations = self.compute_representations(diags, training)\n",
      "    /Users/berkouknicolas/anaconda3/lib/python3.7/site-packages/perslay/perslay.py:172 compute_representations  *\n",
      "        diag = diags[nf]\n",
      "\n",
      "    IndexError: tuple index out of range\n",
      "\n"
     ]
    },
    {
     "ename": "StagingError",
     "evalue": "in user code:\n\n    /Users/berkouknicolas/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /Users/berkouknicolas/anaconda3/lib/python3.7/site-packages/perslay/perslay.py:256 call  *\n        representations = self.compute_representations(diags, training)\n    /Users/berkouknicolas/anaconda3/lib/python3.7/site-packages/perslay/perslay.py:172 compute_representations  *\n        diag = diags[nf]\n\n    IndexError: tuple index out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStagingError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-7372cf2bca75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplots\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-47d54ddbec49>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(L, F, D, train_sub, test_sub, model, optimizer, loss, metrics, num_epochs, batch_size, verbose, plots)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdiags_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdiags_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0mtrain_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdiags_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeats_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdiags_test\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mfeats_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mlabel_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    696\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 697\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3073\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3075\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3077\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mStagingError\u001b[0m: in user code:\n\n    /Users/berkouknicolas/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /Users/berkouknicolas/anaconda3/lib/python3.7/site-packages/perslay/perslay.py:256 call  *\n        representations = self.compute_representations(diags, training)\n    /Users/berkouknicolas/anaconda3/lib/python3.7/site-packages/perslay/perslay.py:172 compute_representations  *\n        diag = diags[nf]\n\n    IndexError: tuple index out of range\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=20, decay_rate=0.5, staircase=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-4)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "model, optimizer, loss, metrics = get_model(dataset, 0)\n",
    "\n",
    "_, tr, te = evaluate_model(L,F,D,train,test,model,optimizer,loss,metrics,num_epochs=epochs,verbose=1,plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = './data/MUTAG/MUTAG.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-6611c0664f89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdiags_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./data/MUTAG/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-47d54ddbec49>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(dataset, path_dataset, filtrations, verbose)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0mpath_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./data/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpath_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0mdiagfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_dataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".hdf5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m     \u001b[0mfilts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiagfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltrations\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfiltrations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = './data/MUTAG/MUTAG.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "diags_dict, F, L = load_data(dataset, path_dataset=\"./data/MUTAG/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., ..., 1., 0., 1.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import List\n",
    "import numpy as np  # type: ignore\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import multiprocessing\n",
    "import os\n",
    "from einops import rearrange  # type: ignore\n",
    "from gtda.homology import WeakAlphaPersistence  # type: ignore\n",
    "from gtda.plotting import plot_diagram  # type: ignore\n",
    "\n",
    "\n",
    "class MAB(nn.Module):\n",
    "    def __init__(self, dim_Q, dim_K, dim_V, num_heads, ln=False):\n",
    "        super(MAB, self).__init__()\n",
    "        self.dim_V = dim_V\n",
    "        self.num_heads = num_heads\n",
    "        self.fc_q = nn.Linear(dim_Q, dim_V)\n",
    "        self.fc_k = nn.Linear(dim_K, dim_V)\n",
    "        self.fc_v = nn.Linear(dim_K, dim_V)\n",
    "        if ln:\n",
    "            self.ln0 = nn.LayerNorm(dim_V)\n",
    "            self.ln1 = nn.LayerNorm(dim_V)\n",
    "        self.fc_o = nn.Linear(dim_V, dim_V)\n",
    "\n",
    "    def forward(self, Q, K):\n",
    "        Q = self.fc_q(Q)\n",
    "        K, V = self.fc_k(K), self.fc_v(K)\n",
    "\n",
    "        dim_split = self.dim_V // self.num_heads\n",
    "        Q_ = torch.cat(Q.split(dim_split, 2), 0)\n",
    "        K_ = torch.cat(K.split(dim_split, 2), 0)\n",
    "        V_ = torch.cat(V.split(dim_split, 2), 0)\n",
    "\n",
    "        A = torch.softmax(Q_.bmm(K_.transpose(1, 2))/math.sqrt(self.dim_V), 2)\n",
    "        O = torch.cat((Q_ + A.bmm(V_)).split(Q.size(0), 0), 2)\n",
    "        O = O if getattr(self, 'ln0', None) is None else self.ln0(O)\n",
    "        O = O + F.relu(self.fc_o(O))\n",
    "        O = O if getattr(self, 'ln1', None) is None else self.ln1(O)\n",
    "        return O\n",
    "\n",
    "\n",
    "class SAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, ln=False):\n",
    "        super(SAB, self).__init__()\n",
    "        self.mab = MAB(dim_in, dim_in, dim_out, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.mab(X, X)\n",
    "\n",
    "\n",
    "class ISAB(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads, num_inds, ln=False):\n",
    "        super(ISAB, self).__init__()\n",
    "        self.I = nn.Parameter(torch.Tensor(1, num_inds, dim_out))\n",
    "        nn.init.xavier_uniform_(self.I)\n",
    "        self.mab0 = MAB(dim_out, dim_in, dim_out, num_heads, ln=ln)\n",
    "        self.mab1 = MAB(dim_in, dim_out, dim_out, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        H = self.mab0(self.I.repeat(X.size(0), 1, 1), X)\n",
    "        return self.mab1(X, H)\n",
    "\n",
    "\n",
    "class PMA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_seeds, ln=False):\n",
    "        super(PMA, self).__init__()\n",
    "        self.S = nn.Parameter(torch.Tensor(1, num_seeds, dim))\n",
    "        nn.init.xavier_uniform_(self.S)\n",
    "        self.mab = MAB(dim, dim, dim, num_heads, ln=ln)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.mab(self.S.repeat(X.size(0), 1, 1), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 290434 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "class SetTransformer(nn.Module):\n",
    "    \"\"\" Vanilla SetTransformer from\n",
    "    https://github.com/juho-lee/set_transformer/blob/master/main_pointcloud.py\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_input=3,  # dimension of input data for each element in the set\n",
    "        num_outputs=1,\n",
    "        dim_output=40,  # number of classes\n",
    "        num_inds=32,  # number of induced points, see  Set Transformer paper\n",
    "        dim_hidden=128,\n",
    "        num_heads=4,\n",
    "        ln=False,  # use layer norm\n",
    "    ):\n",
    "        super(SetTransformer, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "            ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            PMA(dim_hidden, num_heads, num_outputs, ln=ln),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(dim_hidden, dim_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.dec(self.enc(input)).squeeze()\n",
    "\n",
    "\n",
    "model = SetTransformer(dim_input=2, dim_output=2)\n",
    "\n",
    "def num__trainable_params(model: nn.Module) -> int:\n",
    "    return sum([parameter.nelement() for parameter in model.parameters()])\n",
    "\n",
    "print('model has', num__trainable_params(model), 'trainable parameters.')  # type: ignore\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs: int = 10, lr: float = 1e-3,\n",
    "          verbose: bool = False):\n",
    "    \"\"\"Custom training loop for Set Transformer on the dataset ``\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Set Transformer model to be trained\n",
    "        num_epochs (int, optional): Number of training epochs. Defaults to 10.\n",
    "        lr (float, optional): Learning rate for training. Defaults to 1e-3.\n",
    "        verbose (bool, optional): Print training loss, training accuracy and\n",
    "            validation if set to True. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: List of training losses\n",
    "    \"\"\"\n",
    "    if use_cuda:\n",
    "        model = nn.DataParallel(model)\n",
    "        model = model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses: List[float] = []\n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        loss_per_epoch = 0\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            # transfer to GPU\n",
    "            if use_cuda:\n",
    "                x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "            loss = criterion(model(x_batch), y_batch.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_per_epoch += loss.item()\n",
    "        losses.append(loss_per_epoch)\n",
    "        if verbose:\n",
    "            print(\"epoch:\", epoch, \"loss:\", loss_per_epoch)\n",
    "            compute_accuracy(model, 'test')\n",
    "            compute_accuracy(model, 'validation')\n",
    "    return losses\n",
    "\n",
    "\n",
    "def compute_accuracy(model, type: str = 'test') -> None:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    if type == 'test':\n",
    "        dl = dataloader\n",
    "    else:\n",
    "        dl = dataloader_validation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dl:\n",
    "            if use_cuda:\n",
    "                x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "            outputs = model(x_batch).squeeze(1)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predictions == y_batch).sum().item()\n",
    "\n",
    "    print(type.capitalize(),\n",
    "          'accuracy of the network on the', total,\n",
    "          'diagrams: %8.2f %%' % (100 * correct / total)\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pad dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_diagrams = np.array([len(diagram) for diagram in diagrams_MNIST])\n",
    "dim_max = lengths_diagrams.max()\n",
    "number_diagrams = len(diagrams_MNIST)\n",
    "\n",
    "### Initialiaze diagrams padded with zeros\n",
    "diagrams_padded = torch.zeros(number_diagrams, dim_max, 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(number_diagrams):\n",
    "    l = len(diagrams_MNIST[i])\n",
    "    diagrams_padded[i,:l,:] = torch.tensor(diagrams_MNIST[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = (0., 1.)  # different classes of orbits\n",
    "homology_dimensions = (0.,)\n",
    "\n",
    "config = {\n",
    "    'parameters': parameters,\n",
    "    'num_classes': len(parameters),\n",
    "    'num_orbits': 1_000,  # number of orbits per class\n",
    "    'num_pts_per_orbit': 1_000,\n",
    "    'homology_dimensions': homology_dimensions,\n",
    "    'num_homology_dimensions': len(homology_dimensions),\n",
    "    'validation_percentage': 80,  # size of validation dataset relative\n",
    "    # to training\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1\n",
    "test_size = 100\n",
    "permutation = torch.randperm(len(diagrams_padded))\n",
    "train_indices = permutation[:train_size]\n",
    "test_indices = permutation[train_size : train_size + test_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "x_tensor = diagrams_padded\n",
    "y_tensor = torch.Tensor(labels)\n",
    "dataset_type = 'train'\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "if dataset_type == 'train':\n",
    "       dataloader = DataLoader(dataset,\n",
    "                                batch_size=10,\n",
    "                                num_workers=2,\n",
    "                              sampler=SubsetRandomSampler(train_indices))\n",
    "else:\n",
    "        dataloader_validation = DataLoader(dataset,\n",
    "                                           batch_size=10,\n",
    "                                           num_workers=2,\n",
    "                                          sampler=SubsetRandomSampler(test_indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-59f22eb315ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-27ac5ef75ec9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, num_epochs, lr, verbose)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mloss_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;31m# transfer to GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1146\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1148\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1149\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, num_epochs = 1, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39697"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
