{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with  open(\"diagrams_MNIST.pkl\", \"rb\") as f:\n",
    "    diagrams_MNIST = pickle.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0] * int(len(diagrams_MNIST)/2) + [1] * int(len(diagrams_MNIST)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diags_dict ={'diagrams' : diagrams_MNIST}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import *\n",
    "from sklearn.svm import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV, KFold, ShuffleSplit\n",
    "from tensorflow import random_uniform_initializer as rui\n",
    "import gudhi.representations as tda\n",
    "\n",
    "### Uncomment the following to process your diagrams (necessary)\n",
    "thresh = 500\n",
    "\n",
    "# Whole pipeline\n",
    "tmp = Pipeline([\n",
    "        (\"Selector\",      tda.DiagramSelector(use=True, point_type=\"finite\")),\n",
    "        (\"ProminentPts\",  tda.ProminentPoints(use=True, num_pts=thresh)),\n",
    "        (\"Scaler\",        tda.DiagramScaler(use=True, scalers=[([0,1], MinMaxScaler())])),\n",
    "        (\"Padding\",       tda.Padding(use=True)),\n",
    "                ])\n",
    "\n",
    "prm = {filt: {\"ProminentPts__num_pts\": min(thresh, max([len(dgm) for dgm in diags_dict[filt]]))} \n",
    "       for filt in diags_dict.keys() if max([len(dgm) for dgm in diags_dict[filt]]) > 0}\n",
    "\n",
    "# Apply the previous pipeline on the different filtrations.\n",
    "diags = []\n",
    "for dt in prm.keys():\n",
    "    param = prm[dt]\n",
    "    tmp.set_params(**param)\n",
    "    diags.append(tmp.fit_transform(diags_dict[dt]))\n",
    "\n",
    "# For each filtration, concatenate all diagrams in a single array.\n",
    "D, npts = [], len(diags[0])\n",
    "for dt in range(len(prm.keys())):\n",
    "    D.append(np.array(np.concatenate([diags[dt][i][np.newaxis,:] for i in range(npts)],axis=0),dtype=np.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import perslay as pl\n",
    "perslayParameters = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perslayParameters[\"pweight_train\"]  = False\n",
    "perslayParameters[\"pweight\"]        = \"gmix\"\n",
    "perslayParameters[\"pweight_num\"]    = 3\n",
    "perslayParameters[\"pweight_init\"]   = np.array(np.vstack([np.random.uniform(0.,1.,[2,3]), \n",
    "                                                          5.*np.ones([2,3])]), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perslayParameters[\"perm_op\"] = \"mean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perslayParameters[\"layer\"]           = \"Image\"\n",
    "perslayParameters[\"layer_train\"]     = False\n",
    "perslayParameters[\"image_size\"]      = (100, 100)\n",
    "perslayParameters[\"image_bnds\"]      = ((-.501, 1.501), (-.501, 1.501))\n",
    "perslayParameters[\"lvariance_init\"]  = .1\n",
    "perslayParameters[\"final_model\"]     = tf.keras.Sequential([tf.keras.layers.Flatten()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pl.PerslayModel(name=\"perslay\", diagdim=2, perslay_parameters=[perslayParameters], rho=\"identity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import itertools\n",
    "import h5py\n",
    "import numpy              as np\n",
    "import matplotlib.pyplot  as plt\n",
    "import pandas             as pd\n",
    "import tensorflow         as tf\n",
    "import tensorflow_addons  as tfa\n",
    "import gudhi              as gd\n",
    "\n",
    "from scipy.sparse           import csgraph\n",
    "from scipy.io               import loadmat\n",
    "from scipy.linalg           import eigh\n",
    "from sklearn.preprocessing  import LabelEncoder, OneHotEncoder\n",
    "from tensorflow             import random_uniform_initializer as rui\n",
    "from perslay                import PerslayModel\n",
    "\n",
    "\n",
    "def get_parameters(dataset):\n",
    "    if dataset == \"MUTAG\" or dataset == \"PROTEINS\":\n",
    "        dataset_parameters = {\"data_type\": \"graph\", \"filt_names\": [\"Ord0_10.0-hks\", \"Rel1_10.0-hks\", \"Ext0_10.0-hks\", \"Ext1_10.0-hks\"]}\n",
    "    elif dataset == \"COX2\" or dataset == \"DHFR\" or dataset == \"NCI1\" or dataset == \"NCI109\" or dataset == \"IMDB-BINARY\" or dataset == \"IMDB-MULTI\":\n",
    "        dataset_parameters = {\"data_type\": \"graph\", \"filt_names\": [\"Ord0_0.1-hks\", \"Rel1_0.1-hks\", \"Ext0_0.1-hks\", \"Ext1_0.1-hks\", \"Ord0_10.0-hks\", \"Rel1_10.0-hks\", \"Ext0_10.0-hks\", \"Ext1_10.0-hks\"]}\n",
    "    elif dataset == \"ORBIT5K\" or dataset == \"ORBIT100K\":\n",
    "        dataset_parameters = {\"data_type\": \"orbit\", \"filt_names\": [\"Alpha0\", \"Alpha1\"]}\n",
    "    return dataset_parameters\n",
    "\n",
    "def get_model(dataset, nf):\n",
    "\n",
    "    if dataset == \"MUTAG\":\n",
    "\n",
    "        plp = {}\n",
    "        plp[\"pweight\"]        = \"grid\"\n",
    "        plp[\"pweight_init\"]   = rui(1., 1.)\n",
    "        plp[\"pweight_size\"]   = (10, 10)\n",
    "        plp[\"pweight_bnds\"]   = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"pweight_train\"]  = True\n",
    "        plp[\"layer\"]          = \"Image\"\n",
    "        plp[\"image_size\"]     = (20, 20)\n",
    "        plp[\"image_bnds\"]     = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"lvariance_init\"] = rui(3., 3.)\n",
    "        plp[\"layer_train\"]    = True\n",
    "        plp[\"perm_op\"]        = \"sum\"\n",
    "        perslay_parameters    = [plp for _ in range(4)]\n",
    "\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "        with mirrored_strategy.scope():\n",
    "            for i in range(4):\n",
    "                fmodel = tf.keras.Sequential([tf.keras.layers.Conv2D(10, 2, input_shape=(21,21,1)), tf.keras.layers.Flatten()])\n",
    "                perslay_parameters[i][\"final_model\"] = fmodel\n",
    "            rho = tf.keras.Sequential([tf.keras.layers.Dense(2, activation=\"sigmoid\", input_shape=(16000+nf,))])\n",
    "            model = PerslayModel(name=\"PersLay\", diagdim=2, perslay_parameters=perslay_parameters, rho=rho)\n",
    "            lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=20, decay_rate=0.5, staircase=True)\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-4)\n",
    "            optimizer = tfa.optimizers.MovingAverage(optimizer, average_decay=0.9) \n",
    "            loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "            metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "    elif dataset == \"PROTEINS\":\n",
    "\n",
    "        plp = {}\n",
    "        plp[\"pweight\"]        = \"grid\"\n",
    "        plp[\"pweight_init\"]   = rui(1., 1.)\n",
    "        plp[\"pweight_size\"]   = (10, 10)\n",
    "        plp[\"pweight_bnds\"]   = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"pweight_train\"]  = True\n",
    "        plp[\"layer\"]          = \"Image\"\n",
    "        plp[\"image_size\"]     = (15, 15)\n",
    "        plp[\"image_bnds\"]     = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"lvariance_init\"] = rui(3., 3.)\n",
    "        plp[\"layer_train\"]    = True\n",
    "        plp[\"perm_op\"]        = \"sum\"\n",
    "        perslay_parameters    = [plp for _ in range(4)]\n",
    "\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "        with mirrored_strategy.scope():\n",
    "            for i in range(4):\n",
    "                fmodel = tf.keras.Sequential([tf.keras.layers.Conv2D(10, 2, input_shape=(16,16,1)), tf.keras.layers.Flatten()])\n",
    "                perslay_parameters[i][\"final_model\"] = fmodel\n",
    "            rho = tf.keras.Sequential([tf.keras.layers.Dense(2, activation=\"sigmoid\", input_shape=(9000+nf,))])\n",
    "            model = PerslayModel(name=\"PersLay\", diagdim=2, perslay_parameters=perslay_parameters, rho=rho)\n",
    "            lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=20, decay_rate=0.5, staircase=True)\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-4)\n",
    "            optimizer = tfa.optimizers.MovingAverage(optimizer, average_decay=0.9) \n",
    "            loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "            metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "    elif dataset == \"NCI1\" or dataset == \"NCI109\":\n",
    "\n",
    "        plp = {}\n",
    "        plp[\"pweight\"]        = \"grid\"\n",
    "        plp[\"pweight_init\"]   = rui(1., 1.)\n",
    "        plp[\"pweight_size\"]   = (10, 10)\n",
    "        plp[\"pweight_bnds\"]   = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"pweight_train\"]  = True\n",
    "        plp[\"layer\"]          = \"PermutationEquivariant\"\n",
    "        plp[\"lpeq\"]           = [(25, None), (25, \"max\")]\n",
    "        plp[\"layer_train\"]    = True\n",
    "        plp[\"perm_op\"]        = \"sum\"\n",
    "        plp[\"final_model\"]    = \"identity\"\n",
    "        perslay_parameters    = [plp for _ in range(8)]\n",
    "\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "        with mirrored_strategy.scope():\n",
    "            rho = tf.keras.Sequential([tf.keras.layers.Dense(2, activation=\"sigmoid\", input_shape=(200+nf,))])\n",
    "            model = PerslayModel(name=\"PersLay\", diagdim=2, perslay_parameters=perslay_parameters, rho=rho)\n",
    "            lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=20, decay_rate=0.5, staircase=True)\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-4)\n",
    "            optimizer = tfa.optimizers.MovingAverage(optimizer, average_decay=0.9) \n",
    "            loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "            metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "    elif dataset == \"IMDB-MULTI\" or dataset == \"IMDB-BINARY\":\n",
    "\n",
    "        nlab = 2 if dataset == \"IMDB-BINARY\" else 3\n",
    "        plp = {}\n",
    "        plp[\"pweight\"]        = \"grid\"\n",
    "        plp[\"pweight_init\"]   = rui(1., 1.)\n",
    "        plp[\"pweight_size\"]   = (20, 20)\n",
    "        plp[\"pweight_bnds\"]   = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"pweight_train\"]  = True\n",
    "        plp[\"layer\"]          = \"Image\"\n",
    "        plp[\"image_size\"]     = (20, 20)\n",
    "        plp[\"image_bnds\"]     = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"lvariance_init\"] = rui(3., 3.)\n",
    "        plp[\"layer_train\"]    = True\n",
    "        plp[\"perm_op\"]        = \"sum\"\n",
    "        perslay_parameters    = [plp for _ in range(8)]\n",
    "\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "        with mirrored_strategy.scope():\n",
    "            for i in range(8):\n",
    "                fmodel = tf.keras.Sequential([tf.keras.layers.Conv2D(10, 2, input_shape=(21,21,1)), tf.keras.layers.Flatten()])\n",
    "                perslay_parameters[i][\"final_model\"] = fmodel\n",
    "            rho = tf.keras.Sequential([tf.keras.layers.Dense(nlab, activation=\"sigmoid\", input_shape=(32000+nf,))])\n",
    "            model = PerslayModel(name=\"PersLay\", diagdim=2, perslay_parameters=perslay_parameters, rho=rho)\n",
    "            lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=20, decay_rate=0.5, staircase=True)\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-4)\n",
    "            optimizer = tfa.optimizers.MovingAverage(optimizer, average_decay=0.9) \n",
    "            loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "            metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "    elif dataset == \"COX2\" or dataset == \"DHFR\":\n",
    "\n",
    "        plp = {}\n",
    "        plp[\"pweight\"]        = \"grid\"\n",
    "        plp[\"pweight_init\"]   = rui(1., 1.)\n",
    "        plp[\"pweight_size\"]   = (10, 10)\n",
    "        plp[\"pweight_bnds\"]   = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"pweight_train\"]  = True\n",
    "        plp[\"layer\"]          = \"Image\"\n",
    "        plp[\"image_size\"]     = (20, 20)\n",
    "        plp[\"image_bnds\"]     = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"lvariance_init\"] = rui(3., 3.)\n",
    "        plp[\"layer_train\"]    = True\n",
    "        plp[\"perm_op\"]        = \"sum\"\n",
    "        perslay_parameters    = [plp for _ in range(8)]\n",
    "\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "        with mirrored_strategy.scope():\n",
    "            for i in range(8):\n",
    "                fmodel = tf.keras.Sequential([tf.keras.layers.Conv2D(10, 2, input_shape=(21,21,1)), tf.keras.layers.Flatten()])\n",
    "                perslay_parameters[i][\"final_model\"] = fmodel\n",
    "            rho = tf.keras.Sequential([tf.keras.layers.Dense(2, activation=\"sigmoid\", input_shape=(32000+nf,))])\n",
    "            model = PerslayModel(name=\"PersLay\", diagdim=2, perslay_parameters=perslay_parameters, rho=rho)\n",
    "            lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=20, decay_rate=0.5, staircase=True)\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-4)\n",
    "            optimizer = tfa.optimizers.MovingAverage(optimizer, average_decay=0.9) \n",
    "            loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "            metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "    elif dataset == \"ORBIT5K\" or dataset == \"ORBIT100K\":\n",
    "\n",
    "        plp = {}\n",
    "        plp[\"pweight\"]        = \"grid\"\n",
    "        plp[\"pweight_init\"]   = rui(1., 1.)\n",
    "        plp[\"pweight_size\"]   = (10, 10)\n",
    "        plp[\"pweight_bnds\"]   = ((-0.001, 1.001), (-0.001, 1.001))\n",
    "        plp[\"pweight_train\"]  = True\n",
    "        plp[\"layer\"]          = \"PermutationEquivariant\"\n",
    "        plp[\"lpeq\"]           = [(25, None), (25, \"max\")]\n",
    "        plp[\"lweight_init\"]   = rui(0.,1.)\n",
    "        plp[\"lbias_init\"]     = rui(0.,1.)\n",
    "        plp[\"lgamma_init\"]    = rui(0.,1.)\n",
    "        plp[\"layer_train\"]    = True\n",
    "        plp[\"perm_op\"]        = \"topk\"\n",
    "        plp[\"keep\"]           = 5\n",
    "        plp[\"final_model\"]    = \"identity\"\n",
    "        perslay_parameters    = [plp for _ in range(2)]\n",
    "\n",
    "        mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "        with mirrored_strategy.scope():\n",
    "            rho = tf.keras.Sequential([tf.keras.layers.Dense(5, activation=\"sigmoid\", input_shape=(250+nf,))])\n",
    "            model = PerslayModel(name=\"PersLay\", diagdim=2, perslay_parameters=perslay_parameters, rho=rho)\n",
    "            lr = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=0.01, decay_steps=20, decay_rate=1., staircase=True)\n",
    "            optimizer = tf.keras.optimizers.Adam(learning_rate=lr, epsilon=1e-4)\n",
    "            optimizer = tfa.optimizers.MovingAverage(optimizer, average_decay=0.9) \n",
    "            loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "            metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "    return model, optimizer, loss, metrics\n",
    "\n",
    "def hks_signature(eigenvectors, eigenvals, time):\n",
    "    return np.square(eigenvectors).dot(np.diag(np.exp(-time * eigenvals))).sum(axis=1)\n",
    "\n",
    "def generate_orbit(num_pts_per_orbit, param):\n",
    "    X = np.zeros([num_pts_per_orbit, 2])\n",
    "    xcur, ycur = np.random.rand(), np.random.rand()\n",
    "    for idx in range(num_pts_per_orbit):\n",
    "        xcur = (xcur + param * ycur * (1. - ycur)) % 1\n",
    "        ycur = (ycur + param * xcur * (1. - xcur)) % 1\n",
    "        X[idx, :] = [xcur, ycur]\n",
    "    return X\n",
    "\n",
    "def apply_graph_extended_persistence(A, filtration_val):\n",
    "    num_vertices = A.shape[0]\n",
    "    (xs, ys) = np.where(np.triu(A))\n",
    "    st = gd.SimplexTree()\n",
    "    for i in range(num_vertices):\n",
    "        st.insert([i], filtration=-1e10)\n",
    "    for idx, x in enumerate(xs):        \n",
    "        st.insert([x, ys[idx]], filtration=-1e10)\n",
    "    for i in range(num_vertices):\n",
    "        st.assign_filtration([i], filtration_val[i])\n",
    "    st.make_filtration_non_decreasing()\n",
    "    st.extend_filtration()\n",
    "    LD = st.extended_persistence()\n",
    "    dgmOrd0, dgmRel1, dgmExt0, dgmExt1 = LD[0], LD[1], LD[2], LD[3]\n",
    "    dgmOrd0 = np.vstack([np.array([[ min(p[1][0],p[1][1]), max(p[1][0],p[1][1]) ]]) for p in dgmOrd0 if p[0] == 0]) if len(dgmOrd0) else np.empty([0,2])\n",
    "    dgmRel1 = np.vstack([np.array([[ min(p[1][0],p[1][1]), max(p[1][0],p[1][1]) ]]) for p in dgmRel1 if p[0] == 1]) if len(dgmRel1) else np.empty([0,2])\n",
    "    dgmExt0 = np.vstack([np.array([[ min(p[1][0],p[1][1]), max(p[1][0],p[1][1]) ]]) for p in dgmExt0 if p[0] == 0]) if len(dgmExt0) else np.empty([0,2])\n",
    "    dgmExt1 = np.vstack([np.array([[ min(p[1][0],p[1][1]), max(p[1][0],p[1][1]) ]]) for p in dgmExt1 if p[0] == 1]) if len(dgmExt1) else np.empty([0,2])\n",
    "    return dgmOrd0, dgmExt0, dgmRel1, dgmExt1\n",
    "\n",
    "def generate_diagrams_and_features(dataset, path_dataset=\"\"):\n",
    "\n",
    "    dataset_parameters = get_parameters(dataset)\n",
    "    dataset_type = dataset_parameters[\"data_type\"]\n",
    "\n",
    "    if \"REDDIT\" in dataset:\n",
    "        print(\"Unfortunately, REDDIT data are not available yet for memory issues.\\n\")\n",
    "        print(\"Moreover, the link we used to download the data,\")\n",
    "        print(\"http://www.mit.edu/~pinary/kdd/datasets.tar.gz\")\n",
    "        print(\"is down at the commit time (May 23rd).\")\n",
    "        print(\"We will update this repository when we figure out a workaround.\")\n",
    "        return\n",
    "\n",
    "    path_dataset = \"./data/\" + dataset + \"/\" if not len(path_dataset) else path_dataset\n",
    "    if os.path.isfile(path_dataset + dataset + \".hdf5\"):\n",
    "        os.remove(path_dataset + dataset + \".hdf5\")\n",
    "    diag_file = h5py.File(path_dataset + dataset + \".hdf5\", \"w\")\n",
    "    list_filtrations = dataset_parameters[\"filt_names\"]\n",
    "    [diag_file.create_group(str(filtration)) for filtration in dataset_parameters[\"filt_names\"]]\n",
    "    \n",
    "    if dataset_type == \"graph\":\n",
    "\n",
    "        list_hks_times = np.unique([filtration.split(\"_\")[1] for filtration in list_filtrations])\n",
    "\n",
    "        # preprocessing\n",
    "        pad_size = 1\n",
    "        for graph_name in os.listdir(path_dataset + \"mat/\"):\n",
    "            A = np.array(loadmat(path_dataset + \"mat/\" + graph_name)[\"A\"], dtype=np.float32)\n",
    "            pad_size = np.max((A.shape[0], pad_size))\n",
    "\n",
    "        feature_names = [\"eval\"+str(i) for i in range(pad_size)] + [name+\"-percent\"+str(i) for name, i in itertools.product([f for f in list_hks_times if \"hks\" in f], 10*np.arange(11))]\n",
    "        features = pd.DataFrame(index=range(len(os.listdir(path_dataset + \"mat/\"))), columns=[\"label\"] + feature_names)\n",
    "\n",
    "        for idx, graph_name in enumerate((os.listdir(path_dataset + \"mat/\"))):\n",
    "\n",
    "            name = graph_name.split(\"_\")\n",
    "            gid = int(name[name.index(\"gid\") + 1]) - 1\n",
    "            A = np.array(loadmat(path_dataset + \"mat/\" + graph_name)[\"A\"], dtype=np.float32)\n",
    "            num_vertices = A.shape[0]\n",
    "            label = int(name[name.index(\"lb\") + 1])\n",
    "\n",
    "            L = csgraph.laplacian(A, normed=True)\n",
    "            egvals, egvectors = eigh(L)\n",
    "            eigenvectors = np.zeros([num_vertices, pad_size])\n",
    "            eigenvals = np.zeros(pad_size)\n",
    "            eigenvals[:min(pad_size, num_vertices)] = np.flipud(egvals)[:min(pad_size, num_vertices)]\n",
    "            eigenvectors[:, :min(pad_size, num_vertices)] = np.fliplr(egvectors)[:, :min(pad_size, num_vertices)]\n",
    "            graph_features = []\n",
    "            graph_features.append(eigenvals)\n",
    "\n",
    "            for fhks in list_hks_times:\n",
    "                hks_time = float(fhks.split(\"-\")[0])\n",
    "                filtration_val = hks_signature(egvectors, egvals, time=hks_time)\n",
    "                dgmOrd0, dgmExt0, dgmRel1, dgmExt1 = apply_graph_extended_persistence(A, filtration_val)\n",
    "                diag_file[\"Ord0_\" + str(hks_time) + \"-hks\"].create_dataset(name=str(gid), data=dgmOrd0)\n",
    "                diag_file[\"Ext0_\" + str(hks_time) + \"-hks\"].create_dataset(name=str(gid), data=dgmExt0)\n",
    "                diag_file[\"Rel1_\" + str(hks_time) + \"-hks\"].create_dataset(name=str(gid), data=dgmRel1)\n",
    "                diag_file[\"Ext1_\" + str(hks_time) + \"-hks\"].create_dataset(name=str(gid), data=dgmExt1)\n",
    "                graph_features.append(np.percentile(hks_signature(eigenvectors, eigenvals, time=hks_time), 10 * np.arange(11)))\n",
    "            features.loc[gid] = np.insert(np.concatenate(graph_features), 0, label)\n",
    "        features[\"label\"] = features[\"label\"].astype(int)\n",
    "\n",
    "    elif dataset_type == \"orbit\":\n",
    "\n",
    "        labs = []\n",
    "        count = 0\n",
    "        num_diag_per_param = 1000 if \"5K\" in dataset else 20000\n",
    "        for lab, r in enumerate([2.5, 3.5, 4.0, 4.1, 4.3]):\n",
    "            print(\"Generating\", num_diag_per_param, \"orbits and diagrams for r = \", r, \"...\")\n",
    "            for dg in range(num_diag_per_param):\n",
    "                X = generate_orbit(num_pts_per_orbit=1000, param=r)\n",
    "                alpha_complex = gd.AlphaComplex(points=X)\n",
    "                st = alpha_complex.create_simplex_tree(max_alpha_square=1e50)\n",
    "                st.persistence()\n",
    "                diag_file[\"Alpha0\"].create_dataset(name=str(count), data=np.array(st.persistence_intervals_in_dimension(0)))\n",
    "                diag_file[\"Alpha1\"].create_dataset(name=str(count), data=np.array(st.persistence_intervals_in_dimension(1)))\n",
    "                orbit_label = {\"label\": lab, \"pcid\": count}\n",
    "                labs.append(orbit_label)\n",
    "                count += 1\n",
    "        labels = pd.DataFrame(labs)\n",
    "        labels.set_index(\"pcid\")\n",
    "        features = labels[[\"label\"]]\n",
    "\n",
    "    features.to_csv(path_dataset + dataset + \".csv\")\n",
    "\n",
    "    return diag_file.close()\n",
    "\n",
    "def load_data(dataset, path_dataset=\"\", filtrations=[], verbose=False):\n",
    "\n",
    "    path_dataset = \"./data/\" + dataset + \"/\" if not len(path_dataset) else path_dataset\n",
    "    diagfile = h5py.File(path_dataset + dataset + \".hdf5\", \"r\")\n",
    "    filts = list(diagfile.keys()) if len(filtrations) == 0 else filtrations\n",
    "\n",
    "    diags_dict = dict()\n",
    "    if len(filts) == 0:\n",
    "        filts = diagfile.keys()\n",
    "    for filtration in filts:\n",
    "        list_dgm, num_diag = [], len(diagfile[filtration].keys())\n",
    "        for diag in range(num_diag):\n",
    "            list_dgm.append(np.array(diagfile[filtration][str(diag)]))\n",
    "        diags_dict[filtration] = list_dgm\n",
    "\n",
    "    # Extract features and encode labels with integers\n",
    "    feat = pd.read_csv(path_dataset + dataset + \".csv\", index_col=0, header=0)\n",
    "    F = np.array(feat)[:, 1:]  # 1: removes the labels\n",
    "    L = np.array(LabelEncoder().fit_transform(np.array(feat[\"label\"])))\n",
    "    L = OneHotEncoder(sparse=False, categories=\"auto\").fit_transform(L[:, np.newaxis])\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Dataset:\", dataset)\n",
    "        print(\"Number of observations:\", L.shape[0])\n",
    "        print(\"Number of classes:\", L.shape[1])\n",
    "\n",
    "    return diags_dict, F, L\n",
    "\n",
    "def visualize_diagrams(diags_dict, ilist=(0, 10, 20, 30, 40, 50)):\n",
    "    filts = diags_dict.keys()\n",
    "    n, m = len(filts), len(ilist)\n",
    "    fig, axs = plt.subplots(n, m, figsize=(m*n / 2, n*m / 2))\n",
    "    for (i, filtration) in enumerate(filts):\n",
    "        for (j, idx) in enumerate(ilist):\n",
    "            xs, ys = diags_dict[filtration][idx][:, 0], diags_dict[filtration][idx][:, 1]\n",
    "            axs[i, j].scatter(xs, ys)\n",
    "            axs[i, j].plot([0, 1], [0, 1])\n",
    "            axs[i, j].axis([0, 1, 0, 1])\n",
    "            axs[i, j].set_xticks([])\n",
    "            axs[i, j].set_yticks([])\n",
    "    # axis plot\n",
    "    cols = [\"idx = \" + str(i) for i in ilist]\n",
    "    rows = filts\n",
    "    for ax, col in zip(axs[0], cols):\n",
    "        ax.set_title(col)\n",
    "    for ax, row in zip(axs[:, 0], rows):\n",
    "        ax.set_ylabel(row, rotation=90, size=\"large\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def evaluate_model(L, F, D, train_sub, test_sub, model, optimizer, loss, metrics, num_epochs, batch_size=128, verbose=1, plots=False):\n",
    "\n",
    "    num_pts, num_labels, num_features, num_filt = L.shape[0], L.shape[1], F.shape[1], len(D)\n",
    "\n",
    "    train_num_pts, test_num_pts = len(train_sub), len(test_sub)\n",
    "    label_train, label_test = L[train_sub, :], L[test_sub, :]\n",
    "    feats_train, feats_test = F[train_sub, :], F[test_sub, :]\n",
    "    diags_train, diags_test = [D[dt][train_sub, :] for dt in range(num_filt)], [D[dt][test_sub, :] for dt in range(num_filt)]\n",
    "\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    history = model.fit(x=[diags_train, feats_train], y=label_train, validation_data=([diags_test, feats_test], label_test), epochs=num_epochs, batch_size=batch_size, shuffle=True, verbose=verbose)\n",
    "    train_results = model.evaluate([diags_train, feats_train], label_train, verbose=verbose)\n",
    "    test_results = model.evaluate([diags_test,  feats_test],  label_test, verbose=verbose)\n",
    "    \n",
    "    if plots:\n",
    "        ltrain, ltest = history.history[\"categorical_accuracy\"], history.history[\"val_categorical_accuracy\"]\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(np.array(ltrain), color=\"blue\", label=\"train acc\")\n",
    "        ax.plot(np.array(ltest),  color=\"red\",  label=\"test acc\")\n",
    "        ax.set_ylim(top=1.)\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(\"epochs\")\n",
    "        ax.set_ylabel(\"classif. accuracy\")\n",
    "        ax.set_title(\"Evolution of train/test accuracy\")\n",
    "        plt.show()\n",
    "\n",
    "    return history.history, train_results, test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pts = len(D[0])\n",
    "test_size = .3\n",
    "epochs    = 20\n",
    "F = np.array([[]]* num_pts)\n",
    "L = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_perm = np.random.permutation(num_pts)\n",
    "train, test = random_perm[:int((1-test_size)*num_pts)], random_perm[int((1-test_size)*num_pts):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments import *\n",
    "\n",
    "\n",
    "_, tr, te = evaluate_model(L,F,D,train,test,model,optimizer,loss,metrics,num_epochs=epochs,verbose=0,plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 57348,    594, 108603, ...,  81091,  70802,  98043])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
