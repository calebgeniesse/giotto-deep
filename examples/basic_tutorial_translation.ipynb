{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: translation\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with the basic functioning of *giotto-deep* API.\n",
    "\n",
    "The example described in this tutorial is the one of translation.\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. creation of a dataset\n",
    " 2. creation of a model\n",
    " 3. define metrics and losses\n",
    " 4. train the model\n",
    " 5. extract some features of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPUs...\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from gdeep.models import FFNet\n",
    "\n",
    "from gdeep.visualisation import  persistence_diagrams_of_activations\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gdeep.data import TorchDataLoader\n",
    "from gdeep.pipeline import Pipeline\n",
    "\n",
    "from gtda.diagrams import BettiCurve\n",
    "\n",
    "from gtda.plotting import plot_betti_surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the reuslts of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# the only part of the training set we are interested in\n",
    "train_indices = list(range(32*10))\n",
    "\n",
    "dl = TorchDataLoader(name=\"Multi30k\", convert_to_map_dataset=True)\n",
    "dl_tr_str, dl_ts_str = dl.build_dataloaders(sampler=SubsetRandomSampler(train_indices))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a list of pairs of sentences: the English sentecen and its German translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ein Mann, der eine reflektierende Weste und einen Schutzhelm trägt, hält eine Flagge in die Straße.\\n',),\n",
       " ('A man wearing a reflective vest and a hard hat holds a flag in the road\\n',)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dl_tr_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required preprocessing\n",
    "\n",
    "Neural networks cannot direcly deal with strings. We have first to preprocess the dataset in three main ways:\n",
    " 1. Tokenise the string into its words\n",
    " 2. Build a vocabulary out of these words\n",
    " 3. Embed each word into a vector, so that each sentence becomes a list of vectors\n",
    "\n",
    "The first two steps are performed by the `PreprocessTextTranslation`. The embedding will be added directly to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.data import PreprocessText, PreprocessTextTranslation\n",
    "\n",
    "prec = PreprocessTextTranslation((dl_tr_str, dl_ts_str))\n",
    "\n",
    "(dl_tr, dl_ts) = prec.build_dataloaders(batch_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Transformer\n",
    "from torch.optim import Adam, SparseAdam, SGD\n",
    "\n",
    "# my simple transformer model\n",
    "class TranslationTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim):\n",
    "        super(TranslationTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=embed_dim,\n",
    "                                       nhead=2,\n",
    "                                       num_encoder_layers=1,\n",
    "                                       num_decoder_layers=1,\n",
    "                                       dim_feedforward=512,\n",
    "                                       dropout=0.1)\n",
    "        self.embedding_src = nn.Embedding(src_vocab_size, embed_dim, sparse=True)\n",
    "        self.embedding_tgt = nn.Embedding(tgt_vocab_size, embed_dim, sparse=True)\n",
    "        self.generator = nn.Linear(embed_dim, tgt_vocab_size)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #print(X.shape)\n",
    "        src = X[:,0,:]\n",
    "        tgt = X[:,1,:]\n",
    "        #print(src.shape, tgt.shape)\n",
    "        src_emb = self.embedding_src(src)\n",
    "        tgt_emb = self.embedding_tgt(tgt)\n",
    "        #print(src_emb.shape, tgt_emb.shape)\n",
    "        self.outs = self.transformer(src_emb, tgt_emb)\n",
    "        #print(outs.shape)\n",
    "        logits = self.generator(self.outs)\n",
    "        #print(logits.shape)\n",
    "        #out = torch.topk(logits, k=1, dim=2).indices.reshape(-1,44)\n",
    "        #print(out, out.shape)\n",
    "        return logits\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"this method is used only at the inference step\"\"\"\n",
    "        return self.transformer.encoder(\n",
    "                            self.embedding_src(src), src_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        \"\"\"this method is used only at the inference step\"\"\"\n",
    "        return self.transformer.decoder(\n",
    "                          self.embedding_tgt(tgt), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TranslationTransformer(\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=64, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=64, bias=True)\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=64, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=64, bias=True)\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (embedding_src): Embedding(1500, 64, sparse=True)\n",
      "  (embedding_tgt): Embedding(1500, 64, sparse=True)\n",
      "  (generator): Linear(in_features=64, out_features=1500, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 1500 # to be discussed\n",
    "\n",
    "src_vocab_size = vocab_size # prec.vocabulary[\".\"]  # len(prec.vocabulary)\n",
    "tgt_vocab_size = vocab_size # prec.vocabulary_lab[\".\"]  # len(prec.vocabulary_lab)\n",
    "emb_size = 64\n",
    "\n",
    "model = TranslationTransformer(src_vocab_size, tgt_vocab_size, emb_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "This loss function is a adapted version of the Cross Entropy for the trnasformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_fn(logits, tgt_out):\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    return cel(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  4.0121496468782425  \tBatch training accuracy:  2710.9375  \t[ 16 / 16 ]                             \n",
      "Time taken for this epoch: 1.00s\n",
      "No TPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matteocaorsi/Desktop/giotto-deep/gdeep/pipeline/pipeline.py:266: UserWarning:\n",
      "\n",
      "Cannot store data in the PR curve\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " Accuracy: 3151.562500%,                 Avg loss: 2.284808 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  2.1167310625314713  \tBatch training accuracy:  3201.171875  \t[ 16 / 16 ]                            \n",
      "Time taken for this epoch: 1.00s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 3162.500000%,                 Avg loss: 2.084198 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "No TPUs\n",
      "Batch training loss:  1.9731857255101204  \tBatch training accuracy:  3247.265625  \t[ 16 / 16 ]                            \n",
      "Time taken for this epoch: 1.00s\n",
      "No TPUs\n",
      "Validation results: \n",
      " Accuracy: 3307.812500%,                 Avg loss: 1.992500 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.9924995303153992, 3307.8125)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare a pipeline class with the model, dataloaders loss_fn and tensorboard writer\n",
    "pipe = Pipeline(model, (dl_tr, dl_ts), loss_fn, writer)\n",
    "\n",
    "# train the model\n",
    "pipe.train(SGD, 3, False, {\"lr\":0.01}, {\"batch_size\":16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ein Mann in Sandalen und weißer Jacke sitzt auf einer grünen Bank uns spricht am Handy.\\n',) \n",
      " ('A man in sandals and white cardigan sits on a green bench while talking on his cellphone.\\n',)\n"
     ]
    }
   ],
   "source": [
    "de, en = next(iter(dl_tr_str))\n",
    "print(de, \"\\n\", en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Get the vocabulary and numericize the German sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[236, 88, 139, 1, 104, 7, 6, 25, 111, 81, 7, 7, 1, 2, 14, 0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc = prec.vocabulary_lab\n",
    "sent = str.lower(de[0]).split()\n",
    "de_sentence = list(map(voc.__getitem__,sent))\n",
    "de_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German sentence:  Ein Mann in Sandalen und weißer Jacke sitzt auf einer grünen Bank uns spricht am Handy.\n",
      "\n",
      "English translation:  young packing jump tropical packing packing young trees made\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    \"\"\"function to generate output sequence using greedy algorithm\"\"\"\n",
    "    memory = model.encode(src, src_mask)\n",
    "    out = model.decode(src, memory, None)\n",
    "    prob = model.generator(out)\n",
    "    greedy_out = torch.max(prob, dim=2).indices\n",
    "    return greedy_out\n",
    "    \n",
    "\n",
    "def translate(model: torch.nn.Module, dl_ts_item):\n",
    "    \"\"\"actual function to translate input sentence into target language\"\"\"\n",
    "    model.eval()\n",
    "    voc = prec.vocabulary\n",
    "    src = dl_ts_item\n",
    "    num_tokens = src.shape[1]\n",
    "    src_mask = None\n",
    "    tgt_tokens_raw = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=111).flatten()\n",
    "    tgt_tokens = [token for token in tgt_tokens_raw if token < len(list(voc.vocab))]\n",
    "    return \" \".join(list(map(list(voc.vocab).__getitem__, tgt_tokens)))\n",
    "    \n",
    "# translation!\n",
    "print(\"German sentence: \", de[0])\n",
    "print(\"English translation: \", translate(pipe.model, torch.tensor([de_sentence])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract inner data from your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.encoder.layers.0.self_attn.in_proj_weight torch.Size([192, 64])\n",
      "transformer.encoder.layers.0.self_attn.in_proj_bias torch.Size([192])\n",
      "transformer.encoder.layers.0.self_attn.out_proj.weight torch.Size([64, 64])\n",
      "transformer.encoder.layers.0.self_attn.out_proj.bias torch.Size([64])\n",
      "transformer.encoder.layers.0.linear1.weight torch.Size([512, 64])\n",
      "transformer.encoder.layers.0.linear1.bias torch.Size([512])\n",
      "transformer.encoder.layers.0.linear2.weight torch.Size([64, 512])\n",
      "transformer.encoder.layers.0.linear2.bias torch.Size([64])\n",
      "transformer.encoder.layers.0.norm1.weight torch.Size([64])\n",
      "transformer.encoder.layers.0.norm1.bias torch.Size([64])\n",
      "transformer.encoder.layers.0.norm2.weight torch.Size([64])\n",
      "transformer.encoder.layers.0.norm2.bias torch.Size([64])\n",
      "transformer.encoder.norm.weight torch.Size([64])\n",
      "transformer.encoder.norm.bias torch.Size([64])\n",
      "transformer.decoder.layers.0.self_attn.in_proj_weight torch.Size([192, 64])\n",
      "transformer.decoder.layers.0.self_attn.in_proj_bias torch.Size([192])\n",
      "transformer.decoder.layers.0.self_attn.out_proj.weight torch.Size([64, 64])\n",
      "transformer.decoder.layers.0.self_attn.out_proj.bias torch.Size([64])\n",
      "transformer.decoder.layers.0.multihead_attn.in_proj_weight torch.Size([192, 64])\n",
      "transformer.decoder.layers.0.multihead_attn.in_proj_bias torch.Size([192])\n",
      "transformer.decoder.layers.0.multihead_attn.out_proj.weight torch.Size([64, 64])\n",
      "transformer.decoder.layers.0.multihead_attn.out_proj.bias torch.Size([64])\n",
      "transformer.decoder.layers.0.linear1.weight torch.Size([512, 64])\n",
      "transformer.decoder.layers.0.linear1.bias torch.Size([512])\n",
      "transformer.decoder.layers.0.linear2.weight torch.Size([64, 512])\n",
      "transformer.decoder.layers.0.linear2.bias torch.Size([64])\n",
      "transformer.decoder.layers.0.norm1.weight torch.Size([64])\n",
      "transformer.decoder.layers.0.norm1.bias torch.Size([64])\n",
      "transformer.decoder.layers.0.norm2.weight torch.Size([64])\n",
      "transformer.decoder.layers.0.norm2.bias torch.Size([64])\n",
      "transformer.decoder.layers.0.norm3.weight torch.Size([64])\n",
      "transformer.decoder.layers.0.norm3.bias torch.Size([64])\n",
      "transformer.decoder.norm.weight torch.Size([64])\n",
      "transformer.decoder.norm.bias torch.Size([64])\n",
      "embedding_src.weight torch.Size([1500, 64])\n",
      "embedding_tgt.weight torch.Size([1500, 64])\n",
      "generator.weight torch.Size([1500, 64])\n",
      "generator.bias torch.Size([1500])\n"
     ]
    }
   ],
   "source": [
    "from gdeep.models import ModelExtractor\n",
    "\n",
    "me = ModelExtractor(pipe.model, loss_fn)\n",
    "\n",
    "lista = me.get_layers_param()\n",
    "\n",
    "for k, item in lista.items():\n",
    "    print(k,item.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cpu\")\n",
    "x = next(iter(dl_tr))[0]\n",
    "pipe.model.eval()\n",
    "pipe.model(x.to(DEVICE))\n",
    "\n",
    "list_activations = me.get_activations(x)\n",
    "len(list_activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(dl_tr))[0][0]\n",
    "if x.dtype is not torch.int64:\n",
    "    res = me.get_decision_boundary(x, n_epochs=1)\n",
    "    res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, target = next(iter(dl_tr))\n",
    "if x.dtype is torch.float:\n",
    "    for gradient in me.get_gradients(x, target=target)[1]:\n",
    "        print(gradient.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise activations and other topological aspects of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.visualisation import Visualiser\n",
    "\n",
    "vs = Visualiser(pipe)\n",
    "\n",
    "vs.plot_data_model()\n",
    "#vs.plot_activations(x)\n",
    "#vs.plot_persistence_diagrams(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
