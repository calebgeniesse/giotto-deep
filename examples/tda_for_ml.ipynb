{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toplogy of Deep Neural Networks\n",
    "\n",
    "This notebook will show you how easy it is to use gdeep to reproduce the experiments of the paper [Topology of Deep Neural Networks](https://arxiv.org/pdf/2004.06093.pdf), by Naizat et. al. In this work, the authors studied the evolution of the topology of a dataset as embedded in the successive layers of a Neural Network, trained for classification on this dataset.\n",
    "\n",
    "Their main findings can be summarized as follows:\n",
    "\n",
    "- Neural networks tend to simplify the topology of the dataset accross layers.\n",
    "\n",
    "- This decrease in topological complexity is more efficient when the activation functions are non-homeomorphic, as it is the case for ReLu or leakyReLu.\n",
    "\n",
    "Here is an illustration from the paper:\n",
    "\n",
    "![illustration](/notebook_images/tda_dl/intro.png)\n",
    "\n",
    "The main steps of this tutorial will be as follows:\n",
    "\n",
    "1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPUs...\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import autograd  \n",
    "\n",
    "#gdeep\n",
    "from gdeep.data.datasets import DatasetBuilder, DataLoaderBuilder\n",
    "from gdeep.models import FFNet\n",
    "from gdeep.visualisation import persistence_diagrams_of_activations\n",
    "from gdeep.data.preprocessors import ToTensorImage\n",
    "from gdeep.trainer import Trainer\n",
    "from gdeep.search import Benchmark\n",
    "\n",
    "\n",
    "\n",
    "# plot\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# TDA\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.plotting import plot_diagram\n",
    "\n",
    "#Tensorboard\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the reuslts of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the Entangled Tori dataset and prepare the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import  RandomSampler\n",
    "db = DatasetBuilder(name=\"EntangledTori\")\n",
    "ds_tr, ds_val, ds_ts = db.build( n_pts = 50)\n",
    "dl_tr, dl_val, dl_ts = DataLoaderBuilder((ds_tr, ds_val, ds_ts)).build(    \n",
    "     [{\"batch_size\":100, \"sampler\":RandomSampler(ds_tr)}, \n",
    "     {\"batch_size\":100, \"sampler\":RandomSampler(ds_tr)}, \n",
    "     {\"batch_size\":100, \"sampler\":RandomSampler(ds_tr)}]\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define models with different activations functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "architecture = [3,5,5,5,5,5,2]\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "activation_string = [\"relu\", \"leakyrelu\", \"tanh\", \"sigmoid\"]\n",
    "activation_functions = [F.relu, F.leaky_relu, torch.tanh, torch.sigmoid]\n",
    "models = []\n",
    "writers = []\n",
    "trainers = []\n",
    "for i in range(len(activation_functions)):\n",
    "    model_temp = FFNet(arch = architecture, activation = activation_functions[i])\n",
    "    writer_temp = SummaryWriter(log_dir='runs/' + model_temp.__class__.__name__ + activation_string[i])\n",
    "    trainer_temp = Trainer(model_temp, [dl_tr, dl_ts], loss_function, writer_temp)\n",
    "    models.append(model_temp)\n",
    "    writers.append(writer_temp)\n",
    "    trainers.append(trainer_temp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.631131 \tEpoch training accuracy: 59.53%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/berkouknicolas/giotto-deep/gdeep/trainer/trainer.py:455: UserWarning:\n",
      "\n",
      "Cannot store data in the PR curve\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " accuracy: 62.70%,                 Avg loss: 0.603961 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.604639 \tEpoch training accuracy: 62.15%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 61.73%,                 Avg loss: 0.613148 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.599063 \tEpoch training accuracy: 62.39%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 62.83%,                 Avg loss: 0.599826 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.590372 \tEpoch training accuracy: 63.09%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 64.39%,                 Avg loss: 0.585976 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.553227 \tEpoch training accuracy: 68.27%                                                           \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.91%,                 Avg loss: 0.490435 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.483115 \tEpoch training accuracy: 71.85%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.15%,                 Avg loss: 0.477411 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.475412 \tEpoch training accuracy: 72.02%                                                 81 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.80%,                 Avg loss: 0.463148 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.471678 \tEpoch training accuracy: 71.93%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.59%,                 Avg loss: 0.461328 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.462119 \tEpoch training accuracy: 71.86%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.45%,                 Avg loss: 0.450293 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.462369 \tEpoch training accuracy: 71.89%                                                 \n",
      "Time taken for this epoch: 2.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.39%,                 Avg loss: 0.466551 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.634419 \tEpoch training accuracy: 58.61%                                       ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 62.49%,                 Avg loss: 0.600146 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.606435 \tEpoch training accuracy: 61.95%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 62.85%,                 Avg loss: 0.599088 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.602076 \tEpoch training accuracy: 62.36%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 62.95%,                 Avg loss: 0.594520 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.601156 \tEpoch training accuracy: 62.45%                                                             62.5  \t[ 24 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 62.59%,                 Avg loss: 0.597177 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.602025 \tEpoch training accuracy: 62.29%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 62.94%,                 Avg loss: 0.594524 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.585743 \tEpoch training accuracy: 63.82%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 69.66%,                 Avg loss: 0.535061 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.523370 \tEpoch training accuracy: 68.11%                                                            \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 64.84%,                 Avg loss: 0.492047 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.498571 \tEpoch training accuracy: 68.37%                                                            \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 70.67%,                 Avg loss: 0.484617 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.479386 \tEpoch training accuracy: 71.34%                                                 \n",
      "Time taken for this epoch: 2.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 67.89%,                 Avg loss: 0.506610 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.482651 \tEpoch training accuracy: 70.88%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 72.10%,                 Avg loss: 0.467904 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.643629 \tEpoch training accuracy: 59.10%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 63.56%,                 Avg loss: 0.603182 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.582285 \tEpoch training accuracy: 62.62%                                                              \t[ 110 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 64.96%,                 Avg loss: 0.545446 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.536594 \tEpoch training accuracy: 64.92%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 67.67%,                 Avg loss: 0.525588 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.516865 \tEpoch training accuracy: 65.90%                                                             \t[ 104 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 65.86%,                 Avg loss: 0.511105 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.512446 \tEpoch training accuracy: 65.85%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 67.45%,                 Avg loss: 0.504621 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.503978 \tEpoch training accuracy: 66.85%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 67.96%,                 Avg loss: 0.499474 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.493426 \tEpoch training accuracy: 67.57%                                                 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 67.95%,                 Avg loss: 0.476356 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.487004 \tEpoch training accuracy: 68.35%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 67.62%,                 Avg loss: 0.488687 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.487344 \tEpoch training accuracy: 68.78%                                                 \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 68.16%,                 Avg loss: 0.492495 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.482362 \tEpoch training accuracy: 69.35%                                                              \t[ 78 / 160 ]                       \t[ 105 / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 69.36%,                 Avg loss: 0.475711 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.691735 \tEpoch training accuracy: 51.34%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 59.80%,                 Avg loss: 0.671199 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.631610 \tEpoch training accuracy: 60.63%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 60.26%,                 Avg loss: 0.631401 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.618767 \tEpoch training accuracy: 60.69%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 60.16%,                 Avg loss: 0.620069 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.616411 \tEpoch training accuracy: 60.63%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 60.24%,                 Avg loss: 0.619939 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.616491 \tEpoch training accuracy: 60.62%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 60.16%,                 Avg loss: 0.619759 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.615942 \tEpoch training accuracy: 60.42%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 60.20%,                 Avg loss: 0.617970 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.616432 \tEpoch training accuracy: 60.71%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 60.44%,                 Avg loss: 0.624456 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.615096 \tEpoch training accuracy: 60.73%                                       ]                      / 160 ]                     \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 60.25%,                 Avg loss: 0.617486 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.615019 \tEpoch training accuracy: 60.63%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 60.23%,                 Avg loss: 0.617157 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.614944 \tEpoch training accuracy: 60.67%                                                             \n",
      "Time taken for this epoch: 1.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 60.24%,                 Avg loss: 0.620491 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pipe in trainers:\n",
    "    pipe.train(\n",
    "    Adam,\n",
    "    10,\n",
    "    False,\n",
    "    {\"lr\": 0.01},\n",
    "    {\"batch_size\": 200})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.analysis.interpretability import Interpreter\n",
    "from gdeep.visualisation import Visualiser\n",
    "\n",
    "vs = Visualiser(trainers[0]) \n",
    "vs.plot_3d_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_batch_dataset, _, _ = DataLoaderBuilder((ds_tr, ds_val, ds_ts)).build([{\"batch_size\":1600, \"sampler\":RandomSampler(ds_tr)}, {\"batch_size\":1600, \"sampler\":RandomSampler(ds_tr)}, {\"batch_size\":1600, \"sampler\":RandomSampler(ds_tr)}]) \n",
    "\n",
    "\n",
    "for pipe in trainers:\n",
    "    vs = Visualiser(pipe)\n",
    "    vs.plot_persistence_diagrams(next(iter(one_batch_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.plot_3d_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
