{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toplogy of Deep Neural Networks\n",
    "\n",
    "This notebook will show you how easy it is to use gdeep to reproduce the experiments of the paper [Topology of Deep Neural Networks](https://arxiv.org/pdf/2004.06093.pdf), by Naizat et. al. In this work, the authors studied the evolution of the topology of a dataset as embedded in the successive layers of a Neural Network, trained for classification on this dataset.\n",
    "\n",
    "Their main findings can be summarized as follows:\n",
    "\n",
    "- Neural networks tend to simplify the topology of the dataset accross layers.\n",
    "\n",
    "- This decrease in topological complexity is more efficient when the activation functions are non-homeomorphic, as it is the case for ReLu or leakyReLu.\n",
    "\n",
    "Here is an illustration from the paper:\n",
    "\n",
    "![illustration](/notebook_images/tda_dl/intro.png)\n",
    "\n",
    "The main steps of this tutorial will be as follows:\n",
    "\n",
    "1. Import the Entangled Tori dataset.\n",
    "2. Build several fully connected networks, with different activation functions.\n",
    "3. Train these networks to classify the Entangled Tori datasets.\n",
    "4. Visualise in tensorboard the persistence diagrams of the dataset embedded in each layers of each network.\n",
    "5. Study the decrease in topological complexity of the dataset accross layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the packages that will be needed for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPUs...\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import autograd  \n",
    "\n",
    "#gdeep\n",
    "from gdeep.data.datasets import DatasetBuilder, DataLoaderBuilder\n",
    "from gdeep.models import FFNet\n",
    "from gdeep.visualisation import persistence_diagrams_of_activations\n",
    "from gdeep.data.preprocessors import ToTensorImage\n",
    "from gdeep.trainer import Trainer\n",
    "from gdeep.search import Benchmark\n",
    "\n",
    "\n",
    "\n",
    "# plot\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# TDA\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.plotting import plot_diagram\n",
    "\n",
    "#Tensorboard\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize the tensorboard writer and import the Entangled Tori dataset\n",
    "\n",
    "In order to analyse the reuslts of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import  RandomSampler\n",
    "db = DatasetBuilder(name=\"EntangledTori\")\n",
    "ds_tr, ds_val, ds_ts = db.build( n_pts = 50)\n",
    "dl_tr, dl_val, dl_ts = DataLoaderBuilder((ds_tr, ds_val, ds_ts)).build(    \n",
    "     [{\"batch_size\":100, \"sampler\":RandomSampler(ds_tr)}, \n",
    "     {\"batch_size\":100, \"sampler\":RandomSampler(ds_tr)}, \n",
    "     {\"batch_size\":100, \"sampler\":RandomSampler(ds_tr)}]\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choose the architecture and activations functions of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "architecture = [3,5,5,5,5,2]\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "activation_string = [\"relu\", \"leakyrelu\", \"tanh\", \"sigmoid\"]\n",
    "activation_functions = [F.relu, F.leaky_relu, F.tanh, F.sigmoid]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "writers = []\n",
    "trainers = []\n",
    "for i in range(len(activation_functions)):\n",
    "    model_temp = FFNet(arch = architecture, activation = activation_functions[i])\n",
    "    writer_temp = SummaryWriter(log_dir='runs/' + model_temp.__class__.__name__ + activation_string[i])\n",
    "    trainer_temp = Trainer(model_temp, [dl_tr, dl_ts], loss_function, writer_temp)\n",
    "    models.append(model_temp)\n",
    "    writers.append(writer_temp)\n",
    "    trainers.append(trainer_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.637522 \tEpoch training accuracy: 58.57%                                                             \tBatch training accuracy:  51.0  \t[ 34 / 320 ]                      \tBatch training accuracy:  54.0  \t[ 84 / 320 ]                      95 / 320 ]                      \tBatch training accuracy:  60.0  \t[ 141 / 320 ]                      \tBatch training accuracy:  68.0  \t[ 297 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/berkouknicolas/giotto-deep/gdeep/trainer/trainer.py:455: UserWarning:\n",
      "\n",
      "Cannot store data in the PR curve\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " accuracy: 64.26%,                 Avg loss: 0.581360 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.531500 \tEpoch training accuracy: 68.68%                                                             \tBatch training accuracy:  67.0  \t[ 5 / 320 ]                     72.0  \t[ 49 / 320 ]                       \tBatch training accuracy:  76.0  \t[ 186 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 73.17%,                 Avg loss: 0.476448 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.477901 \tEpoch training accuracy: 72.68%                                                 4.0  \t[ 10 / 320 ]                      70.0  \t[ 136 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 73.03%,                 Avg loss: 0.486837 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.472375 \tEpoch training accuracy: 72.80%                                                 \tBatch training accuracy:  71.0  \t[ 31 / 320 ]                     48 / 320 ]                      \tBatch training accuracy:  69.0  \t[ 68 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 72.94%,                 Avg loss: 0.469534 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.472533 \tEpoch training accuracy: 72.44%                                                   \tBatch training accuracy:  74.0  \t[ 162 / 320 ]                      \tBatch training accuracy:  67.0  \t[ 223 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.38%,                 Avg loss: 0.506563 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.468405 \tEpoch training accuracy: 72.94%                                                  \tBatch training accuracy:  66.0  \t[ 71 / 320 ]                     68.0  \t[ 213 / 320 ]                      \tBatch training accuracy:  69.0  \t[ 320 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 72.44%,                 Avg loss: 0.479012 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.467017 \tEpoch training accuracy: 72.98%                                                 \tBatch training accuracy:  67.0  \t[ 20 / 320 ]                       \tBatch training accuracy:  74.0  \t[ 31 / 320 ]                      \tBatch training accuracy:  73.0  \t[ 129 / 320 ]                      \tBatch training accuracy:  74.0  \t[ 301 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.72%,                 Avg loss: 0.479489 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.464751 \tEpoch training accuracy: 73.09%                                                 \tBatch training accuracy:  77.0  \t[ 45 / 320 ]                     320 ]                     / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 73.04%,                 Avg loss: 0.467379 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.464703 \tEpoch training accuracy: 73.23%                                                  \tBatch training accuracy:  75.0  \t[ 56 / 320 ]                      \t[ 134 / 320 ]                      \tBatch training accuracy:  70.0  \t[ 168 / 320 ]                      / 320 ]                     / 320 ]                     \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 72.85%,                 Avg loss: 0.477380 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.465982 \tEpoch training accuracy: 73.12%                                                             ning accuracy:  72.0  \t[ 43 / 320 ]                      \tBatch training accuracy:  81.0  \t[ 249 / 320 ]                     \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 73.35%,                 Avg loss: 0.460922 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.576307 \tEpoch training accuracy: 63.39%                                                             \tBatch training accuracy:  65.0  \t[ 104 / 320 ]                       \tBatch training accuracy:  70.0  \t[ 219 / 320 ]                     \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 67.39%,                 Avg loss: 0.554773 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.475927 \tEpoch training accuracy: 74.36%                                                   \tBatch training accuracy:  68.0  \t[ 103 / 320 ]                      \tBatch training accuracy:  69.0  \t[ 137 / 320 ]                       \t[ 245 / 320 ]                     \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 73.41%,                 Avg loss: 0.464846 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.429067 \tEpoch training accuracy: 77.49%                                                 \tBatch training accuracy:  83.0  \t[ 44 / 320 ]                      \tBatch training accuracy:  78.0  \t[ 51 / 320 ]                       \tBatch training accuracy:  78.0  \t[ 76 / 320 ]                      72.0  \t[ 181 / 320 ]                     77.0  \t[ 205 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 79.15%,                 Avg loss: 0.418400 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.423752 \tEpoch training accuracy: 77.91%                                                 \t[ 73 / 320 ]                     \n",
      "Time taken for this epoch: 7.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 79.03%,                 Avg loss: 0.413256 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.403585 \tEpoch training accuracy: 79.45%                                                 \tBatch training accuracy:  70.0  \t[ 60 / 320 ]                       \tBatch training accuracy:  83.0  \t[ 72 / 320 ]                     95 / 320 ]                     228 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 81.45%,                 Avg loss: 0.387078 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.399124 \tEpoch training accuracy: 80.12%                                                 \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 81.10%,                 Avg loss: 0.404468 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.404369 \tEpoch training accuracy: 79.07%                                                 86.0  \t[ 221 / 320 ]                      \tBatch training accuracy:  72.0  \t[ 288 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 75.90%,                 Avg loss: 0.435669 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.408550 \tEpoch training accuracy: 78.01%                                                 \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 82.16%,                 Avg loss: 0.388630 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.391947 \tEpoch training accuracy: 80.43%                                                 \tBatch training accuracy:  82.0  \t[ 57 / 320 ]                       \tBatch training accuracy:  87.0  \t[ 237 / 320 ]                      78.0  \t[ 247 / 320 ]                     \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 76.22%,                 Avg loss: 0.410636 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.383147 \tEpoch training accuracy: 79.19%                                                   \tBatch training accuracy:  75.0  \t[ 204 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 79.44%,                 Avg loss: 0.399403 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  0.7128083791051593  \tBatch training accuracy:  54.0  \t[ 7 / 320 ]                     \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/berkouknicolas/.local/lib/python3.9/site-packages/torch/nn/functional.py:1933: UserWarning:\n",
      "\n",
      "nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.602180 \tEpoch training accuracy: 61.16%                                                              \tBatch training accuracy:  47.0  \t[ 14 / 320 ]                      / 320 ]                      \tBatch training accuracy:  61.0  \t[ 88 / 320 ]                       \tBatch training accuracy:  66.0  \t[ 248 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 61.94%,                 Avg loss: 0.541753 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.536967 \tEpoch training accuracy: 65.11%                                                              \tBatch training accuracy:  64.0  \t[ 77 / 320 ]                     ]                     \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 65.36%,                 Avg loss: 0.521280 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.508890 \tEpoch training accuracy: 65.98%                                                               \tBatch training accuracy:  66.0  \t[ 187 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 65.14%,                 Avg loss: 0.498201 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.499357 \tEpoch training accuracy: 67.07%                                                               \tBatch training accuracy:  68.0  \t[ 242 / 320 ]                     320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 68.67%,                 Avg loss: 0.488658 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.490420 \tEpoch training accuracy: 69.37%                                                              ing accuracy:  71.0  \t[ 56 / 320 ]                     \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.06%,                 Avg loss: 0.470437 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.483545 \tEpoch training accuracy: 70.05%                                                                        \tBatch training accuracy:  77.0  \t[ 100 / 320 ]                      \tBatch training accuracy:  70.0  \t[ 106 / 320 ]                     67.0  \t[ 250 / 320 ]                       \tBatch training accuracy:  73.0  \t[ 293 / 320 ]                       \tBatch training accuracy:  76.0  \t[ 304 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.61%,                 Avg loss: 0.471184 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.480268 \tEpoch training accuracy: 70.70%                                                              ng accuracy:  71.0  \t[ 9 / 320 ]                       \tBatch training accuracy:  65.0  \t[ 30 / 320 ]                       \tBatch training accuracy:  62.0  \t[ 40 / 320 ]                       \tBatch training accuracy:  60.0  \t[ 71 / 320 ]                       \tBatch training accuracy:  69.0  \t[ 81 / 320 ]                     \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 70.85%,                 Avg loss: 0.471633 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.477048 \tEpoch training accuracy: 70.36%                                                              \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.33%,                 Avg loss: 0.487526 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.481898 \tEpoch training accuracy: 70.41%                                                              ing accuracy:  67.0  \t[ 57 / 320 ]                      80.0  \t[ 163 / 320 ]                       \tBatch training accuracy:  79.0  \t[ 272 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.09%,                 Avg loss: 0.468490 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.478302 \tEpoch training accuracy: 70.34%                                                              320 ]                      \tBatch training accuracy:  70.0  \t[ 256 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 72.00%,                 Avg loss: 0.469363 \n",
      "\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch training loss:  0.6937786042690277  \tBatch training accuracy:  49.0  \t[ 13 / 320 ]                     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/berkouknicolas/.local/lib/python3.9/site-packages/torch/nn/functional.py:1944: UserWarning:\n",
      "\n",
      "nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.671928 \tEpoch training accuracy: 55.22%                                       ]                     \tBatch training accuracy:  45.0  \t[ 55 / 320 ]                     \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 58.15%,                 Avg loss: 0.625799 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.615248 \tEpoch training accuracy: 62.17%                                                             \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 63.56%,                 Avg loss: 0.606195 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.591221 \tEpoch training accuracy: 64.47%                                                              60.0  \t[ 114 / 320 ]                       \tBatch training accuracy:  67.0  \t[ 253 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 65.11%,                 Avg loss: 0.580716 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.576354 \tEpoch training accuracy: 66.00%                                                             \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 65.92%,                 Avg loss: 0.564576 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.550873 \tEpoch training accuracy: 65.91%                                                             \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.75%,                 Avg loss: 0.543109 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.540133 \tEpoch training accuracy: 66.39%                                                                               \tBatch training accuracy:  75.0  \t[ 140 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.17%,                 Avg loss: 0.530591 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.533628 \tEpoch training accuracy: 66.12%                                                             0 ]                      \tBatch training accuracy:  69.0  \t[ 119 / 320 ]                     \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 67.36%,                 Avg loss: 0.535405 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.527800 \tEpoch training accuracy: 66.68%                                                             \tBatch training accuracy:  63.0  \t[ 116 / 320 ]                      \tBatch training accuracy:  75.0  \t[ 169 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.74%,                 Avg loss: 0.525327 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.525299 \tEpoch training accuracy: 66.66%                                                              \t[ 51 / 320 ]                      \t[ 112 / 320 ]                      \t[ 192 / 320 ]                     \n",
      "Time taken for this epoch: 5.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.31%,                 Avg loss: 0.519044 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.521087 \tEpoch training accuracy: 67.22%                                                                       \tBatch training accuracy:  74.0  \t[ 81 / 320 ]                      \tBatch training accuracy:  65.0  \t[ 300 / 320 ]                     \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 67.53%,                 Avg loss: 0.518019 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for pipe in trainers:\n",
    "    pipe.train(\n",
    "    Adam,\n",
    "    epochs,\n",
    "    False,\n",
    "    {\"lr\": 0.01},\n",
    "    {\"batch_size\": 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.analysis.interpretability import Interpreter\n",
    "from gdeep.visualisation import Visualiser\n",
    "\n",
    "vs = Visualiser(trainers[0]) \n",
    "vs.plot_3d_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_batch_dataset, _, _ = DataLoaderBuilder((ds_tr, ds_val, ds_ts)).build([{\"batch_size\":3000}, {\"batch_size\":3000},{\"batch_size\":3000}]) \n",
    "\n",
    "\n",
    "for pipe in trainers:\n",
    "    vs = Visualiser(pipe)\n",
    "    vs.plot_persistence_diagrams(next(iter(one_batch_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_SingleProcessDataLoaderIter' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/berkouknicolas/giotto-deep/examples/tda_for_ml.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2233342e3134302e3139362e3538222c2275736572223a226265726b6f756b6e69636f6c6173227d/home/berkouknicolas/giotto-deep/examples/tda_for_ml.ipynb#ch0000010vscode-remote?line=0'>1</a>\u001b[0m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39;49m(dl_tr)[\u001b[39m5\u001b[39;49m])\n",
      "\u001b[0;31mTypeError\u001b[0m: '_SingleProcessDataLoaderIter' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNet(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (3): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (4): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.674135 \tEpoch training accuracy: 54.95%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 59.86%,                 Avg loss: 0.646908 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.643848 \tEpoch training accuracy: 57.15%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 62.86%,                 Avg loss: 0.607025 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.602208 \tEpoch training accuracy: 60.59%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.43%,                 Avg loss: 0.566409 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.575144 \tEpoch training accuracy: 63.69%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 66.00%,                 Avg loss: 0.519996 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.550739 \tEpoch training accuracy: 66.56%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 65.29%,                 Avg loss: 0.509049 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.550600 \tEpoch training accuracy: 64.10%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 71.86%,                 Avg loss: 0.507524 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.527803 \tEpoch training accuracy: 68.62%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 63.29%,                 Avg loss: 0.501394 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.507035 \tEpoch training accuracy: 68.54%                                              \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 69.86%,                 Avg loss: 0.511708 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.490313 \tEpoch training accuracy: 71.92%                                                         \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 74.57%,                 Avg loss: 0.462196 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.503157 \tEpoch training accuracy: 69.90%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 74.00%,                 Avg loss: 0.449597 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.473748 \tEpoch training accuracy: 72.97%                                      ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 74.86%,                 Avg loss: 0.441772 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.460426 \tEpoch training accuracy: 74.13%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 74.14%,                 Avg loss: 0.454806 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.476365 \tEpoch training accuracy: 72.59%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 72.14%,                 Avg loss: 0.448931 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.472947 \tEpoch training accuracy: 72.67%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 76.14%,                 Avg loss: 0.440738 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.429871 \tEpoch training accuracy: 77.15%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 77.71%,                 Avg loss: 0.403257 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.441364 \tEpoch training accuracy: 77.54%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 77.00%,                 Avg loss: 0.423887 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.405863 \tEpoch training accuracy: 77.41%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 78.29%,                 Avg loss: 0.395945 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.426897 \tEpoch training accuracy: 77.92%                                               78.0  \t[ 8 / 26 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 75.29%,                 Avg loss: 0.430447 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.418932 \tEpoch training accuracy: 77.21%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 80.86%,                 Avg loss: 0.378035 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.401906 \tEpoch training accuracy: 78.18%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 77.71%,                 Avg loss: 0.378924 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.392556 \tEpoch training accuracy: 79.33%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 77.43%,                 Avg loss: 0.422089 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.405209 \tEpoch training accuracy: 78.28%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 76.14%,                 Avg loss: 0.489437 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.404089 \tEpoch training accuracy: 78.00%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 77.57%,                 Avg loss: 0.412841 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.391518 \tEpoch training accuracy: 80.08%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 78.29%,                 Avg loss: 0.404222 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.392458 \tEpoch training accuracy: 79.46%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 77.29%,                 Avg loss: 0.409198 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.383965 \tEpoch training accuracy: 79.95%                                      ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 78.29%,                 Avg loss: 0.430988 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Epoch training loss: 0.409204 \tEpoch training accuracy: 79.38%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 77.71%,                 Avg loss: 0.409387 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.395328 \tEpoch training accuracy: 79.51%                                                          \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 77.86%,                 Avg loss: 0.388396 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.393951 \tEpoch training accuracy: 79.08%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 77.43%,                 Avg loss: 0.452332 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Epoch training loss: 0.408876 \tEpoch training accuracy: 78.41%                                      ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 77.57%,                 Avg loss: 0.423477 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Epoch training loss: 0.412361 \tEpoch training accuracy: 79.15%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 78.86%,                 Avg loss: 0.388417 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Epoch training loss: 0.370999 \tEpoch training accuracy: 81.21%                                      ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 78.00%,                 Avg loss: 0.407501 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Epoch training loss: 0.380971 \tEpoch training accuracy: 81.03%                                      ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 81.57%,                 Avg loss: 0.372545 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Epoch training loss: 0.369645 \tEpoch training accuracy: 82.13%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 79.43%,                 Avg loss: 0.416298 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Epoch training loss: 0.391321 \tEpoch training accuracy: 79.74%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 75.29%,                 Avg loss: 0.408897 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Epoch training loss: 0.380190 \tEpoch training accuracy: 79.64%                                      ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 79.00%,                 Avg loss: 0.401530 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Epoch training loss: 0.385319 \tEpoch training accuracy: 80.41%                                                            26 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 76.14%,                 Avg loss: 0.437507 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Epoch training loss: 0.389924 \tEpoch training accuracy: 79.49%                                      ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 79.71%,                 Avg loss: 0.388632 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Epoch training loss: 0.376255 \tEpoch training accuracy: 81.67%                                      ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 77.71%,                 Avg loss: 0.434298 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Epoch training loss: 0.351882 \tEpoch training accuracy: 82.49%                                      ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 80.86%,                 Avg loss: 0.372799 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Epoch training loss: 0.347922 \tEpoch training accuracy: 82.15%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 79.00%,                 Avg loss: 0.419828 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Epoch training loss: 0.392484 \tEpoch training accuracy: 80.41%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 77.00%,                 Avg loss: 0.442525 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Epoch training loss: 0.368636 \tEpoch training accuracy: 81.62%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 78.00%,                 Avg loss: 0.400297 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Epoch training loss: 0.348859 \tEpoch training accuracy: 83.08%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 79.71%,                 Avg loss: 0.384856 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Epoch training loss: 0.349202 \tEpoch training accuracy: 83.46%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 83.43%,                 Avg loss: 0.361176 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Epoch training loss: 0.371681 \tEpoch training accuracy: 81.87%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 83.43%,                 Avg loss: 0.381860 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Epoch training loss: 0.344617 \tEpoch training accuracy: 83.31%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 84.86%,                 Avg loss: 0.399426 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Epoch training loss: 0.370233 \tEpoch training accuracy: 83.03%                                      ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 83.86%,                 Avg loss: 0.372347 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Epoch training loss: 0.329834 \tEpoch training accuracy: 84.36%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 86.00%,                 Avg loss: 0.336008 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Epoch training loss: 0.336725 \tEpoch training accuracy: 84.10%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 76.57%,                 Avg loss: 0.403365 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Epoch training loss: 0.333148 \tEpoch training accuracy: 83.74%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 85.00%,                 Avg loss: 0.351451 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Epoch training loss: 0.369997 \tEpoch training accuracy: 82.69%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 80.86%,                 Avg loss: 0.356772 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Epoch training loss: 0.348204 \tEpoch training accuracy: 83.77%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 83.29%,                 Avg loss: 0.344419 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Epoch training loss: 0.349374 \tEpoch training accuracy: 84.08%                                                \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 81.57%,                 Avg loss: 0.386892 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Epoch training loss: 0.325924 \tEpoch training accuracy: 85.21%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 88.00%,                 Avg loss: 0.301459 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Epoch training loss: 0.322469 \tEpoch training accuracy: 85.38%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 85.43%,                 Avg loss: 0.349328 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Epoch training loss: 0.307681 \tEpoch training accuracy: 85.97%                                      ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 84.29%,                 Avg loss: 0.347607 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Epoch training loss: 0.331281 \tEpoch training accuracy: 84.64%                                      ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 82.57%,                 Avg loss: 0.371840 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Epoch training loss: 0.317142 \tEpoch training accuracy: 85.38%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 86.00%,                 Avg loss: 0.328026 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Epoch training loss: 0.314502 \tEpoch training accuracy: 85.85%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 83.57%,                 Avg loss: 0.355006 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Epoch training loss: 0.318248 \tEpoch training accuracy: 85.18%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 85.71%,                 Avg loss: 0.327045 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Epoch training loss: 0.315583 \tEpoch training accuracy: 85.72%                                                         \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 87.57%,                 Avg loss: 0.341947 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Epoch training loss: 0.322150 \tEpoch training accuracy: 86.49%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 81.29%,                 Avg loss: 0.388372 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Epoch training loss: 0.385593 \tEpoch training accuracy: 81.08%                                                \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 81.43%,                 Avg loss: 0.403374 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Epoch training loss: 0.341900 \tEpoch training accuracy: 83.36%                                      ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 84.71%,                 Avg loss: 0.311117 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Epoch training loss: 0.282349 \tEpoch training accuracy: 87.49%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 86.00%,                 Avg loss: 0.304612 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Epoch training loss: 0.293298 \tEpoch training accuracy: 86.90%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 89.00%,                 Avg loss: 0.306404 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Epoch training loss: 0.280885 \tEpoch training accuracy: 87.92%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 85.00%,                 Avg loss: 0.348688 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Epoch training loss: 0.324876 \tEpoch training accuracy: 85.54%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 84.71%,                 Avg loss: 0.339987 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Epoch training loss: 0.280973 \tEpoch training accuracy: 87.90%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 88.43%,                 Avg loss: 0.295394 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Epoch training loss: 0.312539 \tEpoch training accuracy: 86.95%                                      ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 82.29%,                 Avg loss: 0.477928 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Epoch training loss: 0.362330 \tEpoch training accuracy: 83.44%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 88.86%,                 Avg loss: 0.300548 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Epoch training loss: 0.308108 \tEpoch training accuracy: 85.46%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 85.00%,                 Avg loss: 0.363721 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Epoch training loss: 0.326445 \tEpoch training accuracy: 84.62%                                               80.0  \t[ 19 / 26 ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 89.71%,                 Avg loss: 0.300372 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Epoch training loss: 0.294706 \tEpoch training accuracy: 87.62%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 85.29%,                 Avg loss: 0.371186 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Epoch training loss: 0.276688 \tEpoch training accuracy: 88.54%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 89.43%,                 Avg loss: 0.303136 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Epoch training loss: 0.272358 \tEpoch training accuracy: 88.15%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 81.71%,                 Avg loss: 0.360116 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Epoch training loss: 0.291021 \tEpoch training accuracy: 86.41%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 87.29%,                 Avg loss: 0.304473 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Epoch training loss: 0.296540 \tEpoch training accuracy: 86.23%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 89.00%,                 Avg loss: 0.302070 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Epoch training loss: 0.269197 \tEpoch training accuracy: 88.87%                                      ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 88.29%,                 Avg loss: 0.293989 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Epoch training loss: 0.280711 \tEpoch training accuracy: 88.51%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 82.86%,                 Avg loss: 0.352057 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Epoch training loss: 0.284784 \tEpoch training accuracy: 87.00%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 85.57%,                 Avg loss: 0.353898 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Epoch training loss: 0.270851 \tEpoch training accuracy: 86.90%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 88.43%,                 Avg loss: 0.338396 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Epoch training loss: 0.337262 \tEpoch training accuracy: 85.05%                                                          \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 80.14%,                 Avg loss: 0.385902 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Epoch training loss: 0.311507 \tEpoch training accuracy: 84.36%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 87.57%,                 Avg loss: 0.285670 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Epoch training loss: 0.277195 \tEpoch training accuracy: 85.87%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 85.43%,                 Avg loss: 0.408915 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Epoch training loss: 0.292232 \tEpoch training accuracy: 87.38%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 90.86%,                 Avg loss: 0.279201 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Epoch training loss: 0.292951 \tEpoch training accuracy: 86.00%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 88.71%,                 Avg loss: 0.324766 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Epoch training loss: 0.279553 \tEpoch training accuracy: 88.69%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 87.29%,                 Avg loss: 0.382660 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Epoch training loss: 0.253358 \tEpoch training accuracy: 89.00%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 85.29%,                 Avg loss: 0.304772 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Epoch training loss: 0.262583 \tEpoch training accuracy: 88.92%                                                \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 90.86%,                 Avg loss: 0.276906 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Epoch training loss: 0.266465 \tEpoch training accuracy: 88.41%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 84.71%,                 Avg loss: 0.330478 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Epoch training loss: 0.287864 \tEpoch training accuracy: 86.82%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 83.14%,                 Avg loss: 0.379972 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Epoch training loss: 0.272893 \tEpoch training accuracy: 88.64%                                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 85.57%,                 Avg loss: 0.323036 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Epoch training loss: 0.276951 \tEpoch training accuracy: 87.72%                                      ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 88.00%,                 Avg loss: 0.305980 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Epoch training loss: 0.296257 \tEpoch training accuracy: 85.77%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 83.57%,                 Avg loss: 0.378501 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Epoch training loss: 0.310228 \tEpoch training accuracy: 86.62%                                               \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 87.00%,                 Avg loss: 0.288034 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Epoch training loss: 0.296440 \tEpoch training accuracy: 87.59%                                                          \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 86.71%,                 Avg loss: 0.322689 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Epoch training loss: 0.316061 \tEpoch training accuracy: 84.97%                                                           \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 88.43%,                 Avg loss: 0.302722 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Epoch training loss: 0.268035 \tEpoch training accuracy: 87.64%                                      ]                     \n",
      "Time taken for this epoch: 0.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " accuracy: 90.86%,                 Avg loss: 0.251322 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2513222247362137, 90.85714285714286)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train NN\n",
    "model = FFNet(arch=[3,10,10,10,10,2])\n",
    "print(model)\n",
    "pipe = Trainer(model, (dl_tr, dl_ts), nn.CrossEntropyLoss(), writer)\n",
    "pipe.train(Adam, 100, False, {\"lr\":0.01}, {\"batch_size\":50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.analysis.interpretability import Interpreter\n",
    "from gdeep.visualisation import Visualiser\n",
    "\n",
    "vs = Visualiser(pipe)\n",
    "one_batch_dataset, _, _ = DataLoaderBuilder((ds_tr, ds_val, ds_ts)).build([{\"batch_size\":1600}, {\"batch_size\":1600}, {\"batch_size\":1600}]) \n",
    "\n",
    "\n",
    "\n",
    "# the diagrams can be seen on tensorboard!\n",
    "vs.plot_persistence_diagrams(next(iter(one_batch_dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs.plot_3d_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/berkouknicolas/giotto-deep/examples/tda_for_ml.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2233342e3134302e3139362e3538222c2275736572223a226265726b6f756b6e69636f6c6173227d/home/berkouknicolas/giotto-deep/examples/tda_for_ml.ipynb#ch0000014vscode-remote?line=0'>1</a>\u001b[0m model_temp \u001b[39m=\u001b[39m FFNet(arch \u001b[39m=\u001b[39;49m [\u001b[39m2\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m3\u001b[39;49m])\n",
      "File \u001b[0;32m~/giotto-deep/gdeep/models/simple_nn.py:25\u001b[0m, in \u001b[0;36mFFNet.__init__\u001b[0;34m(self, arch, activation)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, arch\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m2\u001b[39m), activation \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu):        \n\u001b[0;32m---> 25\u001b[0m     \u001b[39msuper\u001b[39;49m(FFNet, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m     26\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation \u001b[39m=\u001b[39m activation\n\u001b[1;32m     27\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinears \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([nn\u001b[39m.\u001b[39mLinear(arch[i], arch[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(arch)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)])\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    " model_temp = FFNet(arch = [2,3,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 10, 10, 10, 10, 2]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
