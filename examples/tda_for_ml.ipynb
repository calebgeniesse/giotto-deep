{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toplogy of Deep Neural Networks\n",
    "\n",
    "This notebook will show you how easy it is to use gdeep to reproduce the experiments of the paper *Topology of Deep Neural Networks*, by Naizat et. al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU!\n",
      "Using GPU!\n",
      "No TPUs...\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# deep learning\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from gdeep.models import FFNet\n",
    "from gdeep.data import TorchDataLoader\n",
    "from gdeep.pipeline import Pipeline\n",
    "from torch import autograd  \n",
    "\n",
    "# plot\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# ML\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# TDA\n",
    "from gtda.homology import VietorisRipsPersistence\n",
    "from gtda.plotting import plot_diagram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the reuslts of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = TorchDataLoader(name=\"EntangledTori\")\n",
    "dl_tr, dl_ts = dl.build_dataloaders(batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot dataset to tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([135000, 3]), torch.Size([135000]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tensor = torch.cat([batch for batch, _ in dl_tr])\n",
    "label_tensor = torch.cat([batch for _ ,batch in dl_tr])\n",
    "data_tensor.shape, label_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNet(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=15, bias=True)\n",
      "    (1): Linear(in_features=15, out_features=15, bias=True)\n",
      "    (2): Linear(in_features=15, out_features=15, bias=True)\n",
      "    (3): Linear(in_features=15, out_features=15, bias=True)\n",
      "    (4): Linear(in_features=15, out_features=15, bias=True)\n",
      "    (5): Linear(in_features=15, out_features=15, bias=True)\n",
      "    (6): Linear(in_features=15, out_features=15, bias=True)\n",
      "    (7): Linear(in_features=15, out_features=15, bias=True)\n",
      "    (8): Linear(in_features=15, out_features=15, bias=True)\n",
      "    (9): Linear(in_features=15, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.694080 \tEpoch training accuracy: 50.11%                                      0800 ]                      \tBatch training accuracy:  50.65693430656935  \t[ 137 / 10800 ]                     514 / 10800 ]                      ]                      10800 ]                      \tBatch training accuracy:  49.92996498249125  \t[ 1999 / 10800 ]                      \tBatch training accuracy:  49.92857142857143  \t[ 2100 / 10800 ]                      2444 / 10800 ]                      49.77576711250983  \t[ 2542 / 10800 ]                       \tBatch training accuracy:  49.890764647467726  \t[ 3021 / 10800 ]                      50.05307050796057  \t[ 3957 / 10800 ]                      50.14516896715849  \t[ 4202 / 10800 ]                      \tBatch training accuracy:  50.15307288096869  \t[ 4377 / 10800 ]                      50.12012692656391  \t[ 4412 / 10800 ]                      \tBatch training accuracy:  50.11990407673861  \t[ 4587 / 10800 ]                      49.98901300128182  \t[ 5461 / 10800 ]                     49.976027397260275  \t[ 5840 / 10800 ]                     50.00673740946606  \t[ 5937 / 10800 ]                      \t[ 6424 / 10800 ]                     50.21676481643904  \t[ 7981 / 10800 ]                      50.19702007141977  \t[ 8121 / 10800 ]                      50.20107896027465  \t[ 8156 / 10800 ]                      50.216090831400315  \t[ 8191 / 10800 ]                      \tBatch training accuracy:  50.191266274916465  \t[ 8679 / 10800 ]                       \tBatch training accuracy:  50.17323333685434  \t[ 9467 / 10800 ]                     50.16933497536946  \t[ 9744 / 10800 ]                       \tBatch training accuracy:  50.18683996750609  \t[ 9848 / 10800 ]                     \n",
      "Time taken for this epoch: 64.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " Accuracy: 49.99%,                 Avg loss: 0.693181 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.693782 \tEpoch training accuracy: 50.22%                                      10800 ]                     9.70695970695971  \t[ 273 / 10800 ]                      49.59183673469388  \t[ 343 / 10800 ]                     ]                      49.79910714285714  \t[ 448 / 10800 ]                      \tBatch training accuracy:  49.862258953168045  \t[ 1452 / 10800 ]                      \t[ 1555 / 10800 ]                     50.368754956383825  \t[ 2522 / 10800 ]                      10800 ]                       \tBatch training accuracy:  50.44122837980939  \t[ 2833 / 10800 ]                     ]                       \tBatch training accuracy:  50.08421985815603  \t[ 6768 / 10800 ]                      50.08191925102399  \t[ 6836 / 10800 ]                      \tBatch training accuracy:  50.105248954257185  \t[ 7411 / 10800 ]                     50.080684007707134  \t[ 8304 / 10800 ]                       \t[ 8338 / 10800 ]                      \tBatch training accuracy:  50.08694695135373  \t[ 9086 / 10800 ]                       \tBatch training accuracy:  50.10720544273786  \t[ 9701 / 10800 ]                       \t[ 9906 / 10800 ]                     \n",
      "Time taken for this epoch: 64.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " Accuracy: 50.01%,                 Avg loss: 0.693178 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.693894 \tEpoch training accuracy: 50.13%                                      0800 ]                      \tBatch training accuracy:  49.514824797843666  \t[ 742 / 10800 ]                      10800 ]                     ]                     50.01554001554002  \t[ 1287 / 10800 ]                     10800 ]                      \tBatch training accuracy:  49.91364421416235  \t[ 1737 / 10800 ]                     50.0  \t[ 1798 / 10800 ]                       \tBatch training accuracy:  49.92481203007519  \t[ 1862 / 10800 ]                     50.21747885622231  \t[ 2483 / 10800 ]                     10800 ]                      0.6940040735869456  \tBatch training accuracy:  49.95424146427314  \t[ 2841 / 10800 ]                       \tBatch training accuracy:  49.87825124515772  \t[ 3614 / 10800 ]                       \t[ 3994 / 10800 ]                      \tBatch training accuracy:  50.06362058993638  \t[ 5187 / 10800 ]                     50.06129094043287  \t[ 5221 / 10800 ]                      50.085884988797616  \t[ 5356 / 10800 ]                     50.17144797148192  \t[ 5891 / 10800 ]                     10800 ]                      \tBatch training accuracy:  50.18822002271621  \t[ 6163 / 10800 ]                     50.15297450424929  \t[ 7060 / 10800 ]                       \t[ 7542 / 10800 ]                     \n",
      "Time taken for this epoch: 67.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " Accuracy: 49.99%,                 Avg loss: 0.693701 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.693701247241762, 49.992592592592594)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train NN\n",
    "model = FFNet(arch=[3,15,15,15,15,15,15,15,15,15,2])\n",
    "print(model)\n",
    "pipe = Pipeline(model, (dl_tr, dl_ts), nn.CrossEntropyLoss(), writer)\n",
    "pipe.train(Adam, 3, False, {\"lr\":0.01}, {\"batch_size\":10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
