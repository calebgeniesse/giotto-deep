{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPUs...\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "#  ## Benchmarking PersFormer on the graph datasets.\n",
    "#  We will compare the accuracy on the graph datasets of our SetTransformer\n",
    "#  based on PersFormer with the perslayer introduced in the paper:\n",
    "#  https://arxiv.org/abs/1904.09378\n",
    "# %% [markdown]\n",
    "#  ## Benchmarking MUTAG\n",
    "#  We will compare the test accuracies of PersLay and PersFormer on the MUTAG\n",
    "#  dataset. It consists of 188 graphs categorised into two classes.\n",
    "#  We will train the PersFormer on the same input features as PersFormer to\n",
    "#  get a fair comparison.\n",
    "#  The features PersLay is trained on are the extended persistence diagrams of\n",
    "#  the vertices of the graph filtered by the heat kernel signature (HKS)\n",
    "#  at time t=10.\n",
    "#  The maximum (wrt to the architecture and the hyperparameters) mean test\n",
    "#  accuracy of PersLay is 89.8(Â±0.9) and the train accuracy with the same\n",
    "#  model and the same hyperparameters is 92.3.\n",
    "#  They performed 10-fold evaluation, i.e. splitting the dataset into\n",
    "#  10 equally-sized folds and then record the test accuracy of the i-th\n",
    "#  fold and training the model on the 9 other folds.\n",
    "# %%\n",
    "from IPython import get_ipython\n",
    "get_ipython().magic('load_ext autoreload')\n",
    "get_ipython().magic('autoreload 2')\n",
    "\n",
    "# Import libraries:\n",
    "import os\n",
    "import json\n",
    "from dotmap import DotMap\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Import the PyTorch modules\n",
    "import torch  # type: ignore\n",
    "from torch import nn  # type: ignore\n",
    "from torch.optim import SGD, Adam, RMSprop, AdamW  # type: ignore\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Import Tensorflow writer\n",
    "#from torch.utils.tensorboard import SummaryWriter  # type: ignore\n",
    "from gdeep.search import GiottoSummaryWriter\n",
    "\n",
    "from transformers.optimization import get_cosine_with_hard_restarts_schedule_with_warmup, get_constant_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "# Import the giotto-deep modules\n",
    "from gdeep.topology_layers import Persformer\n",
    "from gdeep.pipeline import Pipeline\n",
    "from gdeep.search import Gridsearch\n",
    "#from gdeep.topology_layers import load_data_as_tensor, balance_binary_dataset,\\\n",
    "#    print_class_balance\n",
    "\n",
    "from optuna.pruners import MedianPruner, NopPruner\n",
    "\n",
    "# %%\n",
    "\n",
    "#Configs\n",
    "\n",
    "model_data_file = 'model_data_specifications'\n",
    "\n",
    "with open(os.path.join(model_data_file, 'Mutag_data.json')) as config_data_file:\n",
    "    config_data = DotMap(json.load(config_data_file))\n",
    "\n",
    "\n",
    "with open(os.path.join(model_data_file, 'Mutag_model.json')) as config_data_file:\n",
    "    config_model = DotMap(json.load(config_data_file))\n",
    "    \n",
    "\n",
    "with open(os.path.join(model_data_file, 'Mutag_hyperparameter_space.json')) as config_data_file:\n",
    "    hyperparameters_dicts = DotMap(json.load(config_data_file))\n",
    "    dataloaders_params = hyperparameters_dicts.dataloaders_params\n",
    "    models_hyperparams = hyperparameters_dicts.models_hyperparams\n",
    "    optimizers_params = hyperparameters_dicts.optimizers_params\n",
    "    schedulers_params = hyperparameters_dicts.schedulers_params\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with  open(\"diagrams_MNIST_subsample_500.pkl\", \"rb\") as f:\n",
    "    diagrams_MNIST = pickle.load(f)\n",
    "with open(\"labels_subsample.pkl\", \"rb\") as f2:\n",
    "    labels = pickle.load(f2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([12000, 500, 2]), torch.Size([12000]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pds =  torch.tensor(np.array(diagrams_MNIST), dtype = torch.float32 )\n",
    "y = torch.tensor(np.array(labels), dtype = torch.long)[:,1]\n",
    "x_pds.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class balance: 0.51\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 0.702759 \tEpoch training accuracy: 50.68%                                       ]                      \n",
      "Time taken for this epoch: 1620.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 53.92%,                 Avg loss: 0.691755 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 0.692516 \tEpoch training accuracy: 54.16%                                      5 ]                     \n",
      "Time taken for this epoch: 1511.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 57.67%,                 Avg loss: 0.680298 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 0.687068 \tEpoch training accuracy: 55.08%                                       ]                      \n",
      "Time taken for this epoch: 1634.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 58.46%,                 Avg loss: 0.705631 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch training loss: 0.684414 \tEpoch training accuracy: 56.51%                                      5 ]                     \n",
      "Time taken for this epoch: 1598.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 58.42%,                 Avg loss: 0.673651 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch training loss: 0.680792 \tEpoch training accuracy: 56.83%                                       ]                      \n",
      "Time taken for this epoch: 1310.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 56.58%,                 Avg loss: 0.679679 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch training loss: 0.679361 \tEpoch training accuracy: 57.46%                                      5 ]                     \n",
      "Time taken for this epoch: 1052.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 55.21%,                 Avg loss: 0.685069 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch training loss: 0.676256 \tEpoch training accuracy: 57.57%                                      5 ]                     \n",
      "Time taken for this epoch: 874.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 54.29%,                 Avg loss: 0.684472 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch training loss: 0.677163 \tEpoch training accuracy: 57.09%                                      5 ]                     \n",
      "Time taken for this epoch: 571.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 57.83%,                 Avg loss: 0.676786 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch training loss: 0.674290 \tEpoch training accuracy: 57.56%                                       ]                      \n",
      "Time taken for this epoch: 543.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 58.04%,                 Avg loss: 0.668428 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Epoch training loss: 0.671488 \tEpoch training accuracy: 58.26%                                       ]                      \n",
      "Time taken for this epoch: 550.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 58.46%,                 Avg loss: 0.668601 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Epoch training loss: 0.672751 \tEpoch training accuracy: 57.98%                                       ]                      \n",
      "Time taken for this epoch: 598.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 57.79%,                 Avg loss: 0.671744 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Epoch training loss: 0.673077 \tEpoch training accuracy: 58.03%                                      5 ]                     \n",
      "Time taken for this epoch: 610.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 58.92%,                 Avg loss: 0.671442 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Epoch training loss: 0.670962 \tEpoch training accuracy: 58.42%                                       ]                      \n",
      "Time taken for this epoch: 534.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 54.29%,                 Avg loss: 0.682300 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Epoch training loss: 0.671989 \tEpoch training accuracy: 57.73%                                       ]                      \n",
      "Time taken for this epoch: 611.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 58.17%,                 Avg loss: 0.668459 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Epoch training loss: 0.670202 \tEpoch training accuracy: 58.59%                                      5 ]                     \n",
      "Time taken for this epoch: 569.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 58.17%,                 Avg loss: 0.681404 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Epoch training loss: 0.672031 \tEpoch training accuracy: 57.70%                                       ]                      \n",
      "Time taken for this epoch: 513.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 58.75%,                 Avg loss: 0.667983 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Epoch training loss: 0.669125 \tEpoch training accuracy: 58.00%                                       ]                      \n",
      "Time taken for this epoch: 533.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.29%,                 Avg loss: 0.666339 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Epoch training loss: 0.667575 \tEpoch training accuracy: 58.04%                                       ]                      \n",
      "Time taken for this epoch: 594.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 58.83%,                 Avg loss: 0.664988 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Epoch training loss: 0.669315 \tEpoch training accuracy: 58.33%                                      5 ]                     \n",
      "Time taken for this epoch: 565.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 58.33%,                 Avg loss: 0.665785 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Epoch training loss: 0.667320 \tEpoch training accuracy: 58.43%                                       ]                      \n",
      "Time taken for this epoch: 523.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.29%,                 Avg loss: 0.666268 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Epoch training loss: 0.669069 \tEpoch training accuracy: 58.56%                                      ]                       \n",
      "Time taken for this epoch: 561.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.25%,                 Avg loss: 0.662572 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Epoch training loss: 0.666762 \tEpoch training accuracy: 58.34%                                       ]                      \n",
      "Time taken for this epoch: 519.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.75%,                 Avg loss: 0.664641 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Epoch training loss: 0.667619 \tEpoch training accuracy: 58.59%                                      5 ]                     \n",
      "Time taken for this epoch: 457.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.21%,                 Avg loss: 0.661681 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Epoch training loss: 0.666857 \tEpoch training accuracy: 58.75%                                      5 ]                     \n",
      "Time taken for this epoch: 440.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.71%,                 Avg loss: 0.655833 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Epoch training loss: 0.667745 \tEpoch training accuracy: 58.23%                                       ]                      \n",
      "Time taken for this epoch: 388.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 57.88%,                 Avg loss: 0.675508 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Epoch training loss: 0.664908 \tEpoch training accuracy: 58.92%                                      5 ]                     \n",
      "Time taken for this epoch: 436.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 60.00%,                 Avg loss: 0.673758 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.668333 \tEpoch training accuracy: 58.51%                                       ]                      \n",
      "Time taken for this epoch: 646.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.25%,                 Avg loss: 0.658355 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Epoch training loss: 0.667418 \tEpoch training accuracy: 58.52%                                      ]                       \n",
      "Time taken for this epoch: 578.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.25%,                 Avg loss: 0.662314 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Epoch training loss: 0.665559 \tEpoch training accuracy: 58.42%                                       ]                      \n",
      "Time taken for this epoch: 454.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.75%,                 Avg loss: 0.661105 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Epoch training loss: 0.665635 \tEpoch training accuracy: 58.70%                                       ]                      \n",
      "Time taken for this epoch: 540.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.21%,                 Avg loss: 0.669007 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Epoch training loss: 0.665473 \tEpoch training accuracy: 58.71%                                       ]                      \n",
      "Time taken for this epoch: 491.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.13%,                 Avg loss: 0.660300 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Epoch training loss: 0.668775 \tEpoch training accuracy: 58.60%                                       ]                      \n",
      "Time taken for this epoch: 529.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.25%,                 Avg loss: 0.669809 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Epoch training loss: 0.664255 \tEpoch training accuracy: 58.46%                                       ]                      \n",
      "Time taken for this epoch: 415.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.54%,                 Avg loss: 0.661945 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Epoch training loss: 0.666197 \tEpoch training accuracy: 58.90%                                      5 ]                     \n",
      "Time taken for this epoch: 365.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.46%,                 Avg loss: 0.657878 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Epoch training loss: 0.663242 \tEpoch training accuracy: 59.04%                                       ]                      \n",
      "Time taken for this epoch: 388.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.67%,                 Avg loss: 0.662579 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Epoch training loss: 0.663120 \tEpoch training accuracy: 59.29%                                      5 ]                     \n",
      "Time taken for this epoch: 429.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.58%,                 Avg loss: 0.656762 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Epoch training loss: 0.662653 \tEpoch training accuracy: 59.27%                                      5 ]                     \n",
      "Time taken for this epoch: 444.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.58%,                 Avg loss: 0.662467 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Epoch training loss: 0.664988 \tEpoch training accuracy: 59.02%                                       ]                      \n",
      "Time taken for this epoch: 434.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 60.21%,                 Avg loss: 0.658484 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Epoch training loss: 0.662835 \tEpoch training accuracy: 59.41%                                      5 ]                     \n",
      "Time taken for this epoch: 406.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 60.00%,                 Avg loss: 0.655506 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Epoch training loss: 0.662418 \tEpoch training accuracy: 59.19%                                       ]                      \n",
      "Time taken for this epoch: 414.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 58.67%,                 Avg loss: 0.674134 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Epoch training loss: 0.665026 \tEpoch training accuracy: 59.19%                                       ]                      \n",
      "Time taken for this epoch: 372.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.54%,                 Avg loss: 0.657128 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Epoch training loss: 0.662952 \tEpoch training accuracy: 59.34%                                      5 ]                     \n",
      "Time taken for this epoch: 394.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.25%,                 Avg loss: 0.658841 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Epoch training loss: 0.662991 \tEpoch training accuracy: 59.26%                                       ]                      \n",
      "Time taken for this epoch: 384.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.83%,                 Avg loss: 0.653607 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "Epoch training loss: 0.660078 \tEpoch training accuracy: 59.55%                                       ]                      \n",
      "Time taken for this epoch: 417.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 58.79%,                 Avg loss: 0.668617 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Epoch training loss: 0.661784 \tEpoch training accuracy: 59.34%                                      5 ]                     \n",
      "Time taken for this epoch: 363.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.42%,                 Avg loss: 0.678196 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Epoch training loss: 0.661528 \tEpoch training accuracy: 59.26%                                       ]                      \n",
      "Time taken for this epoch: 352.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.83%,                 Avg loss: 0.659188 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Epoch training loss: 0.660859 \tEpoch training accuracy: 59.33%                                       ]                      \n",
      "Time taken for this epoch: 360.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.92%,                 Avg loss: 0.653250 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Epoch training loss: 0.659847 \tEpoch training accuracy: 59.34%                                      5 ]                     \n",
      "Time taken for this epoch: 365.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 60.04%,                 Avg loss: 0.658169 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Epoch training loss: 0.660987 \tEpoch training accuracy: 59.68%                                       ]                      \n",
      "Time taken for this epoch: 373.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 60.25%,                 Avg loss: 0.654814 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Epoch training loss: 0.660610 \tEpoch training accuracy: 59.64%                                       ]                      \n",
      "Time taken for this epoch: 330.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 60.42%,                 Avg loss: 0.657615 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Epoch training loss: 0.663679 \tEpoch training accuracy: 58.99%                                      5 ]                     \n",
      "Time taken for this epoch: 314.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 60.58%,                 Avg loss: 0.652442 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Epoch training loss: 0.661156 \tEpoch training accuracy: 59.08%                                      5 ]                     \n",
      "Time taken for this epoch: 312.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.96%,                 Avg loss: 0.653890 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Epoch training loss: 0.659100 \tEpoch training accuracy: 59.20%                                      5 ]                     \n",
      "Time taken for this epoch: 311.00s\n",
      "Learning rate value: 0.00100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " Accuracy: 59.83%,                 Avg loss: 0.654446 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Epoch training loss: 0.661186 \tEpoch training accuracy: 59.02%                                       ]                      \n",
      "Time taken for this epoch: 315.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.75%,                 Avg loss: 0.655528 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Epoch training loss: 0.660574 \tEpoch training accuracy: 59.28%                                       ]                      \n",
      "Time taken for this epoch: 319.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.83%,                 Avg loss: 0.660269 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Epoch training loss: 0.660354 \tEpoch training accuracy: 59.27%                                      5 ]                     \n",
      "Time taken for this epoch: 327.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 60.25%,                 Avg loss: 0.649283 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Epoch training loss: 0.659482 \tEpoch training accuracy: 59.65%                                      5 ]                     \n",
      "Time taken for this epoch: 347.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.38%,                 Avg loss: 0.662228 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Epoch training loss: 0.659127 \tEpoch training accuracy: 59.84%                                       ]                      \n",
      "Time taken for this epoch: 340.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 58.21%,                 Avg loss: 0.667839 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Epoch training loss: 0.660354 \tEpoch training accuracy: 59.39%                                       ]                      \n",
      "Time taken for this epoch: 344.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 60.33%,                 Avg loss: 0.650261 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Epoch training loss: 0.657931 \tEpoch training accuracy: 59.56%                                       ]                      \n",
      "Time taken for this epoch: 347.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 60.54%,                 Avg loss: 0.648220 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Epoch training loss: 0.656430 \tEpoch training accuracy: 59.80%                                       ]                      \n",
      "Time taken for this epoch: 346.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.88%,                 Avg loss: 0.659043 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Epoch training loss: 0.655949 \tEpoch training accuracy: 59.73%                                       ]                      \n",
      "Time taken for this epoch: 344.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 58.00%,                 Avg loss: 0.664304 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Epoch training loss: 0.653922 \tEpoch training accuracy: 60.47%                                      5 ]                     \n",
      "Time taken for this epoch: 344.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 61.21%,                 Avg loss: 0.653407 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Epoch training loss: 0.653492 \tEpoch training accuracy: 60.21%                                       ]                      \n",
      "Time taken for this epoch: 343.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 58.29%,                 Avg loss: 0.664197 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Epoch training loss: 0.650254 \tEpoch training accuracy: 60.85%                                       ]                      \n",
      "Time taken for this epoch: 345.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 59.25%,                 Avg loss: 0.653880 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Epoch training loss: 0.641147 \tEpoch training accuracy: 62.52%                                       ]                      \n",
      "Time taken for this epoch: 348.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 54.08%,                 Avg loss: 0.683696 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Epoch training loss: 0.636155 \tEpoch training accuracy: 63.33%                                      5 ]                     \n",
      "Time taken for this epoch: 348.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 64.08%,                 Avg loss: 0.627524 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Epoch training loss: 0.638987 \tEpoch training accuracy: 63.10%                                       ]                      \n",
      "Time taken for this epoch: 358.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 66.38%,                 Avg loss: 0.609057 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Epoch training loss: 0.638344 \tEpoch training accuracy: 63.25%                                       ]                      \n",
      "Time taken for this epoch: 378.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 67.04%,                 Avg loss: 0.609039 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Epoch training loss: 0.635052 \tEpoch training accuracy: 64.58%                                       ]                     \n",
      "Time taken for this epoch: 427.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 56.62%,                 Avg loss: 0.732331 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Epoch training loss: 0.624637 \tEpoch training accuracy: 64.72%                                       ]                     \n",
      "Time taken for this epoch: 358.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 66.38%,                 Avg loss: 0.602561 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Epoch training loss: 0.626245 \tEpoch training accuracy: 65.04%                                       ]                     \n",
      "Time taken for this epoch: 343.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 66.71%,                 Avg loss: 0.603835 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Epoch training loss: 0.620249 \tEpoch training accuracy: 65.02%                                      ]                      \n",
      "Time taken for this epoch: 352.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 66.54%,                 Avg loss: 0.610059 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Epoch training loss: 0.620406 \tEpoch training accuracy: 64.97%                                      ]                      \n",
      "Time taken for this epoch: 342.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 65.50%,                 Avg loss: 0.613364 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Epoch training loss: 0.632307 \tEpoch training accuracy: 63.99%                                       ]                      \n",
      "Time taken for this epoch: 346.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 65.88%,                 Avg loss: 0.598751 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Epoch training loss: 0.641065 \tEpoch training accuracy: 63.22%                                      ]                       \n",
      "Time taken for this epoch: 330.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 61.96%,                 Avg loss: 0.629784 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Epoch training loss: 0.618004 \tEpoch training accuracy: 65.64%                                       ]                     \n",
      "Time taken for this epoch: 332.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 66.92%,                 Avg loss: 0.592026 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Epoch training loss: 0.618920 \tEpoch training accuracy: 64.70%                                       ]                      \n",
      "Time taken for this epoch: 330.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 64.92%,                 Avg loss: 0.621938 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Epoch training loss: 0.620611 \tEpoch training accuracy: 65.39%                                       ]                     \n",
      "Time taken for this epoch: 331.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 64.79%,                 Avg loss: 0.611817 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch training loss: 0.627036 \tEpoch training accuracy: 64.56%                                      ]                       \n",
      "Time taken for this epoch: 330.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 64.50%,                 Avg loss: 0.632882 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Epoch training loss: 0.611705 \tEpoch training accuracy: 66.30%                                       ]                     \n",
      "Time taken for this epoch: 337.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 60.00%,                 Avg loss: 0.748786 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Epoch training loss: 0.627900 \tEpoch training accuracy: 64.89%                                      ]                       \n",
      "Time taken for this epoch: 341.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 60.83%,                 Avg loss: 0.678698 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Epoch training loss: 0.618211 \tEpoch training accuracy: 65.60%                                                             \n",
      "Time taken for this epoch: 351.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 64.75%,                 Avg loss: 0.618750 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Epoch training loss: 0.621402 \tEpoch training accuracy: 64.88%                                       ]                     \n",
      "Time taken for this epoch: 354.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 64.46%,                 Avg loss: 0.616640 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Epoch training loss: 0.610909 \tEpoch training accuracy: 66.74%                                       ]                     \n",
      "Time taken for this epoch: 351.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 65.21%,                 Avg loss: 0.618317 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Epoch training loss: 0.616418 \tEpoch training accuracy: 65.17%                                       ]                     \n",
      "Time taken for this epoch: 354.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 57.75%,                 Avg loss: 0.698833 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "Epoch training loss: 0.611892 \tEpoch training accuracy: 66.20%                                      ]                      \n",
      "Time taken for this epoch: 347.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 63.62%,                 Avg loss: 0.635310 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Epoch training loss: 0.626625 \tEpoch training accuracy: 65.12%                                       ]                     \n",
      "Time taken for this epoch: 350.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 58.67%,                 Avg loss: 0.670892 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Epoch training loss: 0.614465 \tEpoch training accuracy: 65.83%                                       ]                     \n",
      "Time taken for this epoch: 350.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 67.08%,                 Avg loss: 0.602344 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Epoch training loss: 0.615681 \tEpoch training accuracy: 65.66%                                      ]                      \n",
      "Time taken for this epoch: 353.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 64.04%,                 Avg loss: 0.643129 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Epoch training loss: 0.620783 \tEpoch training accuracy: 64.74%                                       ]                     \n",
      "Time taken for this epoch: 353.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 67.75%,                 Avg loss: 0.587867 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Epoch training loss: 0.624960 \tEpoch training accuracy: 64.61%                                       ]                      \n",
      "Time taken for this epoch: 356.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 65.88%,                 Avg loss: 0.600970 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Epoch training loss: 0.619189 \tEpoch training accuracy: 65.49%                                       ]                     \n",
      "Time taken for this epoch: 372.00s\n",
      "Learning rate value: 0.00100000\n",
      "Validation results: \n",
      " Accuracy: 66.08%,                 Avg loss: 0.616477 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Batch training loss:  0.6084722236755791  \tBatch training accuracy:  66.72962763086886  \t[ 436 / 565 ]                     \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-7bc7d1d215c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    102\u001b[0m             optimizers_param={\"lr\": config_model.learning_rate,\n\u001b[1;32m    103\u001b[0m              \"weight_decay\": config_model.weight_decay},\n\u001b[0;32m--> 104\u001b[0;31m             store_grad_layer_hist=False)\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;31m# %%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# Hyperparameter search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/giotto-deep/gdeep/pipeline/pipeline.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, optimizer, n_epochs, cross_validation, optimizers_param, dataloaders_param, lr_scheduler, scheduler_params, optuna_params, profiling, parallel_tpu, keep_training, store_grad_layer_hist, n_accumulated_grads, writer_tag)\u001b[0m\n\u001b[1;32m    662\u001b[0m                                                    \u001b[0mdl_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m                                                    \u001b[0mprof\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_optuna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_metric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m                                                    trial, 0, writer_tag)\n\u001b[0m\u001b[1;32m    665\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                 valloss, valacc = self.parallel_tpu_training_loops(n_epochs, dl_tr,\n",
      "\u001b[0;32m~/Desktop/giotto-deep/gdeep/pipeline/pipeline.py\u001b[0m in \u001b[0;36m_training_loops\u001b[0;34m(self, n_epochs, dl_tr, dl_val, lr_scheduler, scheduler, prof, check_optuna, search_metric, trial, cross_validation, writer_tag)\u001b[0m\n\u001b[1;32m    730\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_tag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mme\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_grad_layer_hist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/giotto-deep/gdeep/pipeline/pipeline.py\u001b[0m in \u001b[0;36m_train_loop\u001b[0;34m(self, dl_tr, writer_tag)\u001b[0m\n\u001b[1;32m    263\u001b[0m                                                  \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                                                  \u001b[0mt_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                                                  correct)\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch training loss: {t_loss:>8f} \\tEpoch training accuracy: {(correct*100):.2f}% \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mljust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/giotto-deep/gdeep/pipeline/pipeline.py\u001b[0m in \u001b[0;36m_inner_train_loop\u001b[0;34m(self, dl_tr, writer_tag, size, steps, loss, t_loss, correct)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             t_loss = self._optimisation_step(dl_tr, steps, loss, \n\u001b[0;32m--> 232\u001b[0;31m                                     t_loss, correct, batch, closure)\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;31m# accuracy:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/giotto-deep/gdeep/pipeline/pipeline.py\u001b[0m in \u001b[0;36m_optimisation_step\u001b[0;34m(self, dl_tr, steps, loss, t_loss, correct, batch, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_accumulated_grads\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# usual case for stochastic gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"xla\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0mxm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbarrier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Note: Cloud TPU-specific code!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Balance labels in dataset\n",
    "\n",
    "if config_data.balance_dataset:\n",
    "    x_pds, y = balance_binary_dataset(x_pds, y, verbose=True)\n",
    "\n",
    "print('class balance: {:.2f}'.format((y.sum() / y.shape[0]).item()))\n",
    "# %%\n",
    "# Set up dataset and dataloader\n",
    "\n",
    "# create the datasets\n",
    "graph_ds = TensorDataset(x_pds, y)\n",
    "\n",
    "# Either use fixed train and validation split or use cross validation\n",
    "if hyperparameters_dicts.cross_validation:\n",
    "    graph_dl = DataLoader(\n",
    "                        graph_ds,\n",
    "                        num_workers=config_data.num_jobs,\n",
    "                        batch_size=config_data.batch_size_train,\n",
    "                        shuffle=True\n",
    "                        )\n",
    "else:\n",
    "    # Split the dataset into training and validation\n",
    "    total_size = x_pds.shape[0]\n",
    "    train_size = int(total_size * config_data.train_percentage)\n",
    "    graph_ds_train, graph_ds_val = torch.utils.data.random_split(\n",
    "                                                        graph_ds,\n",
    "                                                        [train_size,\n",
    "                                                        total_size - train_size],\n",
    "                                                        generator=torch.Generator().manual_seed(config_data.data_split_seed))\n",
    "\n",
    "\n",
    "    # Define data loaders\n",
    "    graph_dl_train = DataLoader(\n",
    "        graph_ds_train,\n",
    "        num_workers=config_data.num_jobs,\n",
    "        batch_size=config_data.batch_size_train,\n",
    "        shuffle=True\n",
    "        )\n",
    "\n",
    "    graph_dl_val = DataLoader(\n",
    "        graph_ds_val,\n",
    "        num_workers=config_data.num_jobs,\n",
    "        batch_size=config_data.batch_size_val,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Compute balance of train and validation datasets\n",
    "\n",
    "    print_class_balance(graph_dl_train, 'train')\n",
    "    print_class_balance(graph_dl_val, 'validation')\n",
    "\n",
    "# %%\n",
    "# Define and initialize the model\n",
    "model = Persformer.from_config(config_model, config_data)\n",
    "\n",
    "\n",
    "# %%\n",
    "# Do training and validation\n",
    "\n",
    "# initialize loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the Tensorflow writer\n",
    "writer = GiottoSummaryWriter(\n",
    "            os.path.join(\"runs\",\n",
    "                        config_model.implementation +\n",
    "                        \"_\" + config_data.dataset_name +\n",
    "                        \"_\" + models_hyperparams.attention_type[0] +\n",
    "                        \"_\" + \"hyperparameter_search_giotto\")\n",
    "            )\n",
    "\n",
    "# initialize pipeline object\n",
    "if hyperparameters_dicts.cross_validation:\n",
    "    pipe = Pipeline(model, [graph_dl, None], loss_fn, writer)\n",
    "else:\n",
    "    pipe = Pipeline(model, [graph_dl_train, graph_dl_val, None], loss_fn, writer)\n",
    "\n",
    "# Use gradient clipping\n",
    "if config_model.gradient_clipping == None:\n",
    "    pipe.clip = 1.0  # use default clipping value 1.0\n",
    "else:\n",
    "    pipe.clip = config_model.gradient_clipping\n",
    "# %%\n",
    "\n",
    "\n",
    "# train the model\n",
    "\"\"\" pipe.train(config_model.optimizer,\n",
    "           config_model.num_epochs,\n",
    "           cross_validation=False,\n",
    "           optimizers_param={\"lr\": config_model.learning_rate,\n",
    "            \"weight_decay\": config_model.weight_decay},\n",
    "           n_accumulated_grads=config_model.n_accumulated_grads,\n",
    "           lr_scheduler=get_cosine_schedule_with_warmup,  #get_constant_schedule_with_warmup,  #get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "           scheduler_params = {\"num_warmup_steps\": int(config_model.warmup * config_model.num_epochs),\n",
    "                               \"num_training_steps\": config_model.num_epochs,},\n",
    "                               #\"num_cycles\": 1},\n",
    "           store_grad_layer_hist=False) \"\"\"\n",
    "\n",
    "pipe.train(eval(config_model.optimizer),\n",
    "            config_model.num_epochs,\n",
    "            cross_validation=False,\n",
    "            optimizers_param={\"lr\": config_model.learning_rate,\n",
    "             \"weight_decay\": config_model.weight_decay},\n",
    "            store_grad_layer_hist=False)\n",
    "# %%\n",
    "# Hyperparameter search\n",
    "\n",
    "#pruner = NopPruner()\n",
    "#search = Gridsearch(pipe,\n",
    "#                    search_metric=\"accuracy\",\n",
    "#                    n_trials=hyperparameters_dicts.n_trials,\n",
    "#                    best_not_mean=False,\n",
    "#                    pruner=pruner)\n",
    "\n",
    "#dictionaries of hyperparameters\n",
    "# optimizers_params = {\"lr\": [1e-3, 1e-0, None, True],\n",
    "#                       \"weight_decay\": [0.0001, 0.2, None, True] }\n",
    "# dataloaders_params = {\"batch_size\": [8, 32, 2]}\n",
    "# models_hyperparams = {\"n_layer_enc\": [2, 4, 1], #(int) - The number of layers in the encoder\n",
    "#                       \"n_layer_dec\": [1, 5, 1], #(int) - The number of layers in the encoder\n",
    "#                       \"num_heads\": [\"2\", \"4\", \"8\"], #(int) - The number of heads in the encoder\n",
    "#                       \"hidden_dim\": [\"16\", \"32\", \"64\", \"96\", \"128\"], #(int) - The number of hidden dimensions in the encoder\n",
    "#                       \"dropout_enc\": [0.0, 0.5, 0.05],\n",
    "#                       \"dropout_dec\": [0.0, 0.5, 0.05], \n",
    "#                       \"layer_norm\": [\"True\", \"False\"],\n",
    "#                       \"pre_layer_norm\": [\"True\", \"False\"],\n",
    "#                       \"bias_attention\": [\"True\", \"False\"],\n",
    "#                       \"input_dim\": [config_model[\"input_dim\"]],\n",
    "#                       \"pooling_type\": [\"pytorch_self_attention_skip\"],\n",
    "#                       \"layer_norm_pooling\": [\"True\", \"False\"],\n",
    "#                       \"activation\": [\"gelu\",]\n",
    "#                       }\n",
    "\n",
    "# schedulers_params = {\"num_warmup_steps\": [int(0.02 * config_model.num_epochs)],  #(int) â The number of steps for the warmup phase.\n",
    "#                     \"num_training_steps\": [config_model.num_epochs], #(int) â The total number of training steps.\n",
    "#                     \"num_cycles\": [1]} #(int) â The number of restart cycles\n",
    "#%%\n",
    "# starting the gridsearch\n",
    "#search.start((eval(config_model.optimizer),),\n",
    "#            n_epochs=schedulers_params.num_training_steps[0],\n",
    "#            cross_validation=hyperparameters_dicts.cross_validation,\n",
    "#            k_folds=hyperparameters_dicts.k_folds,\n",
    "#            optimizers_params=optimizers_params,\n",
    "#            dataloaders_params=dataloaders_params,\n",
    "#            models_hyperparams=models_hyperparams, lr_scheduler=get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "#            schedulers_params=schedulers_params)\n",
    "\n",
    "# %%\n",
    "# from gdeep.visualisation import plotly2tensor\n",
    "# from plotly.io import write_image\n",
    "# import plotly.express as px\n",
    "# df = px.data.iris()\n",
    "\n",
    "# fig = px.scatter(\n",
    "#     df, x=\"sepal_width\", y=\"sepal_length\", color=\"species\"\n",
    "# )\n",
    "# write_image(fig, \"deleteme.jpeg\", format=\"jpeg\", engine=\"orca\")\n",
    "# fig.show()\n",
    "# plotly2tensor(fig)\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-067bfe8b4834>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "next(iter(dataloader))[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(next(iter(graph_dl))[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(graph_dl))[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
