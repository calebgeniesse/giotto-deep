{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np  # type: ignore\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader\n","import multiprocessing\n","import os\n","from einops import rearrange  # type: ignore\n","from gtda.homology import WeakAlphaPersistence  # type: ignore\n","from gtda.plotting import plot_diagram  # type: ignore\n","from gdeep.create_data import generate_orbit\n","from gdeep.topology_layers import SmallSetTransformer\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["try:\n","    assert os.path.isdir('./data/ORBIT5K')\n","except AssertionError:\n","    if not os.path.isdir('./data'):\n","        os.mkdir('./data')\n","    os.mkdir('./data/ORBIT5K')\n","\n","# If `use_precomputed_dgms` is `False` the ORBIT5K dataset will\n","# be recomputed, otherwise the ORBIT5K dataset in the folder\n","# `data/ORBIT5K` will be used\n","use_precomputed_dgms = True\n","\n","dgms_filename = os.path.join('data', 'ORBIT5K',\n","                             'alpha_persistence_diagrams.npy')\n","\n","if use_precomputed_dgms:\n","    try:\n","        assert(os.path.isfile(dgms_filename))\n","    except AssertionError:\n","        print('File data/ORBIT5K/alpha_persistence_diagrams.npy',\n","              ' does not exist.')\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","parameters = (2.5, 3.5, 4.0, 4.1, 4.3)  # different classes of orbits\n","homology_dimensions = (0, 1)\n","\n","config = {\n","    'parameters': parameters,\n","    'num_classes': len(parameters),\n","    'num_orbits': 1000,\n","    'num_pts_per_orbit': 1000,\n","    'homology_dimensions': homology_dimensions,\n","    'num_homology_dimensions': len(homology_dimensions)\n","}\n","\n","if not use_precomputed_dgms:\n","    # Generate dataset consisting of 5 different orbit types with\n","    # 1000 sampled data points each.\n","    # This is the dataset ORBIT5K used in the PersLay paper\n","\n","    x = np.zeros((\n","                    config['num_classes'],  # type: ignore\n","                    config['num_orbits'],\n","                    config['num_pts_per_orbit'],\n","                    2\n","                ))\n","\n","    # generate dataset\n","    for cidx, p in enumerate(config['parameters']):  # type: ignore\n","        for _ in range(config['num_orbits']):  # type: ignore\n","            x[cidx] = generate_orbit(\n","                num_pts_per_orbit=config['num_pts_per_orbit'],  # type: ignore\n","                parameter=p\n","                )\n","\n","    # compute weak alpha persistence\n","    wap = WeakAlphaPersistence(\n","                        homology_dimensions=config['homology_dimensions'],\n","                        n_jobs=multiprocessing.cpu_count()\n","                        )\n","    # c: class, o: orbit, p: point, d: dimension\n","    x_stack = rearrange(x, 'c o p d -> (c o) p d')  # stack classes\n","    diagrams = wap.fit_transform(x_stack)\n","    # shape: (num_classes * n_samples, n_features, 3)\n","\n","    diagrams = rearrange(\n","                            diagrams,\n","                            '(c o) p d -> c o p d',\n","                            c=config['num_classes']  # type: ignore\n","                        )\n","\n","    # plot sample persistence diagrams\n","    if(False):\n","        plot_diagram(diagrams[1, 2])\n","        plot_diagram(diagrams[2, 2])\n","\n","    # save dataset\n","    with open(dgms_filename, 'wb') as f:\n","        np.save(f, diagrams)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load dataset\n","with open(dgms_filename, 'rb') as f:\n","    x = np.load(f)\n","\n","# c: class, o: orbit, p: point in persistence diagram,\n","# d: coordinates + homology dimension\n","x = rearrange(\n","                x,\n","                'c o p d -> (c o) p d',\n","                c=config['num_classes']  # type: ignore\n","            )\n","# convert homology dimension to one-hot encoding\n","x = np.concatenate(\n","    (\n","        x[:, :, :2],\n","        (np.eye(config['num_homology_dimensions'])\n","         [x[:, :, -1].astype(np.int32)]),\n","    ),\n","    axis=-1)\n","# convert from [orbit, sequence_length, feature] to\n","# [orbit, feature, sequence_length] to fit to the\n","# input_shape of `SmallSetTransformer`\n","#x = rearrange(x, 'o s f -> o f s')\n","\n","# generate labels\n","y_list = []\n","for i in range(config['num_classes']):  # type: ignore\n","    y_list += [i] * config['num_orbits']  # type: ignore\n","\n","y = np.array(y_list)\n","\n","\n","# load dataset to PyTorch dataloader\n","\n","x_tensor = torch.Tensor(x)\n","y_tensor = torch.Tensor(y)\n","\n","dataset = TensorDataset(x_tensor, y_tensor)\n","dataloader = DataLoader(dataset,\n","                        shuffle=True,\n","                        batch_size=2 ** 6,\n","                        num_workers=6)\n","\n","# initialize SmallSetTransformer model\n","model = SmallSetTransformer(\n","                            dim_in=4,\n","                            dim_out=16,\n","                            num_heads=4,\n","                            out_features=config['num_classes']  # type: ignore\n","                            )\n","\n","print('model has', model.num_params(), 'trainable parameters.')\n","\n","# CUDA for PyTorch\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n","\n","\n","def train(model, num_epochs: int = 10, lr: float = 1e-4,\n","          verbose: bool = False):\n","    model = model.to_device(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    criterion = nn.CrossEntropyLoss()\n","    losses = []\n","\n","    # training loop\n","    for epoch in range(num_epochs):\n","        loss_per_epoch = 0\n","        for x_batch, y_batch in dataloader:\n","            # transfer to GPU\n","            x_batch = x_batch.to_device(device)\n","            y_batch = y_batch.to_device(device)\n","            loss = criterion(x_batch, y_batch)\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            loss_per_epoch += loss\n","        losses.append(loss_per_epoch)\n","        if verbose:\n","            print(\"epoch:\", epoch, \"loss:\", loss_per_epoch)\n","\n","    return losses"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#model(next(iter(dataloader))[0])\n","z = next(iter(dataloader))[0]\n","model(z)"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}