{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import numpy as np  # type: ignore\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import multiprocessing\n",
    "import os\n",
    "from einops import rearrange  # type: ignore\n",
    "from gtda.homology import WeakAlphaPersistence  # type: ignore\n",
    "from gtda.plotting import plot_diagram  # type: ignore\n",
    "from gdeep.topology_layers import ISAB, PMA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert os.path.isdir('./data/ORBIT5K')\n",
    "except AssertionError:\n",
    "    if not os.path.isdir('./data'):\n",
    "        os.mkdir('./data')\n",
    "    os.mkdir('./data/ORBIT5K')\n",
    "\n",
    "# If `use_precomputed_dgms` is `False` the ORBIT5K dataset will\n",
    "# be recomputed, otherwise the ORBIT5K dataset in the folder\n",
    "# `data/ORBIT5K` will be used\n",
    "use_precomputed_dgms = True\n",
    "\n",
    "dgms_filename = os.path.join('data', 'ORBIT5K',\n",
    "                             'alpha_persistence_diagrams.npy')\n",
    "dgms_filename_validation = os.path.join('data', 'ORBIT5K',\n",
    "                                        'alpha_persistence_diagrams_' +\n",
    "                                        'validation.npy')\n",
    "\n",
    "if use_precomputed_dgms:\n",
    "    try:\n",
    "        assert(os.path.isfile(dgms_filename))\n",
    "    except AssertionError:\n",
    "        print('File data/ORBIT5K/alpha_persistence_diagrams.npy',\n",
    "              ' does not exist.')\n",
    "    try:\n",
    "        assert(os.path.isfile(dgms_filename_validation))\n",
    "    except AssertionError:\n",
    "        print('File data/ORBIT5K/alpha_persistence_diagrams.npy',\n",
    "              ' does not exist.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ORBIT5K dataset like in the PersLay paper\n",
    "parameters = (2.5, 3.5, 4.0, 4.1, 4.3)  # different classes of orbits\n",
    "homology_dimensions = (0, 1)\n",
    "\n",
    "config = {\n",
    "    'parameters': parameters,\n",
    "    'num_classes': len(parameters),\n",
    "    'num_orbits': 1_000,  # number of orbits per class\n",
    "    'num_pts_per_orbit': 1_000,\n",
    "    'homology_dimensions': homology_dimensions,\n",
    "    'num_homology_dimensions': len(homology_dimensions),\n",
    "    'validation_percentage': 100,  # size of validation dataset relative\n",
    "    # to training\n",
    "}\n",
    "\n",
    "if not use_precomputed_dgms:\n",
    "    for dataset_type in ['train', 'validation']:\n",
    "        # Generate dataset consisting of 5 different orbit types with\n",
    "        # 1000 sampled data points each.\n",
    "        # This is the dataset ORBIT5K used in the PersLay paper\n",
    "        if dataset_type == 'train':\n",
    "            num_orbits = config['num_orbits']\n",
    "        else:\n",
    "            num_orbits = int(config['num_orbits']  # type: ignore\n",
    "                             * config['validation_percentage'] / 100)\n",
    "        x = np.zeros((\n",
    "                        config['num_classes'],  # type: ignore\n",
    "                        num_orbits,\n",
    "                        config['num_pts_per_orbit'],\n",
    "                        2\n",
    "                    ))\n",
    "\n",
    "        # generate dataset\n",
    "        for cidx, p in enumerate(config['parameters']):  # type: ignore\n",
    "            x[cidx, :, 0, :] = np.random.rand(num_orbits, 2)\n",
    "\n",
    "            for i in range(1, config['num_pts_per_orbit']):  # type: ignore\n",
    "                x_cur = x[cidx, :, i - 1, 0]\n",
    "                y_cur = x[cidx, :, i - 1, 1]\n",
    "\n",
    "                x[cidx, :, i, 0] = (x_cur + p * y_cur * (1. - y_cur)) % 1\n",
    "                x_next = x[cidx, :, i, 0]\n",
    "                x[cidx, :, i, 1] = (y_cur + p * x_next * (1. - x_next)) % 1\n",
    "\n",
    "        \"\"\"\n",
    "        # old non-parallel version\n",
    "        for cidx, p in enumerate(config['parameters']):  # type: ignore\n",
    "            for i in range(config['num_orbits']):  # type: ignore\n",
    "                x[cidx][i] = generate_orbit(\n",
    "                    num_pts_per_orbit=config['num_pts_per_orbit'],\n",
    "                    parameter=p\n",
    "                    )\n",
    "        \"\"\"\n",
    "\n",
    "        assert(not np.allclose(x[0, 0], x[0, 1]))\n",
    "\n",
    "        # compute weak alpha persistence\n",
    "        wap = WeakAlphaPersistence(\n",
    "                            homology_dimensions=config['homology_dimensions'],\n",
    "                            n_jobs=multiprocessing.cpu_count()\n",
    "                            )\n",
    "        # c: class, o: orbit, p: point, d: dimension\n",
    "        x_stack = rearrange(x, 'c o p d -> (c o) p d')  # stack classes\n",
    "        diagrams = wap.fit_transform(x_stack)\n",
    "        # shape: (num_classes * n_samples, n_features, 3)\n",
    "\n",
    "        # combine class and orbit dimensions\n",
    "        diagrams = rearrange(\n",
    "                                diagrams,\n",
    "                                '(c o) p d -> c o p d',\n",
    "                                c=config['num_classes']  # type: ignore\n",
    "                            )\n",
    "\n",
    "        # plot sample persistence diagrams for debugging\n",
    "        if(False):\n",
    "            plot_diagram(diagrams[1, 2])\n",
    "            plot_diagram(diagrams[2, 2])\n",
    "\n",
    "        # save dataset\n",
    "        if dataset_type == 'train':\n",
    "            with open(dgms_filename, 'wb') as f:\n",
    "                np.save(f, diagrams)\n",
    "        else:\n",
    "            with open(dgms_filename_validation, 'wb') as f:\n",
    "                np.save(f, diagrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "for dataset_type in ['train', 'validation']:\n",
    "\n",
    "    with open(dgms_filename, 'rb') as f:\n",
    "        x = np.load(f)\n",
    "\n",
    "    # c: class, o: orbit, p: point in persistence diagram,\n",
    "    # d: coordinates + homology dimension\n",
    "    x = rearrange(\n",
    "                    x,\n",
    "                    'c o p d -> (c o) p d',\n",
    "                    c=config['num_classes']  # type: ignore\n",
    "                )\n",
    "    # convert homology dimension to one-hot encoding\n",
    "    x = np.concatenate(\n",
    "        (\n",
    "            x[:, :, :2],\n",
    "            (np.eye(config['num_homology_dimensions'])\n",
    "             [x[:, :, -1].astype(np.int32)]),\n",
    "        ),\n",
    "        axis=-1)\n",
    "    # convert from [orbit, sequence_length, feature] to\n",
    "    # [orbit, feature, sequence_length] to fit to the\n",
    "    # input_shape of `SmallSetTransformer`\n",
    "    # x = rearrange(x, 'o s f -> o f s')\n",
    "\n",
    "    # generate labels\n",
    "    y_list = []\n",
    "    for i in range(config['num_classes']):  # type: ignore\n",
    "        y_list += [i] * config['num_orbits']  # type: ignore\n",
    "\n",
    "    y = np.array(y_list)\n",
    "\n",
    "    # load dataset to PyTorch dataloader\n",
    "\n",
    "    x_tensor = torch.Tensor(x)\n",
    "    y_tensor = torch.Tensor(y)\n",
    "\n",
    "    dataset = TensorDataset(x_tensor, y_tensor)\n",
    "    if dataset_type == 'train':\n",
    "        dataloader = DataLoader(dataset,\n",
    "                                shuffle=True,\n",
    "                                batch_size=2 ** 6,\n",
    "                                num_workers=6)\n",
    "    else:\n",
    "        dataloader_validation = DataLoader(dataset,\n",
    "                                           batch_size=2 ** 6,\n",
    "                                           num_workers=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has 291589 trainable parameters.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# initialize SetTransformer model\n",
    "class SetTransformer(nn.Module):\n",
    "    \"\"\" Vanilla SetTransformer from\n",
    "    https://github.com/juho-lee/set_transformer/blob/master/main_pointcloud.py\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_input=3,  # dimension of input data for each element in the set\n",
    "        num_outputs=1,\n",
    "        dim_output=40,  # number of classes\n",
    "        num_inds=32,  # number of induced points, see  Set Transformer paper\n",
    "        dim_hidden=128,\n",
    "        num_heads=4,\n",
    "        ln=False,  # use layer norm\n",
    "    ):\n",
    "        super(SetTransformer, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "            ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            PMA(dim_hidden, num_heads, num_outputs, ln=ln),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(dim_hidden, dim_output),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.dec(self.enc(input)).squeeze()\n",
    "\n",
    "\n",
    "model = SetTransformer(dim_input=4, dim_output=5)\n",
    "\n",
    "def num_params(model: nn.Module) -> int:\n",
    "    return sum([parameter.nelement() for parameter in model.parameters()])\n",
    "\n",
    "print('model has', num_params(model), 'trainable parameters.')  # type: ignore\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, num_epochs: int = 10, lr: float = 1e-3,\n",
    "          verbose: bool = False) -> List[float]:\n",
    "    \"\"\"Custom training loop for Set Transformer on the dataset ``\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Set Transformer model to be trained\n",
    "        num_epochs (int, optional): Number of training epochs. Defaults to 10.\n",
    "        lr (float, optional): Learning rate for training. Defaults to 1e-3.\n",
    "        verbose (bool, optional): Print training loss, training accuracy and\n",
    "            validation if set to True. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        List[float]: List of training losses\n",
    "    \"\"\"\n",
    "    if use_cuda:\n",
    "        model = nn.DataParallel(model)\n",
    "        model = model.cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses: List[float] = []\n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        loss_per_epoch = 0\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            # transfer to GPU\n",
    "            if use_cuda:\n",
    "                x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "            loss = criterion(model(x_batch), y_batch.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_per_epoch += loss.item()\n",
    "        losses.append(loss_per_epoch)\n",
    "        if verbose:\n",
    "            print(\"epoch:\", epoch, \"loss:\", loss_per_epoch)\n",
    "            compute_accuracy(model, 'test')\n",
    "            compute_accuracy(model, 'validation')\n",
    "    return losses\n",
    "\n",
    "\n",
    "def compute_accuracy(model, type: str = 'test') -> None:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    if type == 'test':\n",
    "        dl = dataloader\n",
    "    else:\n",
    "        dl = dataloader_validation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dl:\n",
    "            if use_cuda:\n",
    "                x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "            outputs = model(x_batch).squeeze(1)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predictions == y_batch).sum().item()\n",
    "\n",
    "    print(type.capitalize(),\n",
    "          'accuracy of the network on the', total,\n",
    "          'diagrams: %8.2f %%' % (100 * correct / total)\n",
    "          )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, num_epochs=10, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:giottodeep]",
   "language": "python",
   "name": "conda-env-giottodeep-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
