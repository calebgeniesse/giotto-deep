{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython  # type: ignore\n",
    "get_ipython().magic('load_ext autoreload')\n",
    "get_ipython().magic('autoreload 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPUs...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from dotmap import DotMap\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Import the PyTorch modules\n",
    "import torch  # type: ignore\n",
    "from torch import nn  # type: ignore\n",
    "from torch.optim import SGD, Adam, RMSprop, AdamW  # type: ignore\n",
    "\n",
    "# Import Tensorflow writer\n",
    "from torch.utils.tensorboard import SummaryWriter  # type: ignore\n",
    "\n",
    "from transformers.optimization import get_cosine_with_hard_restarts_schedule_with_warmup, get_constant_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "# Import the giotto-deep modules\n",
    "from gdeep.data import OrbitsGenerator, DataLoaderKwargs\n",
    "from gdeep.topology_layers import Persformer\n",
    "from gdeep.pipeline import Pipeline\n",
    "from gdeep.search import Gridsearch\n",
    "import json\n",
    "\n",
    "from optuna.pruners import MedianPruner, NopPruner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Configs\n",
    "config_data = DotMap({\n",
    "    'batch_size_train': 32,\n",
    "    'num_orbits_per_class': 1_000,\n",
    "    'validation_percentage': 0.0,\n",
    "    'test_percentage': 0.0,\n",
    "    'num_jobs': 8,\n",
    "    'dynamical_system': 'classical_convention',\n",
    "    'homology_dimensions': (0, 1),\n",
    "    'dtype': 'float32',\n",
    "    'arbitrary_precision': False\n",
    "})\n",
    "\n",
    "\n",
    "config_model = DotMap({\n",
    "    'implementation': 'Old_SetTransformer', # SetTransformer, PersFormer,\n",
    "    # PytorchTransformer, DeepSet, X-Transformer\n",
    "    'dim_input': 2 + len(config_data.homology_dimensions) if len(config_data.homology_dimensions) > 1 else 2,\n",
    "    'num_outputs': 1,  # for classification tasks this should be 1\n",
    "    'num_classes': 5,  # number of classes\n",
    "    'dim_hidden': 64,\n",
    "    'num_heads': 4,\n",
    "    'num_induced_points': 32,\n",
    "    'layer_norm': False,  # use layer norm\n",
    "    'simplified_layer_norm': False,  #Xu, J., et al. Understanding and improving layer normalization.\n",
    "    'pre_layer_norm': False,\n",
    "    'layer_norm_pooling': False,\n",
    "    'num_layers_encoder': 2,\n",
    "    'num_layers_decoder': 3,\n",
    "    'attention_type': \"induced_attention\",\n",
    "    'activation': \"gelu\",\n",
    "    'dropout_enc': 0.0,\n",
    "    'dropout_dec': 0.0,\n",
    "    'optimizer': Adam,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_epochs': 200,\n",
    "    'pooling_type': \"attention\",\n",
    "    'weight_decay': 0.00,\n",
    "    'n_accumulated_grads': 0,\n",
    "    'bias_attention': \"True\",\n",
    "    'warmup': 0.02,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the data loader\n",
    "\n",
    "\n",
    "dataloaders_dicts = DataLoaderKwargs(train_kwargs = {\"batch_size\":\n",
    "                                                        config_data.batch_size_train,},\n",
    "                                     val_kwargs = {\"batch_size\": 4},\n",
    "                                     test_kwargs = {\"batch_size\": 3})\n",
    "\n",
    "og = OrbitsGenerator(num_orbits_per_class=config_data.num_orbits_per_class,\n",
    "                     homology_dimensions = config_data.homology_dimensions,\n",
    "                     validation_percentage=config_data.validation_percentage,\n",
    "                     test_percentage=config_data.test_percentage,\n",
    "                     n_jobs=config_data.num_jobs,\n",
    "                     dynamical_system = config_data.dynamical_system,\n",
    "                     dtype=config_data.dtype,\n",
    "                     arbitrary_precision=config_data.arbitrary_precision,\n",
    "                     )\n",
    "\n",
    "if config_data.arbitrary_precision:\n",
    "    orbits = np.load(os.path.join('data', 'orbit5k_arbitrary_precision.npy'))\n",
    "    og.orbits_from_array(orbits)\n",
    "\n",
    "if len(config_data.homology_dimensions) == 0:\n",
    "    dl_train, _, _ = og.get_dataloader_orbits(dataloaders_dicts)\n",
    "else:\n",
    "    dl_train, _, _ = og.get_dataloader_persistence_diagrams(dataloaders_dicts)\n",
    "\n",
    "\n",
    "\n",
    "# Define the model    \n",
    "if config_model.implementation == \"Old_SetTransformer\":\n",
    "    # initialize SetTransformer model\n",
    "\n",
    "    model = Persformer(dim_input=config_model.dim_input, dim_output=5,\n",
    "                           num_inds=config_model.num_induced_points,\n",
    "                           dim_hidden=config_model.dim_hidden,\n",
    "                           num_heads=str(config_model.num_heads),\n",
    "                           layer_norm=str(config_model.layer_norm),  # use layer norm\n",
    "                           pre_layer_norm=str(config_model.pre_layer_norm),\n",
    "                           simplified_layer_norm=str(config_model.simplified_layer_norm),\n",
    "                           dropout_enc=config_model.dropout_enc,\n",
    "                           dropout_dec=config_model.dropout_dec,\n",
    "                           num_layer_enc=config_model.num_layers_encoder,\n",
    "                           num_layer_dec=config_model.num_layers_decoder,\n",
    "                           activation=config_model.activation,\n",
    "                           bias_attention=config_model.bias_attention,\n",
    "                           attention_type=config_model.attention_type,\n",
    "                           layer_norm_pooling=str(config_model.layer_norm_pooling))\n",
    "else:\n",
    "    raise Exception(\"Unknown Implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use float32 model\n",
      "DotMap(implementation='Old_SetTransformer', dim_input=4, num_outputs=1, num_classes=5, dim_hidden=64, num_heads=4, num_induced_points=32, layer_norm=False, simplified_layer_norm=False, pre_layer_norm=False, layer_norm_pooling=False, num_layers_encoder=2, num_layers_decoder=3, attention_type='induced_attention', activation='gelu', dropout_enc=0.0, dropout_dec=0.0, optimizer=<class 'torch.optim.adam.Adam'>, learning_rate=0.0001, num_epochs=200, pooling_type='attention', weight_decay=0.0, n_accumulated_grads=0, bias_attention='True', warmup=0.02)\n",
      "DotMap(batch_size_train=32, num_orbits_per_class=1000, validation_percentage=0.0, test_percentage=0.0, num_jobs=8, dynamical_system='classical_convention', homology_dimensions=(0, 1), dtype='float32', arbitrary_precision=False)\n",
      "Persformer(\n",
      "  (enc): Sequential(\n",
      "    (0): ISAB(\n",
      "      (mab0): MAB(\n",
      "        (fc_q): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (fc_k): Linear(in_features=4, out_features=64, bias=True)\n",
      "        (fc_v): Linear(in_features=4, out_features=64, bias=True)\n",
      "        (fc_o): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (activation_function): GELU()\n",
      "      )\n",
      "      (mab1): MAB(\n",
      "        (fc_q): Linear(in_features=4, out_features=64, bias=True)\n",
      "        (fc_k): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (fc_v): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (fc_o): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (activation_function): GELU()\n",
      "      )\n",
      "    )\n",
      "    (1): ISAB(\n",
      "      (mab0): MAB(\n",
      "        (fc_q): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (fc_k): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (fc_v): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (fc_o): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (activation_function): GELU()\n",
      "      )\n",
      "      (mab1): MAB(\n",
      "        (fc_q): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (fc_k): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (fc_v): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (fc_o): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (activation_function): GELU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dec): Sequential(\n",
      "    (0): Dropout(p=0.0, inplace=False)\n",
      "    (1): PMA(\n",
      "      (mab): MAB(\n",
      "        (fc_q): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (fc_k): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (fc_v): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (fc_o): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (activation_function): GELU()\n",
      "      )\n",
      "    )\n",
      "    (2): Dropout(p=0.0, inplace=False)\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (1): GELU()\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (1): GELU()\n",
      "      (2): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): Linear(in_features=64, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if config_data.dtype == \"float64\":\n",
    "    print(\"Use float64 model\")\n",
    "    model = model.double()\n",
    "else:\n",
    "    print(\"use float32 model\")\n",
    "    print(config_model)\n",
    "    print(config_data)\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do training and validation\n",
    "\n",
    "# initialise loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the Tensorflow writer\n",
    "#writer = SummaryWriter(comment=json.dumps(config_model.toDict())\\\n",
    "#                                + json.dumps(config_data.toDict()))\n",
    "writer = SummaryWriter(comment=config_model.implementation)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "# initialise pipeline class\n",
    "pipe = Pipeline(model, [dl_train, None], loss_fn, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "# pipe.train(config_model.optimizer,\n",
    "#            config_model.num_epochs,\n",
    "#            cross_validation=False,\n",
    "#            optimizers_param={\"lr\": config_model.learning_rate,\n",
    "#            \"weight_decay\": config_model.weight_decay},\n",
    "#            n_accumulated_grads=config_model.n_accumulated_grads,\n",
    "#            lr_scheduler=get_cosine_schedule_with_warmup,  #get_constant_schedule_with_warmup,  #get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "#            scheduler_params = {\"num_warmup_steps\": int(config_model.warmup * config_model.num_epochs),\n",
    "#                                \"num_training_steps\": config_model.num_epochs,},\n",
    "#                                #\"num_cycles\": 1},\n",
    "#            store_grad_layer_hist=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep training\n",
    "#pipe.train(Adam, 100, False, keep_training=True, store_grad_layer_hist=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearch\n",
    "\n",
    "# initialise gridsearch\n",
    "pruner = NopPruner()\n",
    "search = Gridsearch(pipe, search_metric=\"accuracy\", n_trials=50, best_not_last=False, pruner=pruner)\n",
    "\n",
    "# dictionaries of hyperparameters\n",
    "optimizers_params = {\"lr\": [1e-3, 1e-0, None, True],\n",
    "                      \"weight_decay\": [0.0001, 0.2, None, True] }\n",
    "dataloaders_params = {\"batch_size\": [8, 16, 2]}\n",
    "models_hyperparams = {\"n_layer_enc\": [2, 4],\n",
    "                      \"n_layer_dec\": [1, 5],\n",
    "                      \"num_heads\": [\"2\", \"4\", \"8\"],\n",
    "                      \"hidden_dim\": [\"16\", \"32\", \"64\"],\n",
    "                      \"dropout\": [0.0, 0.5, 0.05],\n",
    "                      \"layer_norm\": [\"True\", \"False\"],\n",
    "                      \"bias_attention\": [\"True\", \"False\"]}#,\n",
    "                      #'pre_layer_norm': [\"True\", \"False\"]}\n",
    "    \n",
    "scheduler_params = {\"num_warmup_steps\": int(0.1 * config_model.num_epochs),  #(int) – The number of steps for the warmup phase.\n",
    "                    \"num_training_steps\": config_model.num_epochs, #(int) – The total number of training steps.\n",
    "                    \"num_cycles\": [1, 3, 1]}\n",
    "\n",
    "# # starting the gridsearch\n",
    "# search.start((AdamW,), n_epochs=config_model.num_epochs, cross_validation=False,\n",
    "#             optimizers_params=optimizers_params,\n",
    "#             dataloaders_params=dataloaders_params,\n",
    "#             models_hyperparams=models_hyperparams, lr_scheduler=get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "#             scheduler_params=scheduler_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
