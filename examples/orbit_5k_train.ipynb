{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import get_ipython  # type: ignore\n",
    "\n",
    "get_ipython().magic(\"load_ext autoreload\")\n",
    "get_ipython().magic(\"autoreload 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotmap import DotMap\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Import the PyTorch modules\n",
    "import torch  # type: ignore\n",
    "from torch import nn  # type: ignore\n",
    "from torch.optim import SGD, Adam, RMSprop, AdamW  # type: ignore\n",
    "\n",
    "# Import Tensorflow writer\n",
    "from torch.utils.tensorboard import SummaryWriter  # type: ignore\n",
    "\n",
    "from transformers.optimization import (\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "    get_constant_schedule_with_warmup,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "# Import the giotto-deep modules\n",
    "from gdeep.data.datasets import OrbitsGenerator, DataLoaderKwargs\n",
    "from gdeep.topology_layers import Persformer\n",
    "from gdeep.trainer import Trainer\n",
    "from gdeep.search import HyperParameterOptimization\n",
    "import json\n",
    "\n",
    "from optuna.pruners import MedianPruner, NopPruner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "config_data = DotMap(\n",
    "    {\n",
    "        \"batch_size_train\": 32,\n",
    "        \"num_orbits_per_class\": 1_000,\n",
    "        \"validation_percentage\": 0.0,\n",
    "        \"test_percentage\": 0.0,\n",
    "        \"num_jobs\": 8,\n",
    "        \"dynamical_system\": \"classical_convention\",\n",
    "        \"homology_dimensions\": (0, 1),\n",
    "        \"dtype\": \"float32\",\n",
    "        \"arbitrary_precision\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "config_model = DotMap(\n",
    "    {\n",
    "        \"implementation\": \"Old_SetTransformer\",  # SetTransformer, PersFormer,\n",
    "        # PytorchTransformer, DeepSet, X-Transformer\n",
    "        \"dim_input\": 2 + len(config_data.homology_dimensions)\n",
    "        if len(config_data.homology_dimensions) > 1\n",
    "        else 2,\n",
    "        \"num_outputs\": 1,  # for classification tasks this should be 1\n",
    "        \"num_classes\": 5,  # number of classes\n",
    "        \"dim_hidden\": 64,\n",
    "        \"num_heads\": 4,\n",
    "        \"num_induced_points\": 32,\n",
    "        \"layer_norm\": False,  # use layer norm\n",
    "        \"simplified_layer_norm\": False,  # Xu, J., et al. Understanding and improving layer normalization.\n",
    "        \"pre_layer_norm\": False,\n",
    "        \"layer_norm_pooling\": False,\n",
    "        \"num_layers_encoder\": 2,\n",
    "        \"num_layers_decoder\": 3,\n",
    "        \"attention_type\": \"induced_attention\",\n",
    "        \"activation\": \"gelu\",\n",
    "        \"dropout_enc\": 0.0,\n",
    "        \"dropout_dec\": 0.0,\n",
    "        \"optimizer\": Adam,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"num_epochs\": 200,\n",
    "        \"pooling_type\": \"attention\",\n",
    "        \"weight_decay\": 0.00,\n",
    "        \"n_accumulated_grads\": 0,\n",
    "        \"bias_attention\": \"True\",\n",
    "        \"warmup\": 0.02,\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data loader\n",
    "\n",
    "\n",
    "dataloaders_dicts = DataLoaderKwargs(\n",
    "    train_kwargs={\"batch_size\": config_data.batch_size_train,},\n",
    "    val_kwargs={\"batch_size\": 4},\n",
    "    test_kwargs={\"batch_size\": 3},\n",
    ")\n",
    "\n",
    "og = OrbitsGenerator(\n",
    "    num_orbits_per_class=config_data.num_orbits_per_class,\n",
    "    homology_dimensions=config_data.homology_dimensions,\n",
    "    validation_percentage=config_data.validation_percentage,\n",
    "    test_percentage=config_data.test_percentage,\n",
    "    n_jobs=config_data.num_jobs,\n",
    "    dynamical_system=config_data.dynamical_system,\n",
    "    dtype=config_data.dtype,\n",
    "    arbitrary_precision=config_data.arbitrary_precision,\n",
    ")\n",
    "\n",
    "if config_data.arbitrary_precision:\n",
    "    orbits = np.load(os.path.join(\"data\", \"orbit5k_arbitrary_precision.npy\"))\n",
    "    og.orbits_from_array(orbits)\n",
    "\n",
    "if len(config_data.homology_dimensions) == 0:\n",
    "    dl_train, _, _ = og.get_dataloader_orbits(dataloaders_dicts)\n",
    "else:\n",
    "    dl_train, _, _ = og.get_dataloader_persistence_diagrams(dataloaders_dicts)\n",
    "\n",
    "\n",
    "# Define the model\n",
    "if config_model.implementation == \"Old_SetTransformer\":\n",
    "    # initialize SetTransformer model\n",
    "\n",
    "    model = Persformer(\n",
    "        dim_input=config_model.dim_input,\n",
    "        dim_output=5,\n",
    "        num_inds=config_model.num_induced_points,\n",
    "        dim_hidden=config_model.dim_hidden,\n",
    "        num_heads=str(config_model.num_heads),\n",
    "        layer_norm=str(config_model.layer_norm),  # use layer norm\n",
    "        pre_layer_norm=str(config_model.pre_layer_norm),\n",
    "        simplified_layer_norm=str(config_model.simplified_layer_norm),\n",
    "        dropout_enc=config_model.dropout_enc,\n",
    "        dropout_dec=config_model.dropout_dec,\n",
    "        num_layer_enc=config_model.num_layers_encoder,\n",
    "        num_layer_dec=config_model.num_layers_decoder,\n",
    "        activation=config_model.activation,\n",
    "        bias_attention=config_model.bias_attention,\n",
    "        attention_type=config_model.attention_type,\n",
    "        layer_norm_pooling=str(config_model.layer_norm_pooling),\n",
    "    )\n",
    "else:\n",
    "    raise Exception(\"Unknown Implementation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config_data.dtype == \"float64\":\n",
    "    print(\"Use float64 model\")\n",
    "    model = model.double()\n",
    "else:\n",
    "    print(\"use float32 model\")\n",
    "    print(config_model)\n",
    "    print(config_data)\n",
    "    print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do training and validation\n",
    "\n",
    "# initialise loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize the Tensorflow writer\n",
    "# writer = SummaryWriter(comment=json.dumps(config_model.toDict())\\\n",
    "#                                + json.dumps(config_data.toDict()))\n",
    "writer = SummaryWriter(comment=config_model.implementation)\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "# initialise pipeline class\n",
    "pipe = Trainer(model, [dl_train, None], loss_fn, writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "# pipe.train(config_model.optimizer,\n",
    "#            config_model.num_epochs,\n",
    "#            cross_validation=False,\n",
    "#            optimizers_param={\"lr\": config_model.learning_rate,\n",
    "#            \"weight_decay\": config_model.weight_decay},\n",
    "#            n_accumulated_grads=config_model.n_accumulated_grads,\n",
    "#            lr_scheduler=get_cosine_schedule_with_warmup,  #get_constant_schedule_with_warmup,  #get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "#            scheduler_params = {\"num_warmup_steps\": int(config_model.warmup * config_model.num_epochs),\n",
    "#                                \"num_training_steps\": config_model.num_epochs,},\n",
    "#                                #\"num_cycles\": 1},\n",
    "#            store_grad_layer_hist=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep training\n",
    "# pipe.train(Adam, 100, False, keep_training=True, store_grad_layer_hist=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridsearch\n",
    "\n",
    "# initialise gridsearch\n",
    "pruner = NopPruner()\n",
    "search = HyperParameterOptimization(\n",
    "    pipe, search_metric=\"accuracy\", n_trials=50, best_not_last=False, pruner=pruner\n",
    ")\n",
    "\n",
    "# dictionaries of hyperparameters\n",
    "optimizers_params = {\n",
    "    \"lr\": [1e-3, 1e-0, None, True],\n",
    "    \"weight_decay\": [0.0001, 0.2, None, True],\n",
    "}\n",
    "dataloaders_params = {\"batch_size\": [8, 16, 2]}\n",
    "models_hyperparams = {\n",
    "    \"n_layer_enc\": [2, 4],\n",
    "    \"n_layer_dec\": [1, 5],\n",
    "    \"num_heads\": [\"2\", \"4\", \"8\"],\n",
    "    \"hidden_dim\": [\"16\", \"32\", \"64\"],\n",
    "    \"dropout\": [0.0, 0.5, 0.05],\n",
    "    \"layer_norm\": [\"True\", \"False\"],\n",
    "    \"bias_attention\": [\"True\", \"False\"],\n",
    "}  # ,\n",
    "#'pre_layer_norm': [\"True\", \"False\"]}\n",
    "\n",
    "scheduler_params = {\n",
    "    \"num_warmup_steps\": int(\n",
    "        0.1 * config_model.num_epochs\n",
    "    ),  # (int) – The number of steps for the warmup phase.\n",
    "    \"num_training_steps\": config_model.num_epochs,  # (int) – The total number of training steps.\n",
    "    \"num_cycles\": [1, 3, 1],\n",
    "}\n",
    "\n",
    "# # starting the gridsearch\n",
    "# search.start((AdamW,), n_epochs=config_model.num_epochs, cross_validation=False,\n",
    "#             optimizers_params=optimizers_params,\n",
    "#             dataloaders_params=dataloaders_params,\n",
    "#             models_hyperparams=models_hyperparams, lr_scheduler=get_cosine_with_hard_restarts_schedule_with_warmup,\n",
    "#             scheduler_params=scheduler_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
