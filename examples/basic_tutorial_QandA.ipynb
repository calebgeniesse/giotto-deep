{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: Question answer\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with the basic functioning of *giotto-deep* API.\n",
    "\n",
    "The example described in this tutorial is the one of question answer.\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. creation of a dataset\n",
    " 2. creation of a model\n",
    " 3. define metrics and losses\n",
    " 4. train the model\n",
    " 5. extract some features of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPUs...\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from gdeep.models import FFNet\n",
    "\n",
    "from gdeep.visualisation import  persistence_diagrams_of_activations\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gdeep.data import TorchDataLoader\n",
    "from gdeep.pipeline import Pipeline\n",
    "\n",
    "from gtda.diagrams import BettiCurve\n",
    "\n",
    "from gtda.plotting import plot_betti_surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the reuslts of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/example` folder. There run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training to see all the visualisation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# the only part of the training set we are interested in\n",
    "train_indices = list(range(32*10))\n",
    "\n",
    "dl = TorchDataLoader(name=\"SQuAD2\", convert_to_map_dataset=True)\n",
    "dl_tr_str, dl_ts_str = dl.build_dataloaders(sampler=SubsetRandomSampler(train_indices), batch_size=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains a context and a question whose answer can be found within that context. The correct answer as well as the starting characters are also provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Beyoncé embarked on The Mrs. Carter Show World Tour on April 15 in Belgrade, Serbia; the tour included 132 dates that ran through to March 2014. It became the most successful tour of her career and one of the most-successful tours of all time. In May, Beyoncé\\'s cover of Amy Winehouse\\'s \"Back to Black\" with André 3000 on The Great Gatsby soundtrack was released. She was also honorary chair of the 2013 Met Gala. Beyoncé voiced Queen Tara in the 3D CGI animated film, Epic, released by 20th Century Fox on May 24, and recorded an original song for the film, \"Rise Up\", co-written with Sia.',),\n",
       " (\"What was the name of Beyoncé's tour that she started on April 15?\",),\n",
       " [('The Mrs. Carter Show World Tour',)],\n",
       " [tensor([20])]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datum = next(iter(dl_tr_str))\n",
    "datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"When finally Edward the Confessor returned from his father's refuge in 1041, at the invitation of his half-brother Harthacnut, he brought with him a Norman-educated mind. He also brought many Norman counsellors and fighters, some of whom established an English cavalry force. This concept never really took root, but it is a typical example of the attitudes of Edward. He appointed Robert of Jumièges archbishop of Canterbury and made Ralph the Timid earl of Hereford. He invited his brother-in-law Eustace II, Count of Boulogne to his court in 1051, an event which resulted in the greatest of early conflicts between Saxon and Norman and ultimately resulted in the exile of Earl Godwin of Wessex.\",),\n",
       " ('Who did Edward make archbishop of Canterbury?',),\n",
       " [('Robert of Jumièges',), ('Robert of Jumièges',), ('Robert of Jumièges',)],\n",
       " [tensor([382]), tensor([382]), tensor([382])]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datum = next(iter(dl_ts_str))\n",
    "datum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required preprocessing\n",
    "\n",
    "Neural networks cannot direcly deal with strings. We have first to preprocess the dataset in three main ways:\n",
    " 1. Tokenise the strings into its words\n",
    " 2. Build a vocabulary out of these words\n",
    " 3. Embed each word into a vector, so that each sentence becomes a list of vectors\n",
    "\n",
    "The first two steps are performed by the `PreprocessTextQA`. The embedding will be added directly to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.data import PreprocessTextQA\n",
    "\n",
    "prec = PreprocessTextQA((dl_tr_str, dl_ts_str))\n",
    "\n",
    "(dl_tr, dl_ts) = prec.build_dataloaders(batch_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 778,   53,  110,  ...,   41,  191, 2567],\n",
       "          [ 715,  271,   27,  ..., 2567, 2567, 2567]],\n",
       " \n",
       "         [[3792,  281,   14,  ..., 2567, 2567, 2567],\n",
       "          [  66,  211, 4077,  ..., 2567, 2567, 2567]],\n",
       " \n",
       "         [[ 943,   53,  114,  ..., 2567, 2567, 2567],\n",
       "          [ 133,  110,  144,  ..., 2567, 2567, 2567]]]),\n",
       " tensor([[184, 185],\n",
       "         [ 46,  49],\n",
       "         [128, 129]])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = next(iter(dl_tr))\n",
    "aa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train your model\n",
    "\n",
    "The model for QA shall accept as input the context and the question and return the probabilities for the initial and final token of the answer in the input context. The output than, is a pair of logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Transformer\n",
    "from torch.optim import Adam, SparseAdam, SGD\n",
    "import copy\n",
    "# my simple transformer model\n",
    "class QATransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, embed_dim):\n",
    "        super(QATransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=embed_dim,\n",
    "                                       nhead=2,\n",
    "                                       num_encoder_layers=1,\n",
    "                                       num_decoder_layers=1,\n",
    "                                       dim_feedforward=512,\n",
    "                                       dropout=0.1)\n",
    "        self.embedding_src = nn.Embedding(src_vocab_size, embed_dim, sparse=True)\n",
    "        self.embedding_tgt = nn.Embedding(tgt_vocab_size, embed_dim, sparse=True)\n",
    "        self.generator = nn.Linear(embed_dim, 2)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        #print(X.shape)\n",
    "        src = X[:,0,:]\n",
    "        tgt = X[:,1,:]\n",
    "        #print(src.shape, tgt.shape)\n",
    "        src_emb = self.embedding_src(src)\n",
    "        tgt_emb = self.embedding_tgt(tgt)\n",
    "        #print(src_emb.shape, tgt_emb.shape)\n",
    "        self.outs = self.transformer(src_emb, tgt_emb)\n",
    "        #print(outs.shape)\n",
    "        logits = self.generator(self.outs)\n",
    "        #print(logits.shape)\n",
    "        #out = torch.topk(logits, k=1, dim=2).indices.reshape(-1,44)\n",
    "        #print(out, out.shape)\n",
    "        return logits\n",
    "    \n",
    "    def __deepcopy__(self, memo):\n",
    "        \"\"\"this is needed to make sure that the \n",
    "        non-leaf nodes do not\n",
    "        interfere with copy.deepcopy()\n",
    "        \"\"\"\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        memo[id(self)] = result\n",
    "        for k, v in self.__dict__.items():\n",
    "            setattr(result, k, copy.deepcopy(v, memo))\n",
    "        return result\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"this method is used only at the inference step\"\"\"\n",
    "        return self.transformer.encoder(\n",
    "                            self.embedding_src(src), src_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        \"\"\"this method is used only at the inference step\"\"\"\n",
    "        return self.transformer.decoder(\n",
    "                          self.embedding_tgt(tgt), memory,\n",
    "                          tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QATransformer(\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=64, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=64, bias=True)\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=64, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=64, bias=True)\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (embedding_src): Embedding(5500, 64, sparse=True)\n",
      "  (embedding_tgt): Embedding(5500, 64, sparse=True)\n",
      "  (generator): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5500\n",
    "\n",
    "src_vocab_size = vocab_size\n",
    "tgt_vocab_size = vocab_size\n",
    "emb_size = 64\n",
    "\n",
    "model = QATransformer(src_vocab_size, tgt_vocab_size, emb_size)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "This loss function is a adapted version of the Cross Entropy for the trnasformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss_fn(output_of_network, label_of_dataloader):\n",
    "    #print(output_of_network.shape, label_of_dataloader.shape)\n",
    "    tgt_out = label_of_dataloader\n",
    "    #print(tgt_out)\n",
    "    logits = output_of_network\n",
    "    cel = nn.CrossEntropyLoss()\n",
    "    return cel(logits, tgt_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch training loss: 5.798102 \tEpoch training accuracy: 0.39%                                                            \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/matteocaorsi/Desktop/giotto-deep/gdeep/pipeline/pipeline.py:315: UserWarning:\n",
      "\n",
      "Cannot store data in the PR curve\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation results: \n",
      " Accuracy: 3.12%,                 Avg loss: 5.585218 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch training loss: 5.593606 \tEpoch training accuracy: 0.39%                                                             \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " Accuracy: 1.56%,                 Avg loss: 5.435014 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch training loss: 5.488928 \tEpoch training accuracy: 1.17%                                                             \n",
      "Time taken for this epoch: 4.00s\n",
      "Learning rate value: 0.01000000\n",
      "Validation results: \n",
      " Accuracy: 0.00%,                 Avg loss: 5.397848 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5.397847771644592, 0.0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare a pipeline class with the model, dataloaders loss_fn and tensorboard writer\n",
    "pipe = Pipeline(model, (dl_tr, dl_ts), loss_fn, writer)\n",
    "\n",
    "# train the model\n",
    "pipe.train(SGD, 3, False, {\"lr\":0.01}, {\"batch_size\":16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answering questions!\n",
    "\n",
    "Here we have a question and its associated context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In the visual arts, the Normans did not have the rich and distinctive traditions of the cultures they conquered. However, in the early 11th century the dukes began a programme of church reform, encouraging the Cluniac reform of monasteries and patronising intellectual pursuits, especially the proliferation of scriptoria and the reconstitution of a compilation of lost illuminated manuscripts. The church was utilised by the dukes as a unifying force for their disparate duchy. The chief monasteries taking part in this \"renaissance\" of Norman art and scholarship were Mont-Saint-Michel, Fécamp, Jumièges, Bec, Saint-Ouen, Saint-Evroul, and Saint-Wandrille. These centres were in contact with the so-called \"Winchester school\", which channeled a pure Carolingian artistic tradition to Normandy. In the final decade of the 11th and first of the 12th century, Normandy experienced a golden age of illustrated manuscripts, but it was brief and the major scriptoria of Normandy ceased to function after the midpoint of the century.',),\n",
       " ('Who used the church to unify themselves?',)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb = next(iter(dl_ts_str))\n",
    "bb[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Get the vocabulary and numericize the question and context to then input both to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = prec.vocabulary\n",
    "context = prec.tokenizer(bb[0][0])\n",
    "question = prec.tokenizer(bb[1][0])\n",
    "\n",
    "# get the indexes in the vocabulary of the tokens\n",
    "context_idx = torch.tensor(list(map(voc.__getitem__,context)))\n",
    "question_idx = torch.tensor(list(map(voc.__getitem__,question)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_to_pad = aa[0].shape[-1]\n",
    "pad_fn = lambda item : torch.cat([item, prec.pad_item * torch.ones(length_to_pad - item.shape[0])])\n",
    "\n",
    "# these tansors are ready for be fitted into teh model\n",
    "context_ready_for_model = pad_fn(context_idx)\n",
    "question_ready_for_model = pad_fn(question_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the two tensors of context and question together and input them to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.stack((context_ready_for_model, question_ready_for_model)).reshape(1,*aa[0].shape[1:]).long()\n",
    "out = model(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is the ligits for the start and end tokens of the answer. It is now time to extract them with `torch.argmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model proposes: ' ['arts', ',', 'the', 'normans'] ...'\n",
      "The actual answer was: 'dukes'\n"
     ]
    }
   ],
   "source": [
    "answer_idx = torch.argmax(out, dim=1)\n",
    "\n",
    "try:\n",
    "    if answer_idx[0][1] > answer_idx[0][0]:\n",
    "        print(\"The model proposes: '\", context[answer_idx[0][0]:answer_idx[0][1]],\"...'\")\n",
    "    else:\n",
    "        print(\"The model proposes: '\", context[answer_idx[0][0]],\"...'\")\n",
    "except IndexError:\n",
    "    print(\"The model was not able to find the answer.\")\n",
    "print(\"The actual answer was: '\" + bb[2][0][0]+\"'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract inner data from your models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.encoder.layers.0.self_attn.in_proj_weight torch.Size([192, 64])\n",
      "transformer.encoder.layers.0.self_attn.in_proj_bias torch.Size([192])\n",
      "transformer.encoder.layers.0.self_attn.out_proj.weight torch.Size([64, 64])\n",
      "transformer.encoder.layers.0.self_attn.out_proj.bias torch.Size([64])\n",
      "transformer.encoder.layers.0.linear1.weight torch.Size([512, 64])\n",
      "transformer.encoder.layers.0.linear1.bias torch.Size([512])\n",
      "transformer.encoder.layers.0.linear2.weight torch.Size([64, 512])\n",
      "transformer.encoder.layers.0.linear2.bias torch.Size([64])\n",
      "transformer.encoder.layers.0.norm1.weight torch.Size([64])\n",
      "transformer.encoder.layers.0.norm1.bias torch.Size([64])\n",
      "transformer.encoder.layers.0.norm2.weight torch.Size([64])\n",
      "transformer.encoder.layers.0.norm2.bias torch.Size([64])\n",
      "transformer.encoder.norm.weight torch.Size([64])\n",
      "transformer.encoder.norm.bias torch.Size([64])\n",
      "transformer.decoder.layers.0.self_attn.in_proj_weight torch.Size([192, 64])\n",
      "transformer.decoder.layers.0.self_attn.in_proj_bias torch.Size([192])\n",
      "transformer.decoder.layers.0.self_attn.out_proj.weight torch.Size([64, 64])\n",
      "transformer.decoder.layers.0.self_attn.out_proj.bias torch.Size([64])\n",
      "transformer.decoder.layers.0.multihead_attn.in_proj_weight torch.Size([192, 64])\n",
      "transformer.decoder.layers.0.multihead_attn.in_proj_bias torch.Size([192])\n",
      "transformer.decoder.layers.0.multihead_attn.out_proj.weight torch.Size([64, 64])\n",
      "transformer.decoder.layers.0.multihead_attn.out_proj.bias torch.Size([64])\n",
      "transformer.decoder.layers.0.linear1.weight torch.Size([512, 64])\n",
      "transformer.decoder.layers.0.linear1.bias torch.Size([512])\n",
      "transformer.decoder.layers.0.linear2.weight torch.Size([64, 512])\n",
      "transformer.decoder.layers.0.linear2.bias torch.Size([64])\n",
      "transformer.decoder.layers.0.norm1.weight torch.Size([64])\n",
      "transformer.decoder.layers.0.norm1.bias torch.Size([64])\n",
      "transformer.decoder.layers.0.norm2.weight torch.Size([64])\n",
      "transformer.decoder.layers.0.norm2.bias torch.Size([64])\n",
      "transformer.decoder.layers.0.norm3.weight torch.Size([64])\n",
      "transformer.decoder.layers.0.norm3.bias torch.Size([64])\n",
      "transformer.decoder.norm.weight torch.Size([64])\n",
      "transformer.decoder.norm.bias torch.Size([64])\n",
      "embedding_src.weight torch.Size([5500, 64])\n",
      "embedding_tgt.weight torch.Size([5500, 64])\n",
      "generator.weight torch.Size([2, 64])\n",
      "generator.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "from gdeep.models import ModelExtractor\n",
    "\n",
    "me = ModelExtractor(pipe.model, loss_fn)\n",
    "\n",
    "lista = me.get_layers_param()\n",
    "\n",
    "for k, item in lista.items():\n",
    "    print(k,item.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cpu\")\n",
    "x = next(iter(dl_tr))[0]\n",
    "pipe.model.eval()\n",
    "pipe.model(x.to(DEVICE))\n",
    "\n",
    "list_activations = me.get_activations(x)\n",
    "len(list_activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(dl_tr))[0][0]\n",
    "if x.dtype is not torch.int64:\n",
    "    res = me.get_decision_boundary(x, n_epochs=1)\n",
    "    res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, target = next(iter(dl_tr))\n",
    "if x.dtype is torch.float:\n",
    "    for gradient in me.get_gradients(x, target=target)[1]:\n",
    "        print(gradient.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise activations and other topological aspects of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdeep.visualisation import Visualiser\n",
    "\n",
    "vs = Visualiser(pipe)\n",
    "\n",
    "vs.plot_data_model()\n",
    "#vs.plot_activations(x)\n",
    "#vs.plot_persistence_diagrams(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
