{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# for gridsearch\n","\n","#!pip install pyyaml==5.4.1\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from IPython import get_ipython  # type: ignore\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["get_ipython().magic('load_ext autoreload')\n","get_ipython().magic('autoreload 2')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","from dotmap import DotMap\n","import json\n","import os\n","\n","import numpy as np\n","\n","# Import the PyTorch modules\n","import torch  # type: ignore\n","from torch import nn  # type: ignore\n","from torch.optim import SGD, Adam, RMSprop  # type: ignore\n","\n","# Import Tensorflow writer\n","from torch.utils.tensorboard import SummaryWriter  # type: ignore\n","\n","# Import modules from XTransformers\n","from x_transformers.x_transformers import AttentionLayers, Encoder, ContinuousTransformerWrapper\n","\n","\n","# Import the giotto-deep modules\n","from gdeep.data import OrbitsGenerator, DataLoaderKwargs\n","from gdeep.topology_layers import SetTransformer, PersFormer, DeepSet, PytorchTransformer\n","from gdeep.topology_layers import AttentionPooling\n","from gdeep.pipeline import Pipeline\n","from gdeep.search import Gridsearch\n","import json\n","#from gdeep.search import Gridsearch\n","\n","from optuna.pruners import MedianPruner, NopPruner\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","#Configs\n","config_data = DotMap({\n","    'batch_size_train': 32,\n","    'num_orbits_per_class': 1_000,\n","    'validation_percentage': 0.0,\n","    'test_percentage': 0.0,\n","    'num_jobs': 2,\n","    'dynamical_system': 'classical_convention',\n","    'homology_dimensions': (0, 1),\n","    'dtype': 'float32',\n","    'arbitrary_precision': True\n","})\n","\n","\n","config_model = DotMap({\n","    'implementation': 'SetTransformer', # SetTransformer, PersFormer,\n","    # PytorchTransformer, DeepSet, X-Transformer\n","    'dim_input': 2,\n","    'num_outputs': 1,  # for classification tasks this should be 1\n","    'num_classes': 5,  # number of classes\n","    'dim_hidden': 64,\n","    'num_heads': 4,\n","    'num_induced_points': 32,\n","    'layer_norm': False,  # use layer norm\n","    'pre_layer_norm': False,\n","    'num_layers_encoder': 3,\n","    'num_layers_decoder': 3,\n","    'attention_type': \"self_attention\",\n","    'activation': nn.GELU,\n","    'dropout': 0.0,\n","    'batch_size_train': 32,\n","    'optimizer': torch.optim.Adam,\n","    'learning_rate': 5e-4,\n","    'num_epochs': 10,\n","    'pooling_type': \"max\"\n","})\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","\n","# Define the data loader\n","\n","\n","dataloaders_dicts = DataLoaderKwargs(train_kwargs = {\"batch_size\":\n","                                                        config_data.batch_size_train,},\n","                                     val_kwargs = {\"batch_size\": 4},\n","                                     test_kwargs = {\"batch_size\": 3})\n","\n","og = OrbitsGenerator(num_orbits_per_class=config_data.num_orbits_per_class,\n","                     homology_dimensions = config_data.homology_dimensions,\n","                     validation_percentage=config_data.validation_percentage,\n","                     test_percentage=config_data.test_percentage,\n","                     n_jobs=config_data.num_jobs,\n","                     dynamical_system = config_data.dynamical_system,\n","                     dtype=config_data.dtype,\n","                     arbitrary_precision=config_data.arbitrary_precision,\n","                     )\n","\n","if config_data.arbitrary_precision:\n","    orbits = np.load(os.path.join('data', 'orbit5k_arbitrary_precision.npy'))\n","    og.orbits_from_array(orbits)\n","\n","dl_train, _, _ = og.get_dataloader_orbits(dataloaders_dicts)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","orbits = og.get_orbits()\n","\n","np.save('orbit5k_arbitrary_precision.npy', orbits)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","\n","# Define the model\n","if config_model.implementation == 'SetTransformer':\n","    model = SetTransformer(\n","            dim_input=config_model.dim_input,\n","            num_outputs=1,  # for classification tasks this should be 1\n","            dim_output=config_model.num_classes,  # number of classes\n","            dim_hidden=config_model.dim_hidden,\n","            num_heads=config_model.num_heads,\n","            num_inds=config_model.num_induced_points,\n","            ln=config_model.layer_norm,  # use layer norm\n","            n_layers_encoder=config_model.num_layers_encoder,\n","            n_layers_decoder=config_model.num_layers_decoder,\n","            attention_type=config_model.attention_type,\n","            dropout=config_model.dropout\n","    )\n","\n","elif config_model.implementation == 'PersFormer':\n","    model = PersFormer(\n","            dim_input=2,\n","            dim_output=5,\n","            n_layers=5,\n","            hidden_size=32,\n","            n_heads=4,\n","            dropout=0.1,\n","            layer_norm=True,\n","            pre_layer_norm=False,\n","            activation=nn.GELU,\n","            attention_layer_type=\"self_attention\")\n","\n","elif config_model.implementation == 'PytorchTransformer':\n","    model = PytorchTransformer(\n","            dim_input=2,\n","            dim_output=5,\n","            hidden_size=64,\n","            nhead=8,\n","            activation='gelu',\n","            norm_first=True,\n","            num_layers=3,\n","            dropout=0.0,\n","    )\n","elif config_model.implementation == 'DeepSet':\n","    model = DeepSet(dim_input=2,\n","                    dim_output=config_model.num_classes,\n","                    dim_hidden=config_model.dim_hidden,\n","                    n_layers_encoder=config_model.num_layers_encoder,\n","                    n_layers_decoder=config_model.num_layers_decoder,\n","                    pool=config_model.pooling_type).double()\n","\n","elif config_model.implementation == \"X-Transformer\":\n","    model = \\\n","    nn.Sequential(\n","        ContinuousTransformerWrapper(\n","            dim_in = 2,\n","            use_pos_emb = True,\n","            max_seq_len = None,\n","            attn_layers = Encoder(\n","                dim = config_model.dim_hidden,\n","                depth = config_model.num_layers_encoder,\n","                heads = config_model.num_heads,\n","            ),\n","        ),\n","        AttentionPooling(hidden_dim = config_model.dim_hidden, q_length=1),\n","        nn.Sequential(*[nn.Sequential(nn.Linear(config_model.dim_hidden,\n","                            config_model.dim_hidden),\n","                            nn.ReLU())\n","                for _ in range(config_model.num_layers_decoder)]),\n","        nn.Linear(config_model.dim_hidden, config_model.num_classes)\n","    )\n","\n","else:\n","    raise Exception(\"Unknown Implementation\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SmallDeepSet(nn.Module):\n","    def __init__(self,\n","        pool=\"max\",\n","        dim_input=2,\n","        dim_output=5,):\n","        super().__init__()\n","        self.enc = nn.Sequential(\n","            nn.Linear(in_features=dim_input, out_features=64),\n","            nn.ReLU(),\n","            nn.Linear(in_features=64, out_features=64),\n","            nn.ReLU(),\n","            nn.Linear(in_features=64, out_features=64),\n","            nn.ReLU(),\n","            nn.Linear(in_features=64, out_features=64),\n","        )\n","        self.dec = nn.Sequential(\n","            nn.Linear(in_features=64, out_features=64),\n","            nn.ReLU(),\n","            nn.Linear(in_features=64, out_features=dim_output),\n","        )\n","        self.pool = pool\n","\n","    def forward(self, x):\n","        x = self.enc(x)\n","        if self.pool == \"max\":\n","            x = x.max(dim=1)[0]\n","        elif self.pool == \"mean\":\n","            x = x.mean(dim=1)\n","        elif self.pool == \"sum\":\n","            x = x.sum(dim=1)\n","        x = self.dec(x)\n","        return x\n","\n","if config_data.dtype == \"float64\":\n","    print(\"Use float64 model\")\n","    model = SmallDeepSet().double()\n","else:\n","    print(\"use float32 model\")\n","    model = SmallDeepSet()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Do training and validation\n","\n","# initialise loss\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# Initialize the Tensorflow writer\n","#writer = SummaryWriter(comment=json.dumps(config_model.toDict())\\\n","#                                + json.dumps(config_data.toDict()))\n","writer = SummaryWriter(comment=config_model.implementation)\n","\n","# initialise pipeline class\n","pipe = Pipeline(model, [dl_train, None], loss_fn, writer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# train the model\n","# pipe.train(config_model.optimizer,\n","#            config_model.num_epochs,\n","#            cross_validation=False,\n","#            optimizers_param={\"lr\": config_model.learning_rate})\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# keep training\n","#pipe.train(Adam, 300, False, keep_training=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Gridsearch\n","\n","# initialise gridsearch\n","search = Gridsearch(pipe, search_metric=\"accuracy\", n_trials=10, best_not_last=True, pruner=NopPruner)\n","\n","# dictionaries of hyperparameters\n","optimizers_params = {\"lr\": [1e-6, 1e-3]}\n","dataloaders_params = {\"batch_size\": [32, 64, 16]}\n","models_hyperparams = {\"n_layer_enc\": [2, 5],\n","                      \"n_layer_dec\": [2, 4],\n","                      \"num_heads\": [\"2\", \"4\", \"8\"],\n","                      \"hidden_dim\": [\"16\", \"32\", \"64\"],\n","                      \"dropout\": [0.0, 0.2],\n","                      \"layer_norm\": [\"True\", \"False\"],\n","                      'pre_layer_norm': [\"True\", \"False\"]}\n","\n","# starting the gridsearch\n","search.start((Adam,), n_epochs=300, cross_validation=False,\n","             optimizers_params=optimizers_params,\n","             dataloaders_params=dataloaders_params,\n","             models_hyperparams=models_hyperparams, lr_scheduler=None,\n","             scheduler_params=None)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(search.best_val_acc_gs, search.best_val_loss_gs)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_res = search._results()\n","df_res"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"},"kernelspec":{"display_name":"Python 3.7.12 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
