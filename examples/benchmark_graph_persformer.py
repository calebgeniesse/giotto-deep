# %% [markdown]
#  ## Benchmarking PersFormer on the graph datasets.
#  We will compare the accuracy on the graph datasets of our SetTransformer
#  based on PersFormer with the perslayer introduced in the paper:
#  https://arxiv.org/abs/1904.09378
# %% [markdown]
#  ## Benchmarking MUTAG
#  We will compare the test accuracies of PersLay and PersFormer on the MUTAG
#  dataset. It consists of 188 graphs categorised into two classes.
#  We will train the PersFormer on the same input features as PersFormer to
#  get a fair comparison.
#  The features PersLay is trained on are the extended persistence diagrams of
#  the vertices of the graph filtered by the heat kernel signature (HKS)
#  at time t=10.
#  The maximum (wrt to the architecture and the hyperparameters) mean test
#  accuracy of PersLay is 89.8(Â±0.9) and the train accuracy with the same
#  model and the same hyperparameters is 92.3.
#  They performed 10-fold evaluation, i.e. splitting the dataset into
#  10 equally-sized folds and then record the test accuracy of the i-th
#  fold and training the model on the 9 other folds.

# %%
from IPython import get_ipython
get_ipython().magic('load_ext autoreload')
get_ipython().magic('autoreload 2')

# Import libraries:
import os
from attrdict import AttrDict
from pathlib import Path
import pprint
pp = pprint.PrettyPrinter(indent=2)
from torch.utils.tensorboard import SummaryWriter
from gdeep.pipeline import Pipeline
import warnings
import random
import time
from datetime import datetime
import torch
from torch import Tensor
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader, Subset
from torch.optim import SGD, Adam
import matplotlib.pyplot as plt  # type: ignore

from gdeep.topology_layers import load_data_as_tensor,\
    load_augmented_data_as_tensor, GraphClassifier

from gdeep.pipeline import Pipeline


# %%

config = AttrDict({
    'dataset_name': "MUTAG",
    # Use on of the following datasets to train the model
    # MUTAG, PROTEINS, COX2
    'pers_only': True,
    'use_sam': False,
    'n_epochs': 100,
    'lr': 1e-3,
    'batch_size': 64,
    'ln': True,  # LayerNorm in Set Transformer
    'use_regularization': False,  # Use L2-regularization
    'balance_dataset': False,  # balance dataset to 50% by removing datapoint 
    # from the bigger dataset
    'use_induced_attention': False,  # use trainable query vector instead of
    # self-attention; use induced attention for large sets because of the
    # quadratic scaling of self-attention.
    # the class with more points
    # only use the persistence diagrams as features not the spectral features
    'train_size': 0.8,  # ratio train size to total size of dataset
    'optimizer': lambda params: torch.optim.Adam(params, lr=lr),  # noqa: E731
    'use_data_augmentation': True,
})


# Only balance the dataset if no data augmentation is used.
if config.use_data_augmentation:
    config.balance_dataset = False

# Compare with PersLay baseline
if config.dataset_name == "PROTEINS" and config.pers_only:
    benchmark_accuracy = 72.2
elif config.dataset_name == "MUTAG" and config.pers_only:
    benchmark_accuracy = 85.2
elif config.dataset_name == "COX2" and config.pers_only:
    benchmark_accuracy = 81.5
elif config.dataset_name == "COLLAB" and config.pers_only:
    benchmark_accuracy = 71.6
elif config.dataset_name == "NCI1" and config.pers_only:
    benchmark_accuracy = 72.63
else:
    raise NotImplementedError("benchmark_accuracy not defined")


# %%
# Load extended persistence diagrams and additional features

# Data augmentation using the graphs generated by GRAN
if config.use_data_augmentation and\
        config.dataset_name in ['PROTEINS', 'MUTAG'] :
    x_pds, x_features, y, original_data_size, idx_class =\
        load_augmented_data_as_tensor(config.dataset_name)
else:
    x_pds, x_features, y = load_data_as_tensor(config.dataset_name)

# if `pers_only` set spectral features to 0
if config.pers_only:
    x_features = 0.0 * x_features


# %%
# warnings.warn("You use debuging code, this should be removed in\
# future versions")
# x_pds = x_pds[188:]
# x_features = x_features[188:]
# y = y[188:]


# %%
# Balance labels in dataset

if config.balance_dataset:
    if y.sum() / y.shape[0] > 0.5:
        class_to_remove = 1
        num_classes_to_remove = int(2 * y.sum() - y.shape[0])
    else:
        #class_to_remove = 0
        num_classes_to_remove = int(y.shape[0] - 2 * y.sum())
    idxs_to_remove = ((y == class_to_remove)
                      .nonzero(as_tuple=False)[:num_classes_to_remove, 0]
                      .tolist())

    idxs_to_remain = [i for i in range(y.shape[0]) if i not in idxs_to_remove]

    y = y[idxs_to_remain]
    x_pds = x_pds[idxs_to_remain]
    x_features = x_features[idxs_to_remain]

    print('number of data points removed:', num_classes_to_remove)

print('balance: {:.2f}'.format(y.sum() / y.shape[0]).item())


# %%
# Set up dataset and dataloader
# create the datasets
# https://discuss.pytorch.org/t/make-a-tensordataset-and-dataloader
# -with-multiple-inputs-parameters/26605

graph_ds = TensorDataset(x_pds, x_features, y)


if config.use_data_augmentation and config.dataset_name == 'PROTEINS':
    # Use the graphs in the validation dataset that where not used for the
    # graph augmentation
    # valid_idx = list(set(range(original_data_size)) -  \
    # set(idx_class['1'] + idx_class['2']))

    valid_idx = list(set(range(original_data_size))
                    - set(range(idx_class['1'][0], idx_class['1'][-1]))
                    - set(range(idx_class['2'][0], idx_class['2'][-1]))
                    )
    train_idx = [i for i in range(x_pds.shape[0]) if i not in valid_idx]
    graph_ds_train = Subset(graph_ds, train_idx),
    graph_ds_val = Subset(graph_ds, valid_idx)
else:
    total_size = x_pds.shape[0]
    train_size = int(total_size * config.train_size)
    val_size = int(0.5 * (total_size - train_size))
    test_size = total_size - train_size - val_size
    graph_ds_train, graph_ds_val,  graph_ds_test = \
        torch.utils.data.random_split(graph_ds,
                                      [train_size,
                                      val_size,
                                      test_size],
                                      generator=torch.Generator()
                                        .manual_seed(36))


# Only use on worker if using Colab
if 'google.colab' in str(get_ipython()):
    num_workers = 1
else:
    num_workers = 8


# create data loaders for train, validation, test datasets
graph_dl_train = DataLoader(
    graph_ds_train,
    num_workers=num_workers,
    batch_size=config.batch_size,
    shuffle=True
)

graph_dl_val = DataLoader(
    graph_ds_val,
    num_workers=num_workers,
    batch_size=config.batch_size,
    shuffle=False
)

graph_dl_test = DataLoader(
    graph_ds_test,
    num_workers=num_workers,
    batch_size=config.batch_size,
    shuffle=False
)

config.num_features = graph_ds_train[0][1].shape[0]  # number of additional
# spectral features. Note that these are ignored in the current implementation
config.dim_input = graph_ds_train[0][0].shape[1]  # embedding dimension of
# tokens, in our case this is 2 (x-y-coordinate of the points in the
# persistence diagram + # different types of persistence diagrams


# %%
# Compute balance of the two classes for the train and validation datasets

def print_class_balance(dataloader, type_):
    assert type_ in ['train', 'val', 'test'],\
        "type_ must be 'train', 'val', 'test'"
    balance = 0
    total_size = 0
    for _, _, y_batch in graph_dl_train:
        balance += y_batch.sum()
        total_size += y_batch.shape[0]
    print(type_, 'size:', total_size)
    print(type_, 'balance', (balance / total_size).item())

print_class_balance(graph_dl_train, 'train')
print_class_balance(graph_dl_val, 'val')
print_class_balance(graph_dl_test, 'test')

# %%
def generate_config_run_str(config_run):
    """Generate configuration string for the label of the training run.

    Args:
        config_run (AttrDict): Dictionary of model hyperparameters

    Returns:
        str: String of hyperparameters
    """
    out = ""
    for key, value in config_run.items():
        out += key + "_" + str(value) + "__"
    return out[:-2]

# %%

# Use CUDA for PyTorch if  available
use_cuda = torch.cuda.is_available()
if not use_cuda:
    raise Exception('Please use a GPU for training!')
device = torch.device("cuda:0" if use_cuda else "cpu")

# Specify the random parameter search procedure
num_runs = 1
n_epochs = 1000


for i in range(num_runs):
    print("Test run number", i + 1, "of", num_runs)

    lr = random.choice([1e-5])
    config_run = AttrDict({
        'lr': lr,
        'ln': random.choice([True]),  # LayerNorm in Set Transformer
        'use_regularization': random.choice([False]),
        # Use L2-regularization
        'use_induced_attention': random.choice([False]),
        # use trainable query vector instead of
        # self-attention; use induced attention for large sets because of the
        # quadratic scaling of self-attention.
        # the class with more points
        # only use the persistence diagrams as features not the spectral
        # features
        'optimizer': torch.optim.Adam,  # noqa: E731
        'dim_output': random.choice([300]),
        'dim_hidden': random.choice([512]),
        'num_heads': random.choice([32]),
        'use_sam': random.choice([False]),
        'n_layers': random.choice([3]),
        'dropout': random.choice([0.5])
    })


    assert config_run.use_sam == False,\
        "SAM optimization is not yet implemented"
    assert config_run.use_regularization == False,\
        "weight regularization is not yet implemented"

    config_run_str = generate_config_run_str(config_run)

    # define graph classifier
    gc = GraphClassifier(
            num_features=config.num_features,
            dim_input=config.dim_input,
            num_outputs=1,
            dim_output=config_run.dim_output,
            dim_hidden=config_run.dim_hidden,
            num_heads=config_run.num_heads,
            use_induced_attention=config_run.use_induced_attention)

    # Print the model configuration
    pp.pprint(config_run)


    # train the model and return losses and accuracies information
    tic = time.perf_counter()

    # Define loss function for the model
    loss_fn = nn.CrossEntropyLoss()

    # Use timestamp as train run metadata
    now = datetime.now().strftime("%d_%m_%Y_%H_%M_%S")

    # Define tensorboard writer
    writer = SummaryWriter('runs/MUTAG_architecture_search/'
                    + now
                    + config_run_str)

    # Define pipeline for the model training
    pipe = Pipeline(gc, (graph_dl_train, graph_dl_val, graph_dl_val),
                        loss_fn, writer)

    # train the model
    pipe.train(config_run.optimizer, n_epochs, cross_validation = True,
                    batch_size = 64, lr=config_run.lr)

    toc = time.perf_counter()
    print(f"Trained model for {n_epochs} in {toc - tic:0.4f} seconds")
    del gc
# %%