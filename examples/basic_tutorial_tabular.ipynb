{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic tutorial: tabular data\n",
    "#### Author: Matteo Caorsi\n",
    "\n",
    "This short tutorial provides you with the basic functioning of *giotto-deep* API. As an example, we take a simple classification problem in which we have a point cloud representing two entangled tori.\n",
    "\n",
    "## Scope\n",
    "\n",
    "The goal of this problem is to build a simple feed-forward neural network that is able to separate the two point clouds. The topology of the dataset is non trivial and we empirically discovered that a minimum number of nodes per layer is required to have a good splitting.\n",
    "\n",
    "## Plan for this tutorial\n",
    "\n",
    "The main steps of the tutorial are the following:\n",
    " 1. creation of a dataset\n",
    " 2. creation of a model\n",
    " 3. define metrics and losses\n",
    " 4. run trainig\n",
    " 5. visualise results interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gtda.diagrams import BettiCurve\n",
    "from gtda.plotting import plot_betti_surfaces\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "from gdeep.models import FFNet\n",
    "from gdeep.models import ModelExtractor\n",
    "from gdeep.analysis.interpretability import Interpreter\n",
    "from gdeep.utility.optimisation import SAMOptimizer\n",
    "from gdeep.visualisation import  persistence_diagrams_of_activations\n",
    "from gdeep.trainer import Trainer\n",
    "from gdeep.data.datasets import DatasetBuilder, DataLoaderBuilder\n",
    "from gdeep.visualisation import Visualiser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the tensorboard writer\n",
    "\n",
    "In order to analyse the results of your models, you need to start tensorboard.\n",
    "On the terminal, move inside the `/exampled` folder. There you can run the following command:\n",
    "\n",
    "```\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "Then go [here](http://localhost:6006/) after the training pahse to see all the visualisation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create your dataset\n",
    "\n",
    "The dataset consists of two entangled tori and the goal is to classify the points that belong to one or the other torus. Note that the topology of the decision boundary is non trivial!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd = DatasetBuilder(name=\"DoubleTori\")\n",
    "ds_tr, ds_val, _ = bd.build()\n",
    "\n",
    "dl_builder = DataLoaderBuilder((ds_tr, ds_val))\n",
    "dl_tr, dl_val, dl_ts = dl_builder.build(({\"batch_size\":23},{\"batch_size\":23}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and train your model\n",
    "\n",
    "In the next few cells we show hoe easy t is in giotto-deep to define a model and start th training: ndeed, once we have the model and the data, only a couple of code lines is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class model1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(model1, self).__init__()\n",
    "        self.seqmodel = nn.Sequential(nn.Flatten(), FFNet(arch=[3, 5, 10, 5, 2]))\n",
    "    def forward(self, x):\n",
    "        return self.seqmodel(x)\n",
    "\n",
    "model = model1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initlaise the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# initialise the pipelien class\n",
    "pipe = Trainer(model, (dl_tr, dl_val), loss_fn, writer, k_fold_class=StratifiedKFold(3, shuffle=True))\n",
    "\n",
    "# initialise the SAM optimiser\n",
    "optim = SAMOptimizer(SGD)  # this is a class, not an instance!\n",
    "\n",
    "# train the model with learning rate scheduler and cross-validation\n",
    "pipe.train(optim, 5, True, optimizers_param={\"lr\": 0.01}, lr_scheduler=ExponentialLR, scheduler_params={\"gamma\": 0.9}, \n",
    "           profiling=False, store_grad_layer_hist=True, writer_tag=\"tori\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Evaluation\n",
    "\n",
    "We can easily evaluate the model performance and compute the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of the model performances for the classification task\n",
    "pipe.evaluate_classification(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced topic: add pipeline hooks\n",
    "\n",
    "It is possible to add a hook (a callable) that is called at the end of each training epoch.\n",
    "\n",
    "The arguments of the hook are fix and have to respect the order you see in this example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_of_hook(epoch, optim, me, writer):\n",
    "    print(f\"Here we print the learning rate {optim.param_groups[0]['lr']} at epoch={epoch}\")\n",
    "    print(f\"We can also get the value of gradients and parameters of the model \"\n",
    "          f\"using the model extractor! {me.get_layers_param():}\")\n",
    "    \n",
    "# register the hook\n",
    "pipe.register_pipe_hook(example_of_hook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model with cross validation: we just have to set the parameter `cross_validation = True`.\n",
    "\n",
    "The `keep_training = True` flag allow us to restart from the same scheduler, optimiser and trained model obtained at thhe end of the last training in the instance of the class `pipe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model with CV\n",
    "pipe.train(SGD, 3, cross_validation=True, keep_training=True, profiling=True, writer_tag=\"tori/kt\")\n",
    "\n",
    "# since we used the keep training flag, the optimiser has not been modified compared to the previous training.\n",
    "print(pipe.optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simply use interpretability tools\n",
    "\n",
    "Eventually, one would like to know why a neural network has made e certain classification choice. Even though the models are complex, there are tools in giotto-deep to help you understand which feature mostly contributed to the network choice.\n",
    "\n",
    "In the next celle we use the [Integrated Gradients](https://towardsdatascience.com/integrated-gradients-from-scratch-b46311e4ab4) technique on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inter = Interpreter(pipe.model, \"IntegratedGradients\")\n",
    "inter.interpret_tabular(next(iter(dl_tr))[0], \n",
    "                        target=next(iter(dl_tr))[1], \n",
    "                        n_steps=50);\n",
    "\n",
    "vs = Visualiser(pipe)\n",
    "plt = vs.plot_interpreter_tabular(inter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualise activations and other topological aspects of your model\n",
    "\n",
    "This concluding section shows you how it is possible to visualise a lot of nformation on your network in one line thanks to giotto-deep!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# a batch of data\n",
    "x = next(iter(dl_tr))[0]\n",
    "\n",
    "# send to tensorboard the model graph!\n",
    "vs.plot_data_model()\n",
    "\n",
    "# send to tensorboard the point cloud, whose coordinates are the activations in each layer, corresponding to the batch x\n",
    "vs.plot_activations(x)\n",
    "\n",
    "# plot persistence diagrams of the actiivation point clouds\n",
    "vs.plot_persistence_diagrams(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sampled decision boudnary!\n",
    "vs.plot_decision_boundary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the betti curves of each activaton layer\n",
    "vs.betti_plot_layers((0, 1), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
